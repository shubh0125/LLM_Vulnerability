{
    "V_001": {
        "vulnerable_code": {
            "Code": "futex_wait_restart(struct restart_block *restart){\n\tu32 __user *uaddr = (u32 __user *)restart->futex.uaddr;\n\tint fshared = 0;\n\tktime_t t, *tp = NULL;\n\n\tif (restart->futex.flags & FLAGS_HAS_TIMEOUT) {\n\t\tt.tv64 = restart->futex.time;\n\t\ttp = &t;\n\t}\n\trestart->fn = do_no_restart_syscall;\n\tif (restart->futex.flags & FLAGS_SHARED)\n\t\tfshared = 1;\n\treturn (long)futex_wait(uaddr, fshared, restart->futex.val, tp,\n\t\t\t\trestart->futex.bitset,\n\t\t\t\trestart->futex.flags & FLAGS_CLOCKRT);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "futex_wait_restart(struct restart_block *restart){\n\tu32 __user *uaddr = (u32 __user *)restart->futex.uaddr;\n\tint fshared = 0;\n\tktime_t t, *tp = NULL;\n\n\tif (restart->futex.flags & FLAGS_HAS_TIMEOUT) {\n\t\tt.tv64 = restart->futex.time;\n\t\ttp = &t;\n\t}\n\trestart->fn = do_no_restart_syscall;\n\tif (restart->futex.flags & FLAGS_SHARED)\n\t\tfshared = 1;\n\treturn (long)futex_wait(uaddr, fshared, restart->futex.val, tp,\n\t\t\t\trestart->futex.bitset,\n\t\t\t\trestart->futex.flags & FLAGS_CLOCKRT);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_002": {
        "vulnerable_code": {
            "Code": "lookup_pi_state(u32 uval, struct futex_hash_bucket *hb,\n\t\tunion futex_key *key, struct futex_pi_state **ps){\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_q *this, *next;\n\tstruct plist_head *head;\n\tstruct task_struct *p;\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\n\thead = &hb->chain;\n\n\tplist_for_each_entry_safe(this, next, head, list) {\n\t\tif (match_futex(&this->key, key)) {\n\t\t\t/*\n\t\t\t * Another waiter already exists - bump up\n\t\t\t * the refcount and return its pi_state:\n\t\t\t */\n\t\t\tpi_state = this->pi_state;\n\t\t\t/*\n\t\t\t * Userspace might have messed up non PI and PI futexes\n\t\t\t */\n\t\t\tif (unlikely(!pi_state))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tWARN_ON(!atomic_read(&pi_state->refcount));\n\t\t\tWARN_ON(pid && pi_state->owner &&\n\t\t\t\tpi_state->owner->pid != pid);\n\n\t\t\tatomic_inc(&pi_state->refcount);\n\t\t\t*ps = pi_state;\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0\n\t */\n\tif (!pid)\n\t\treturn -ESRCH;\n\tp = futex_find_get_task(pid);\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\t/*\n\t * We need to look at the task state flags to figure out,\n\t * whether the task is exiting. To protect against the do_exit\n\t * change of the task flags, we do this protected by\n\t * p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->flags & PF_EXITING)) {\n\t\t/*\n\t\t * The task is on the way out. When PF_EXITPIDONE is\n\t\t * set, we know that the task has finished the\n\t\t * cleanup:\n\t\t */\n\t\tint ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\tpi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make 'p'\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\tpi_state->owner = p;\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\t*ps = pi_state;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "lookup_pi_state(u32 uval, struct futex_hash_bucket *hb,\n\t\tunion futex_key *key, struct futex_pi_state **ps){\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_q *this, *next;\n\tstruct plist_head *head;\n\tstruct task_struct *p;\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\n\thead = &hb->chain;\n\n\tplist_for_each_entry_safe(this, next, head, list) {\n\t\tif (match_futex(&this->key, key)) {\n\t\t\t/*\n\t\t\t * Another waiter already exists - bump up\n\t\t\t * the refcount and return its pi_state:\n\t\t\t */\n\t\t\tpi_state = this->pi_state;\n\t\t\t/*\n\t\t\t * Userspace might have messed up non PI and PI futexes\n\t\t\t */\n\t\t\tif (unlikely(!pi_state))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tWARN_ON(!atomic_read(&pi_state->refcount));\n\t\t\tWARN_ON(pid && pi_state->owner &&\n\t\t\t\tpi_state->owner->pid != pid);\n\n\t\t\tatomic_inc(&pi_state->refcount);\n\t\t\t*ps = pi_state;\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0\n\t */\n\tif (!pid)\n\t\treturn -ESRCH;\n\tp = futex_find_get_task(pid);\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\t/*\n\t * We need to look at the task state flags to figure out,\n\t * whether the task is exiting. To protect against the do_exit\n\t * change of the task flags, we do this protected by\n\t * p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->flags & PF_EXITING)) {\n\t\t/*\n\t\t * The task is on the way out. When PF_EXITPIDONE is\n\t\t * set, we know that the task has finished the\n\t\t * cleanup:\n\t\t */\n\t\tint ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\tpi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make 'p'\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\tpi_state->owner = p;\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\t*ps = pi_state;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_003": {
        "vulnerable_code": {
            "Code": "match_futex(union futex_key *key1, union futex_key *key2){\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "match_futex(union futex_key *key1, union futex_key *key2){\n\treturn (key1 && key2\n\t\t&& key1->both.word == key2->both.word\n\t\t&& key1->both.ptr == key2->both.ptr\n\t\t&& key1->both.offset == key2->both.offset);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_004": {
        "vulnerable_code": {
            "Code": "refill_pi_state_cache(void){\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\tatomic_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "refill_pi_state_cache(void){\n\tstruct futex_pi_state *pi_state;\n\n\tif (likely(current->pi_state_cache))\n\t\treturn 0;\n\n\tpi_state = kzalloc(sizeof(*pi_state), GFP_KERNEL);\n\n\tif (!pi_state)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&pi_state->list);\n\t/* pi_mutex gets initialized later */\n\tpi_state->owner = NULL;\n\tatomic_set(&pi_state->refcount, 1);\n\tpi_state->key = FUTEX_KEY_INIT;\n\n\tcurrent->pi_state_cache = pi_state;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_005": {
        "vulnerable_code": {
            "Code": "fetch_robust_entry(struct robust_list __user **entry,\n\t\t\t\t     struct robust_list __user * __user *head,\n\t\t\t\t     unsigned int *pi){\n\tunsigned long uentry;\n\n\tif (get_user(uentry, (unsigned long __user *)head))\n\t\treturn -EFAULT;\n\n\t*entry = (void __user *)(uentry & ~1UL);\n\t*pi = uentry & 1;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fetch_robust_entry(struct robust_list __user **entry,\n\t\t\t\t     struct robust_list __user * __user *head,\n\t\t\t\t     unsigned int *pi){\n\tunsigned long uentry;\n\n\tif (get_user(uentry, (unsigned long __user *)head))\n\t\treturn -EFAULT;\n\n\t*entry = (void __user *)(uentry & ~1UL);\n\t*pi = uentry & 1;\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_006": {
        "vulnerable_code": {
            "Code": "free_pi_state(struct futex_pi_state *pi_state){\n\tif (!atomic_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\traw_spin_lock_irq(&pi_state->owner->pi_lock);\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock_irq(&pi_state->owner->pi_lock);\n\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex, pi_state->owner);\n\t}\n\n\tif (current->pi_state_cache)\n\t\tkfree(pi_state);\n\telse {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\tatomic_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "free_pi_state(struct futex_pi_state *pi_state){\n\tif (!atomic_dec_and_test(&pi_state->refcount))\n\t\treturn;\n\n\t/*\n\t * If pi_state->owner is NULL, the owner is most probably dying\n\t * and has cleaned up the pi_state already\n\t */\n\tif (pi_state->owner) {\n\t\traw_spin_lock_irq(&pi_state->owner->pi_lock);\n\t\tlist_del_init(&pi_state->list);\n\t\traw_spin_unlock_irq(&pi_state->owner->pi_lock);\n\n\t\trt_mutex_proxy_unlock(&pi_state->pi_mutex, pi_state->owner);\n\t}\n\n\tif (current->pi_state_cache)\n\t\tkfree(pi_state);\n\telse {\n\t\t/*\n\t\t * pi_state->list is already empty.\n\t\t * clear pi_state->owner.\n\t\t * refcount is at 0 - put it back to 1.\n\t\t */\n\t\tpi_state->owner = NULL;\n\t\tatomic_set(&pi_state->refcount, 1);\n\t\tcurrent->pi_state_cache = pi_state;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_007": {
        "vulnerable_code": {
            "Code": "futex_get_mm(union futex_key *key){\n\tatomic_inc(&key->private.mm->mm_count);\n\t/*\n\t * Ensure futex_get_mm() implies a full barrier such that\n\t * get_futex_key() implies a full barrier. This is relied upon\n\t * as full barrier (B), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic_inc();\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "futex_get_mm(union futex_key *key){\n\tatomic_inc(&key->private.mm->mm_count);\n\t/*\n\t * Ensure futex_get_mm() implies a full barrier such that\n\t * get_futex_key() implies a full barrier. This is relied upon\n\t * as full barrier (B), see the ordering comment above.\n\t */\n\tsmp_mb__after_atomic_inc();\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_008": {
        "vulnerable_code": {
            "Code": "futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,\n\t\t      ktime_t *abs_time, u32 bitset){\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, holds hb lock and increments\n\t * q.key refs.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* queue_me and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\t/* unqueue_me() drops q.key ref */\n\tif (!unqueue_me(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current_thread_info()->restart_block;\n\trestart->fn = futex_wait_restart;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = abs_time->tv64;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = -ERESTART_RESTARTBLOCK;\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,\n\t\t      ktime_t *abs_time, u32 bitset){\n\tstruct hrtimer_sleeper timeout, *to = NULL;\n\tstruct restart_block *restart;\n\tstruct futex_hash_bucket *hb;\n\tstruct futex_q q = futex_q_init;\n\tint ret;\n\n\tif (!bitset)\n\t\treturn -EINVAL;\n\tq.bitset = bitset;\n\n\tif (abs_time) {\n\t\tto = &timeout;\n\n\t\thrtimer_init_on_stack(&to->timer, (flags & FLAGS_CLOCKRT) ?\n\t\t\t\t      CLOCK_REALTIME : CLOCK_MONOTONIC,\n\t\t\t\t      HRTIMER_MODE_ABS);\n\t\thrtimer_init_sleeper(to, current);\n\t\thrtimer_set_expires_range_ns(&to->timer, *abs_time,\n\t\t\t\t\t     current->timer_slack_ns);\n\t}\n\nretry:\n\t/*\n\t * Prepare to wait on uaddr. On success, holds hb lock and increments\n\t * q.key refs.\n\t */\n\tret = futex_wait_setup(uaddr, val, flags, &q, &hb);\n\tif (ret)\n\t\tgoto out;\n\n\t/* queue_me and wait for wakeup, timeout, or a signal. */\n\tfutex_wait_queue_me(hb, &q, to);\n\n\t/* If we were woken (and unqueued), we succeeded, whatever. */\n\tret = 0;\n\t/* unqueue_me() drops q.key ref */\n\tif (!unqueue_me(&q))\n\t\tgoto out;\n\tret = -ETIMEDOUT;\n\tif (to && !to->task)\n\t\tgoto out;\n\n\t/*\n\t * We expect signal_pending(current), but we might be the\n\t * victim of a spurious wakeup as well.\n\t */\n\tif (!signal_pending(current))\n\t\tgoto retry;\n\n\tret = -ERESTARTSYS;\n\tif (!abs_time)\n\t\tgoto out;\n\n\trestart = &current_thread_info()->restart_block;\n\trestart->fn = futex_wait_restart;\n\trestart->futex.uaddr = uaddr;\n\trestart->futex.val = val;\n\trestart->futex.time = abs_time->tv64;\n\trestart->futex.bitset = bitset;\n\trestart->futex.flags = flags | FLAGS_HAS_TIMEOUT;\n\n\tret = -ERESTART_RESTARTBLOCK;\n\nout:\n\tif (to) {\n\t\thrtimer_cancel(&to->timer);\n\t\tdestroy_hrtimer_on_stack(&to->timer);\n\t}\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_009": {
        "vulnerable_code": {
            "Code": "hb_waiters_dec(struct futex_hash_bucket *hb){\n#ifdef CONFIG_SMP\n\tatomic_dec(&hb->waiters);\n#endif\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "hb_waiters_dec(struct futex_hash_bucket *hb){\n#ifdef CONFIG_SMP\n\tatomic_dec(&hb->waiters);\n#endif\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_010": {
        "vulnerable_code": {
            "Code": "lookup_pi_state(u32 uval, struct futex_hash_bucket *hb,\n\t\tunion futex_key *key, struct futex_pi_state **ps,\n\t\tstruct task_struct *task){\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_q *this, *next;\n\tstruct task_struct *p;\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (match_futex(&this->key, key)) {\n\t\t\t/*\n\t\t\t * Another waiter already exists - bump up\n\t\t\t * the refcount and return its pi_state:\n\t\t\t */\n\t\t\tpi_state = this->pi_state;\n\t\t\t/*\n\t\t\t * Userspace might have messed up non-PI and PI futexes\n\t\t\t */\n\t\t\tif (unlikely(!pi_state))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tWARN_ON(!atomic_read(&pi_state->refcount));\n\n\t\t\t/*\n\t\t\t * When pi_state->owner is NULL then the owner died\n\t\t\t * and another waiter is on the fly. pi_state->owner\n\t\t\t * is fixed up by the task which acquires\n\t\t\t * pi_state->rt_mutex.\n\t\t\t *\n\t\t\t * We do not check for pid == 0 which can happen when\n\t\t\t * the owner died and robust_list_exit() cleared the\n\t\t\t * TID.\n\t\t\t */\n\t\t\tif (pid && pi_state->owner) {\n\t\t\t\t/*\n\t\t\t\t * Bail out if user space manipulated the\n\t\t\t\t * futex value.\n\t\t\t\t */\n\t\t\t\tif (pid != task_pid_vnr(pi_state->owner))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Protect against a corrupted uval. If uval\n\t\t\t * is 0x80000000 then pid is 0 and the waiter\n\t\t\t * bit is set. So the deadlock check in the\n\t\t\t * calling code has failed and we did not fall\n\t\t\t * into the check above due to !pid.\n\t\t\t */\n\t\t\tif (task && pi_state->owner == task)\n\t\t\t\treturn -EDEADLK;\n\n\t\t\tatomic_inc(&pi_state->refcount);\n\t\t\t*ps = pi_state;\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0\n\t */\n\tif (!pid)\n\t\treturn -ESRCH;\n\tp = futex_find_get_task(pid);\n\tif (!p)\n\t\treturn -ESRCH;\n\n\tif (!p->mm) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state flags to figure out,\n\t * whether the task is exiting. To protect against the do_exit\n\t * change of the task flags, we do this protected by\n\t * p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->flags & PF_EXITING)) {\n\t\t/*\n\t\t * The task is on the way out. When PF_EXITPIDONE is\n\t\t * set, we know that the task has finished the\n\t\t * cleanup:\n\t\t */\n\t\tint ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\tpi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make 'p'\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\tpi_state->owner = p;\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\t*ps = pi_state;\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "lookup_pi_state(u32 uval, struct futex_hash_bucket *hb,\n\t\tunion futex_key *key, struct futex_pi_state **ps,\n\t\tstruct task_struct *task){\n\tstruct futex_pi_state *pi_state = NULL;\n\tstruct futex_q *this, *next;\n\tstruct task_struct *p;\n\tpid_t pid = uval & FUTEX_TID_MASK;\n\n\tplist_for_each_entry_safe(this, next, &hb->chain, list) {\n\t\tif (match_futex(&this->key, key)) {\n\t\t\t/*\n\t\t\t * Another waiter already exists - bump up\n\t\t\t * the refcount and return its pi_state:\n\t\t\t */\n\t\t\tpi_state = this->pi_state;\n\t\t\t/*\n\t\t\t * Userspace might have messed up non-PI and PI futexes\n\t\t\t */\n\t\t\tif (unlikely(!pi_state))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tWARN_ON(!atomic_read(&pi_state->refcount));\n\n\t\t\t/*\n\t\t\t * When pi_state->owner is NULL then the owner died\n\t\t\t * and another waiter is on the fly. pi_state->owner\n\t\t\t * is fixed up by the task which acquires\n\t\t\t * pi_state->rt_mutex.\n\t\t\t *\n\t\t\t * We do not check for pid == 0 which can happen when\n\t\t\t * the owner died and robust_list_exit() cleared the\n\t\t\t * TID.\n\t\t\t */\n\t\t\tif (pid && pi_state->owner) {\n\t\t\t\t/*\n\t\t\t\t * Bail out if user space manipulated the\n\t\t\t\t * futex value.\n\t\t\t\t */\n\t\t\t\tif (pid != task_pid_vnr(pi_state->owner))\n\t\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Protect against a corrupted uval. If uval\n\t\t\t * is 0x80000000 then pid is 0 and the waiter\n\t\t\t * bit is set. So the deadlock check in the\n\t\t\t * calling code has failed and we did not fall\n\t\t\t * into the check above due to !pid.\n\t\t\t */\n\t\t\tif (task && pi_state->owner == task)\n\t\t\t\treturn -EDEADLK;\n\n\t\t\tatomic_inc(&pi_state->refcount);\n\t\t\t*ps = pi_state;\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * We are the first waiter - try to look up the real owner and attach\n\t * the new pi_state to it, but bail out when TID = 0\n\t */\n\tif (!pid)\n\t\treturn -ESRCH;\n\tp = futex_find_get_task(pid);\n\tif (!p)\n\t\treturn -ESRCH;\n\n\tif (!p->mm) {\n\t\tput_task_struct(p);\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * We need to look at the task state flags to figure out,\n\t * whether the task is exiting. To protect against the do_exit\n\t * change of the task flags, we do this protected by\n\t * p->pi_lock:\n\t */\n\traw_spin_lock_irq(&p->pi_lock);\n\tif (unlikely(p->flags & PF_EXITING)) {\n\t\t/*\n\t\t * The task is on the way out. When PF_EXITPIDONE is\n\t\t * set, we know that the task has finished the\n\t\t * cleanup:\n\t\t */\n\t\tint ret = (p->flags & PF_EXITPIDONE) ? -ESRCH : -EAGAIN;\n\n\t\traw_spin_unlock_irq(&p->pi_lock);\n\t\tput_task_struct(p);\n\t\treturn ret;\n\t}\n\n\tpi_state = alloc_pi_state();\n\n\t/*\n\t * Initialize the pi_mutex in locked state and make 'p'\n\t * the owner of it:\n\t */\n\trt_mutex_init_proxy_locked(&pi_state->pi_mutex, p);\n\n\t/* Store the key for possible exit cleanups: */\n\tpi_state->key = *key;\n\n\tWARN_ON(!list_empty(&pi_state->list));\n\tlist_add(&pi_state->list, &p->pi_state_list);\n\tpi_state->owner = p;\n\traw_spin_unlock_irq(&p->pi_lock);\n\n\tput_task_struct(p);\n\n\t*ps = pi_state;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_011": {
        "vulnerable_code": {
            "Code": "__d_alloc(struct super_block *sb, const struct qstr *name){\n\tstruct dentry *dentry;\n\tchar *dname;\n\n\tdentry = kmem_cache_alloc(dentry_cache, GFP_KERNEL);\n\tif (!dentry)\n\t\treturn NULL;\n\n\t/*\n\t * We guarantee that the inline name is always NUL-terminated.\n\t * This way the memcpy() done by the name switching in rename\n\t * will still always have a NUL at the end, even if we might\n\t * be overwriting an internal NUL character\n\t */\n\tdentry->d_iname[DNAME_INLINE_LEN-1] = 0;\n\tif (name->len > DNAME_INLINE_LEN-1) {\n\t\tsize_t size = offsetof(struct external_name, name[1]);\n\t\tstruct external_name *p = kmalloc(size + name->len, GFP_KERNEL);\n\t\tif (!p) {\n\t\t\tkmem_cache_free(dentry_cache, dentry); \n\t\t\treturn NULL;\n\t\t}\n\t\tatomic_set(&p->u.count, 1);\n\t\tdname = p->name;\n\t} else  {\n\t\tdname = dentry->d_iname;\n\t}\t\n\n\tdentry->d_name.len = name->len;\n\tdentry->d_name.hash = name->hash;\n\tmemcpy(dname, name->name, name->len);\n\tdname[name->len] = 0;\n\n\t/* Make sure we always see the terminating NUL character */\n\tsmp_wmb();\n\tdentry->d_name.name = dname;\n\n\tdentry->d_lockref.count = 1;\n\tdentry->d_flags = 0;\n\tspin_lock_init(&dentry->d_lock);\n\tseqcount_init(&dentry->d_seq);\n\tdentry->d_inode = NULL;\n\tdentry->d_parent = dentry;\n\tdentry->d_sb = sb;\n\tdentry->d_op = NULL;\n\tdentry->d_fsdata = NULL;\n\tINIT_HLIST_BL_NODE(&dentry->d_hash);\n\tINIT_LIST_HEAD(&dentry->d_lru);\n\tINIT_LIST_HEAD(&dentry->d_subdirs);\n\tINIT_HLIST_NODE(&dentry->d_alias);\n\tINIT_LIST_HEAD(&dentry->d_u.d_child);\n\td_set_d_op(dentry, dentry->d_sb->s_d_op);\n\n\tthis_cpu_inc(nr_dentry);\n\n\treturn dentry;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_alloc(struct super_block *sb, const struct qstr *name){\n\tstruct dentry *dentry;\n\tchar *dname;\n\n\tdentry = kmem_cache_alloc(dentry_cache, GFP_KERNEL);\n\tif (!dentry)\n\t\treturn NULL;\n\n\t/*\n\t * We guarantee that the inline name is always NUL-terminated.\n\t * This way the memcpy() done by the name switching in rename\n\t * will still always have a NUL at the end, even if we might\n\t * be overwriting an internal NUL character\n\t */\n\tdentry->d_iname[DNAME_INLINE_LEN-1] = 0;\n\tif (name->len > DNAME_INLINE_LEN-1) {\n\t\tsize_t size = offsetof(struct external_name, name[1]);\n\t\tstruct external_name *p = kmalloc(size + name->len, GFP_KERNEL);\n\t\tif (!p) {\n\t\t\tkmem_cache_free(dentry_cache, dentry); \n\t\t\treturn NULL;\n\t\t}\n\t\tatomic_set(&p->u.count, 1);\n\t\tdname = p->name;\n\t} else  {\n\t\tdname = dentry->d_iname;\n\t}\t\n\n\tdentry->d_name.len = name->len;\n\tdentry->d_name.hash = name->hash;\n\tmemcpy(dname, name->name, name->len);\n\tdname[name->len] = 0;\n\n\t/* Make sure we always see the terminating NUL character */\n\tsmp_wmb();\n\tdentry->d_name.name = dname;\n\n\tdentry->d_lockref.count = 1;\n\tdentry->d_flags = 0;\n\tspin_lock_init(&dentry->d_lock);\n\tseqcount_init(&dentry->d_seq);\n\tdentry->d_inode = NULL;\n\tdentry->d_parent = dentry;\n\tdentry->d_sb = sb;\n\tdentry->d_op = NULL;\n\tdentry->d_fsdata = NULL;\n\tINIT_HLIST_BL_NODE(&dentry->d_hash);\n\tINIT_LIST_HEAD(&dentry->d_lru);\n\tINIT_LIST_HEAD(&dentry->d_subdirs);\n\tINIT_HLIST_NODE(&dentry->d_u.d_alias);\n\tINIT_LIST_HEAD(&dentry->d_child);\n\td_set_d_op(dentry, dentry->d_sb->s_d_op);\n\n\tthis_cpu_inc(nr_dentry);\n\n\treturn dentry;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_012": {
        "vulnerable_code": {
            "Code": "__d_find_alias(struct inode *inode){\n\tstruct dentry *alias, *discon_alias;\n\nagain:\n\tdiscon_alias = NULL;\n\thlist_for_each_entry(alias, &inode->i_dentry, d_alias) {\n\t\tspin_lock(&alias->d_lock);\n \t\tif (S_ISDIR(inode->i_mode) || !d_unhashed(alias)) {\n\t\t\tif (IS_ROOT(alias) &&\n\t\t\t    (alias->d_flags & DCACHE_DISCONNECTED)) {\n\t\t\t\tdiscon_alias = alias;\n\t\t\t} else {\n\t\t\t\t__dget_dlock(alias);\n\t\t\t\tspin_unlock(&alias->d_lock);\n\t\t\t\treturn alias;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&alias->d_lock);\n\t}\n\tif (discon_alias) {\n\t\talias = discon_alias;\n\t\tspin_lock(&alias->d_lock);\n\t\tif (S_ISDIR(inode->i_mode) || !d_unhashed(alias)) {\n\t\t\t__dget_dlock(alias);\n\t\t\tspin_unlock(&alias->d_lock);\n\t\t\treturn alias;\n\t\t}\n\t\tspin_unlock(&alias->d_lock);\n\t\tgoto again;\n\t}\n\treturn NULL;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_find_alias(struct inode *inode){\n\tstruct dentry *alias, *discon_alias;\n\nagain:\n\tdiscon_alias = NULL;\n\thlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) {\n\t\tspin_lock(&alias->d_lock);\n \t\tif (S_ISDIR(inode->i_mode) || !d_unhashed(alias)) {\n\t\t\tif (IS_ROOT(alias) &&\n\t\t\t    (alias->d_flags & DCACHE_DISCONNECTED)) {\n\t\t\t\tdiscon_alias = alias;\n\t\t\t} else {\n\t\t\t\t__dget_dlock(alias);\n\t\t\t\tspin_unlock(&alias->d_lock);\n\t\t\t\treturn alias;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&alias->d_lock);\n\t}\n\tif (discon_alias) {\n\t\talias = discon_alias;\n\t\tspin_lock(&alias->d_lock);\n\t\tif (S_ISDIR(inode->i_mode) || !d_unhashed(alias)) {\n\t\t\t__dget_dlock(alias);\n\t\t\tspin_unlock(&alias->d_lock);\n\t\t\treturn alias;\n\t\t}\n\t\tspin_unlock(&alias->d_lock);\n\t\tgoto again;\n\t}\n\treturn NULL;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_013": {
        "vulnerable_code": {
            "Code": "__d_find_any_alias(struct inode *inode){\n\tstruct dentry *alias;\n\n\tif (hlist_empty(&inode->i_dentry))\n\t\treturn NULL;\n\talias = hlist_entry(inode->i_dentry.first, struct dentry, d_alias);\n\t__dget(alias);\n\treturn alias;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_find_any_alias(struct inode *inode){\n\tstruct dentry *alias;\n\n\tif (hlist_empty(&inode->i_dentry))\n\t\treturn NULL;\n\talias = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);\n\t__dget(alias);\n\treturn alias;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_014": {
        "vulnerable_code": {
            "Code": "__d_free(struct rcu_head *head){\n\tstruct dentry *dentry = container_of(head, struct dentry, d_u.d_rcu);\n\n\tWARN_ON(!hlist_unhashed(&dentry->d_alias));\n\tkmem_cache_free(dentry_cache, dentry); \n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_free(struct rcu_head *head){\n\tstruct dentry *dentry = container_of(head, struct dentry, d_u.d_rcu);\n\n\tkmem_cache_free(dentry_cache, dentry); \n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_015": {
        "vulnerable_code": {
            "Code": "__d_free_external(struct rcu_head *head){\n\tstruct dentry *dentry = container_of(head, struct dentry, d_u.d_rcu);\n\tWARN_ON(!hlist_unhashed(&dentry->d_alias));\n\tkfree(external_name(dentry));\n\tkmem_cache_free(dentry_cache, dentry); \n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_free_external(struct rcu_head *head){\n\tstruct dentry *dentry = container_of(head, struct dentry, d_u.d_rcu);\n\tkfree(external_name(dentry));\n\tkmem_cache_free(dentry_cache, dentry); \n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_016": {
        "vulnerable_code": {
            "Code": "__d_instantiate(struct dentry *dentry, struct inode *inode){\n\tunsigned add_flags = d_flags_for_inode(inode);\n\n\tspin_lock(&dentry->d_lock);\n\t__d_set_type(dentry, add_flags);\n\tif (inode)\n\t\thlist_add_head(&dentry->d_alias, &inode->i_dentry);\n\tdentry->d_inode = inode;\n\tdentry_rcuwalk_barrier(dentry);\n\tspin_unlock(&dentry->d_lock);\n\tfsnotify_d_instantiate(dentry, inode);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_instantiate(struct dentry *dentry, struct inode *inode){\n\tunsigned add_flags = d_flags_for_inode(inode);\n\n\tspin_lock(&dentry->d_lock);\n\t__d_set_type(dentry, add_flags);\n\tif (inode)\n\t\thlist_add_head(&dentry->d_u.d_alias, &inode->i_dentry);\n\tdentry->d_inode = inode;\n\tdentry_rcuwalk_barrier(dentry);\n\tspin_unlock(&dentry->d_lock);\n\tfsnotify_d_instantiate(dentry, inode);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_017": {
        "vulnerable_code": {
            "Code": "__d_instantiate_unique(struct dentry *entry,\n\t\t\t\t\t     struct inode *inode){\n\tstruct dentry *alias;\n\tint len = entry->d_name.len;\n\tconst char *name = entry->d_name.name;\n\tunsigned int hash = entry->d_name.hash;\n\n\tif (!inode) {\n\t\t__d_instantiate(entry, NULL);\n\t\treturn NULL;\n\t}\n\n\thlist_for_each_entry(alias, &inode->i_dentry, d_alias) {\n\t\t/*\n\t\t * Don't need alias->d_lock here, because aliases with\n\t\t * d_parent == entry->d_parent are not subject to name or\n\t\t * parent changes, because the parent inode i_mutex is held.\n\t\t */\n\t\tif (alias->d_name.hash != hash)\n\t\t\tcontinue;\n\t\tif (alias->d_parent != entry->d_parent)\n\t\t\tcontinue;\n\t\tif (alias->d_name.len != len)\n\t\t\tcontinue;\n\t\tif (dentry_cmp(alias, name, len))\n\t\t\tcontinue;\n\t\t__dget(alias);\n\t\treturn alias;\n\t}\n\n\t__d_instantiate(entry, inode);\n\treturn NULL;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_instantiate_unique(struct dentry *entry,\n\t\t\t\t\t     struct inode *inode){\n\tstruct dentry *alias;\n\tint len = entry->d_name.len;\n\tconst char *name = entry->d_name.name;\n\tunsigned int hash = entry->d_name.hash;\n\n\tif (!inode) {\n\t\t__d_instantiate(entry, NULL);\n\t\treturn NULL;\n\t}\n\n\thlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) {\n\t\t/*\n\t\t * Don't need alias->d_lock here, because aliases with\n\t\t * d_parent == entry->d_parent are not subject to name or\n\t\t * parent changes, because the parent inode i_mutex is held.\n\t\t */\n\t\tif (alias->d_name.hash != hash)\n\t\t\tcontinue;\n\t\tif (alias->d_parent != entry->d_parent)\n\t\t\tcontinue;\n\t\tif (alias->d_name.len != len)\n\t\t\tcontinue;\n\t\tif (dentry_cmp(alias, name, len))\n\t\t\tcontinue;\n\t\t__dget(alias);\n\t\treturn alias;\n\t}\n\n\t__d_instantiate(entry, inode);\n\treturn NULL;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_018": {
        "vulnerable_code": {
            "Code": "__d_lookup(const struct dentry *parent, const struct qstr *name){\n\tunsigned int len = name->len;\n\tunsigned int hash = name->hash;\n\tconst unsigned char *str = name->name;\n\tstruct hlist_bl_head *b = d_hash(parent, hash);\n\tstruct hlist_bl_node *node;\n\tstruct dentry *found = NULL;\n\tstruct dentry *dentry;\n\n\t/*\n\t * Note: There is significant duplication with __d_lookup_rcu which is\n\t * required to prevent single threaded performance regressions\n\t * especially on architectures where smp_rmb (in seqcounts) are costly.\n\t * Keep the two functions in sync.\n\t */\n\n\t/*\n\t * The hash list is protected using RCU.\n\t *\n\t * Take d_lock when comparing a candidate dentry, to avoid races\n\t * with d_move().\n\t *\n\t * It is possible that concurrent renames can mess up our list\n\t * walk here and result in missing our dentry, resulting in the\n\t * false-negative result. d_lookup() protects against concurrent\n\t * renames using rename_lock seqlock.\n\t *\n\t * See Documentation/filesystems/path-lookup.txt for more details.\n\t */\n\trcu_read_lock();\n\t\n\thlist_bl_for_each_entry_rcu(dentry, node, b, d_hash) {\n\n\t\tif (dentry->d_name.hash != hash)\n\t\t\tcontinue;\n\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (dentry->d_parent != parent)\n\t\t\tgoto next;\n\t\tif (d_unhashed(dentry))\n\t\t\tgoto next;\n\n\t\t/*\n\t\t * It is safe to compare names since d_move() cannot\n\t\t * change the qstr (protected by d_lock).\n\t\t */\n\t\tif (parent->d_flags & DCACHE_OP_COMPARE) {\n\t\t\tint tlen = dentry->d_name.len;\n\t\t\tconst char *tname = dentry->d_name.name;\n\t\t\tif (parent->d_op->d_compare(parent, dentry, tlen, tname, name))\n\t\t\t\tgoto next;\n\t\t} else {\n\t\t\tif (dentry->d_name.len != len)\n\t\t\t\tgoto next;\n\t\t\tif (dentry_cmp(dentry, str, len))\n\t\t\t\tgoto next;\n\t\t}\n\n\t\tdentry->d_lockref.count++;\n\t\tfound = dentry;\n\t\tspin_unlock(&dentry->d_lock);\n\t\tbreak;\nnext:\n\t\tspin_unlock(&dentry->d_lock);\n \t}\n \trcu_read_unlock();\n\n \treturn found;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_lookup(const struct dentry *parent, const struct qstr *name){\n\tunsigned int len = name->len;\n\tunsigned int hash = name->hash;\n\tconst unsigned char *str = name->name;\n\tstruct hlist_bl_head *b = d_hash(parent, hash);\n\tstruct hlist_bl_node *node;\n\tstruct dentry *found = NULL;\n\tstruct dentry *dentry;\n\n\t/*\n\t * Note: There is significant duplication with __d_lookup_rcu which is\n\t * required to prevent single threaded performance regressions\n\t * especially on architectures where smp_rmb (in seqcounts) are costly.\n\t * Keep the two functions in sync.\n\t */\n\n\t/*\n\t * The hash list is protected using RCU.\n\t *\n\t * Take d_lock when comparing a candidate dentry, to avoid races\n\t * with d_move().\n\t *\n\t * It is possible that concurrent renames can mess up our list\n\t * walk here and result in missing our dentry, resulting in the\n\t * false-negative result. d_lookup() protects against concurrent\n\t * renames using rename_lock seqlock.\n\t *\n\t * See Documentation/filesystems/path-lookup.txt for more details.\n\t */\n\trcu_read_lock();\n\t\n\thlist_bl_for_each_entry_rcu(dentry, node, b, d_hash) {\n\n\t\tif (dentry->d_name.hash != hash)\n\t\t\tcontinue;\n\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (dentry->d_parent != parent)\n\t\t\tgoto next;\n\t\tif (d_unhashed(dentry))\n\t\t\tgoto next;\n\n\t\t/*\n\t\t * It is safe to compare names since d_move() cannot\n\t\t * change the qstr (protected by d_lock).\n\t\t */\n\t\tif (parent->d_flags & DCACHE_OP_COMPARE) {\n\t\t\tint tlen = dentry->d_name.len;\n\t\t\tconst char *tname = dentry->d_name.name;\n\t\t\tif (parent->d_op->d_compare(parent, dentry, tlen, tname, name))\n\t\t\t\tgoto next;\n\t\t} else {\n\t\t\tif (dentry->d_name.len != len)\n\t\t\t\tgoto next;\n\t\t\tif (dentry_cmp(dentry, str, len))\n\t\t\t\tgoto next;\n\t\t}\n\n\t\tdentry->d_lockref.count++;\n\t\tfound = dentry;\n\t\tspin_unlock(&dentry->d_lock);\n\t\tbreak;\nnext:\n\t\tspin_unlock(&dentry->d_lock);\n \t}\n \trcu_read_unlock();\n\n \treturn found;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_019": {
        "vulnerable_code": {
            "Code": "__d_move(struct dentry *dentry, struct dentry *target,\n\t\t     bool exchange){\n\tif (!dentry->d_inode)\n\t\tprintk(KERN_WARNING \"VFS: moving negative dcache entry\\n\");\n\n\tBUG_ON(d_ancestor(dentry, target));\n\tBUG_ON(d_ancestor(target, dentry));\n\n\tdentry_lock_for_move(dentry, target);\n\n\twrite_seqcount_begin(&dentry->d_seq);\n\twrite_seqcount_begin_nested(&target->d_seq, DENTRY_D_LOCK_NESTED);\n\n\t/* __d_drop does write_seqcount_barrier, but they're OK to nest. */\n\n\t/*\n\t * Move the dentry to the target hash queue. Don't bother checking\n\t * for the same hash queue because of how unlikely it is.\n\t */\n\t__d_drop(dentry);\n\t__d_rehash(dentry, d_hash(target->d_parent, target->d_name.hash));\n\n\t/*\n\t * Unhash the target (d_delete() is not usable here).  If exchanging\n\t * the two dentries, then rehash onto the other's hash queue.\n\t */\n\t__d_drop(target);\n\tif (exchange) {\n\t\t__d_rehash(target,\n\t\t\t   d_hash(dentry->d_parent, dentry->d_name.hash));\n\t}\n\n\t/* Switch the names.. */\n\tif (exchange)\n\t\tswap_names(dentry, target);\n\telse\n\t\tcopy_name(dentry, target);\n\n\t/* ... and switch them in the tree */\n\tif (IS_ROOT(dentry)) {\n\t\t/* splicing a tree */\n\t\tdentry->d_parent = target->d_parent;\n\t\ttarget->d_parent = target;\n\t\tlist_del_init(&target->d_u.d_child);\n\t\tlist_move(&dentry->d_u.d_child, &dentry->d_parent->d_subdirs);\n\t} else {\n\t\t/* swapping two dentries */\n\t\tswap(dentry->d_parent, target->d_parent);\n\t\tlist_move(&target->d_u.d_child, &target->d_parent->d_subdirs);\n\t\tlist_move(&dentry->d_u.d_child, &dentry->d_parent->d_subdirs);\n\t\tif (exchange)\n\t\t\tfsnotify_d_move(target);\n\t\tfsnotify_d_move(dentry);\n\t}\n\n\twrite_seqcount_end(&target->d_seq);\n\twrite_seqcount_end(&dentry->d_seq);\n\n\tdentry_unlock_for_move(dentry, target);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_move(struct dentry *dentry, struct dentry *target,\n\t\t     bool exchange){\n\tif (!dentry->d_inode)\n\t\tprintk(KERN_WARNING \"VFS: moving negative dcache entry\\n\");\n\n\tBUG_ON(d_ancestor(dentry, target));\n\tBUG_ON(d_ancestor(target, dentry));\n\n\tdentry_lock_for_move(dentry, target);\n\n\twrite_seqcount_begin(&dentry->d_seq);\n\twrite_seqcount_begin_nested(&target->d_seq, DENTRY_D_LOCK_NESTED);\n\n\t/* __d_drop does write_seqcount_barrier, but they're OK to nest. */\n\n\t/*\n\t * Move the dentry to the target hash queue. Don't bother checking\n\t * for the same hash queue because of how unlikely it is.\n\t */\n\t__d_drop(dentry);\n\t__d_rehash(dentry, d_hash(target->d_parent, target->d_name.hash));\n\n\t/*\n\t * Unhash the target (d_delete() is not usable here).  If exchanging\n\t * the two dentries, then rehash onto the other's hash queue.\n\t */\n\t__d_drop(target);\n\tif (exchange) {\n\t\t__d_rehash(target,\n\t\t\t   d_hash(dentry->d_parent, dentry->d_name.hash));\n\t}\n\n\t/* Switch the names.. */\n\tif (exchange)\n\t\tswap_names(dentry, target);\n\telse\n\t\tcopy_name(dentry, target);\n\n\t/* ... and switch them in the tree */\n\tif (IS_ROOT(dentry)) {\n\t\t/* splicing a tree */\n\t\tdentry->d_parent = target->d_parent;\n\t\ttarget->d_parent = target;\n\t\tlist_del_init(&target->d_child);\n\t\tlist_move(&dentry->d_child, &dentry->d_parent->d_subdirs);\n\t} else {\n\t\t/* swapping two dentries */\n\t\tswap(dentry->d_parent, target->d_parent);\n\t\tlist_move(&target->d_child, &target->d_parent->d_subdirs);\n\t\tlist_move(&dentry->d_child, &dentry->d_parent->d_subdirs);\n\t\tif (exchange)\n\t\t\tfsnotify_d_move(target);\n\t\tfsnotify_d_move(dentry);\n\t}\n\n\twrite_seqcount_end(&target->d_seq);\n\twrite_seqcount_end(&dentry->d_seq);\n\n\tdentry_unlock_for_move(dentry, target);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_020": {
        "vulnerable_code": {
            "Code": "__d_obtain_alias(struct inode *inode, int disconnected){\n\tstatic const struct qstr anonstring = QSTR_INIT(\"/\", 1);\n\tstruct dentry *tmp;\n\tstruct dentry *res;\n\tunsigned add_flags;\n\n\tif (!inode)\n\t\treturn ERR_PTR(-ESTALE);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\n\tres = d_find_any_alias(inode);\n\tif (res)\n\t\tgoto out_iput;\n\n\ttmp = __d_alloc(inode->i_sb, &anonstring);\n\tif (!tmp) {\n\t\tres = ERR_PTR(-ENOMEM);\n\t\tgoto out_iput;\n\t}\n\n\tspin_lock(&inode->i_lock);\n\tres = __d_find_any_alias(inode);\n\tif (res) {\n\t\tspin_unlock(&inode->i_lock);\n\t\tdput(tmp);\n\t\tgoto out_iput;\n\t}\n\n\t/* attach a disconnected dentry */\n\tadd_flags = d_flags_for_inode(inode);\n\n\tif (disconnected)\n\t\tadd_flags |= DCACHE_DISCONNECTED;\n\n\tspin_lock(&tmp->d_lock);\n\ttmp->d_inode = inode;\n\ttmp->d_flags |= add_flags;\n\thlist_add_head(&tmp->d_alias, &inode->i_dentry);\n\thlist_bl_lock(&tmp->d_sb->s_anon);\n\thlist_bl_add_head(&tmp->d_hash, &tmp->d_sb->s_anon);\n\thlist_bl_unlock(&tmp->d_sb->s_anon);\n\tspin_unlock(&tmp->d_lock);\n\tspin_unlock(&inode->i_lock);\n\tsecurity_d_instantiate(tmp, inode);\n\n\treturn tmp;\n\n out_iput:\n\tif (res && !IS_ERR(res))\n\t\tsecurity_d_instantiate(res, inode);\n\tiput(inode);\n\treturn res;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__d_obtain_alias(struct inode *inode, int disconnected){\n\tstatic const struct qstr anonstring = QSTR_INIT(\"/\", 1);\n\tstruct dentry *tmp;\n\tstruct dentry *res;\n\tunsigned add_flags;\n\n\tif (!inode)\n\t\treturn ERR_PTR(-ESTALE);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\n\tres = d_find_any_alias(inode);\n\tif (res)\n\t\tgoto out_iput;\n\n\ttmp = __d_alloc(inode->i_sb, &anonstring);\n\tif (!tmp) {\n\t\tres = ERR_PTR(-ENOMEM);\n\t\tgoto out_iput;\n\t}\n\n\tspin_lock(&inode->i_lock);\n\tres = __d_find_any_alias(inode);\n\tif (res) {\n\t\tspin_unlock(&inode->i_lock);\n\t\tdput(tmp);\n\t\tgoto out_iput;\n\t}\n\n\t/* attach a disconnected dentry */\n\tadd_flags = d_flags_for_inode(inode);\n\n\tif (disconnected)\n\t\tadd_flags |= DCACHE_DISCONNECTED;\n\n\tspin_lock(&tmp->d_lock);\n\ttmp->d_inode = inode;\n\ttmp->d_flags |= add_flags;\n\thlist_add_head(&tmp->d_u.d_alias, &inode->i_dentry);\n\thlist_bl_lock(&tmp->d_sb->s_anon);\n\thlist_bl_add_head(&tmp->d_hash, &tmp->d_sb->s_anon);\n\thlist_bl_unlock(&tmp->d_sb->s_anon);\n\tspin_unlock(&tmp->d_lock);\n\tspin_unlock(&inode->i_lock);\n\tsecurity_d_instantiate(tmp, inode);\n\n\treturn tmp;\n\n out_iput:\n\tif (res && !IS_ERR(res))\n\t\tsecurity_d_instantiate(res, inode);\n\tiput(inode);\n\treturn res;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_021": {
        "vulnerable_code": {
            "Code": "__dentry_kill(struct dentry *dentry){\n\tstruct dentry *parent = NULL;\n\tbool can_free = true;\n\tif (!IS_ROOT(dentry))\n\t\tparent = dentry->d_parent;\n\n\t/*\n\t * The dentry is now unrecoverably dead to the world.\n\t */\n\tlockref_mark_dead(&dentry->d_lockref);\n\n\t/*\n\t * inform the fs via d_prune that this dentry is about to be\n\t * unhashed and destroyed.\n\t */\n\tif (dentry->d_flags & DCACHE_OP_PRUNE)\n\t\tdentry->d_op->d_prune(dentry);\n\n\tif (dentry->d_flags & DCACHE_LRU_LIST) {\n\t\tif (!(dentry->d_flags & DCACHE_SHRINK_LIST))\n\t\t\td_lru_del(dentry);\n\t}\n\t/* if it was on the hash then remove it */\n\t__d_drop(dentry);\n\tlist_del(&dentry->d_u.d_child);\n\t/*\n\t * Inform d_walk() that we are no longer attached to the\n\t * dentry tree\n\t */\n\tdentry->d_flags |= DCACHE_DENTRY_KILLED;\n\tif (parent)\n\t\tspin_unlock(&parent->d_lock);\n\tdentry_iput(dentry);\n\t/*\n\t * dentry_iput drops the locks, at which point nobody (except\n\t * transient RCU lookups) can reach this dentry.\n\t */\n\tBUG_ON((int)dentry->d_lockref.count > 0);\n\tthis_cpu_dec(nr_dentry);\n\tif (dentry->d_op && dentry->d_op->d_release)\n\t\tdentry->d_op->d_release(dentry);\n\n\tspin_lock(&dentry->d_lock);\n\tif (dentry->d_flags & DCACHE_SHRINK_LIST) {\n\t\tdentry->d_flags |= DCACHE_MAY_FREE;\n\t\tcan_free = false;\n\t}\n\tspin_unlock(&dentry->d_lock);\n\tif (likely(can_free))\n\t\tdentry_free(dentry);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__dentry_kill(struct dentry *dentry){\n\tstruct dentry *parent = NULL;\n\tbool can_free = true;\n\tif (!IS_ROOT(dentry))\n\t\tparent = dentry->d_parent;\n\n\t/*\n\t * The dentry is now unrecoverably dead to the world.\n\t */\n\tlockref_mark_dead(&dentry->d_lockref);\n\n\t/*\n\t * inform the fs via d_prune that this dentry is about to be\n\t * unhashed and destroyed.\n\t */\n\tif (dentry->d_flags & DCACHE_OP_PRUNE)\n\t\tdentry->d_op->d_prune(dentry);\n\n\tif (dentry->d_flags & DCACHE_LRU_LIST) {\n\t\tif (!(dentry->d_flags & DCACHE_SHRINK_LIST))\n\t\t\td_lru_del(dentry);\n\t}\n\t/* if it was on the hash then remove it */\n\t__d_drop(dentry);\n\tlist_del(&dentry->d_child);\n\t/*\n\t * Inform d_walk() that we are no longer attached to the\n\t * dentry tree\n\t */\n\tdentry->d_flags |= DCACHE_DENTRY_KILLED;\n\tif (parent)\n\t\tspin_unlock(&parent->d_lock);\n\tdentry_iput(dentry);\n\t/*\n\t * dentry_iput drops the locks, at which point nobody (except\n\t * transient RCU lookups) can reach this dentry.\n\t */\n\tBUG_ON((int)dentry->d_lockref.count > 0);\n\tthis_cpu_dec(nr_dentry);\n\tif (dentry->d_op && dentry->d_op->d_release)\n\t\tdentry->d_op->d_release(dentry);\n\n\tspin_lock(&dentry->d_lock);\n\tif (dentry->d_flags & DCACHE_SHRINK_LIST) {\n\t\tdentry->d_flags |= DCACHE_MAY_FREE;\n\t\tcan_free = false;\n\t}\n\tspin_unlock(&dentry->d_lock);\n\tif (likely(can_free))\n\t\tdentry_free(dentry);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_022": {
        "vulnerable_code": {
            "Code": "__dentry_path(struct dentry *d, char *buf, int buflen){\n\tstruct dentry *dentry;\n\tchar *end, *retval;\n\tint len, seq = 0;\n\tint error = 0;\n\n\tif (buflen < 2)\n\t\tgoto Elong;\n\n\trcu_read_lock();\nrestart:\n\tdentry = d;\n\tend = buf + buflen;\n\tlen = buflen;\n\tprepend(&end, &len, \"\\0\", 1);\n\t/* Get '/' right */\n\tretval = end-1;\n\t*retval = '/';\n\tread_seqbegin_or_lock(&rename_lock, &seq);\n\twhile (!IS_ROOT(dentry)) {\n\t\tstruct dentry *parent = dentry->d_parent;\n\n\t\tprefetch(parent);\n\t\terror = prepend_name(&end, &len, &dentry->d_name);\n\t\tif (error)\n\t\t\tbreak;\n\n\t\tretval = end;\n\t\tdentry = parent;\n\t}\n\tif (!(seq & 1))\n\t\trcu_read_unlock();\n\tif (need_seqretry(&rename_lock, seq)) {\n\t\tseq = 1;\n\t\tgoto restart;\n\t}\n\tdone_seqretry(&rename_lock, seq);\n\tif (error)\n\t\tgoto Elong;\n\treturn retval;\nElong:\n\treturn ERR_PTR(-ENAMETOOLONG);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__dentry_path(struct dentry *d, char *buf, int buflen){\n\tstruct dentry *dentry;\n\tchar *end, *retval;\n\tint len, seq = 0;\n\tint error = 0;\n\n\tif (buflen < 2)\n\t\tgoto Elong;\n\n\trcu_read_lock();\nrestart:\n\tdentry = d;\n\tend = buf + buflen;\n\tlen = buflen;\n\tprepend(&end, &len, \"\\0\", 1);\n\t/* Get '/' right */\n\tretval = end-1;\n\t*retval = '/';\n\tread_seqbegin_or_lock(&rename_lock, &seq);\n\twhile (!IS_ROOT(dentry)) {\n\t\tstruct dentry *parent = dentry->d_parent;\n\n\t\tprefetch(parent);\n\t\terror = prepend_name(&end, &len, &dentry->d_name);\n\t\tif (error)\n\t\t\tbreak;\n\n\t\tretval = end;\n\t\tdentry = parent;\n\t}\n\tif (!(seq & 1))\n\t\trcu_read_unlock();\n\tif (need_seqretry(&rename_lock, seq)) {\n\t\tseq = 1;\n\t\tgoto restart;\n\t}\n\tdone_seqretry(&rename_lock, seq);\n\tif (error)\n\t\tgoto Elong;\n\treturn retval;\nElong:\n\treturn ERR_PTR(-ENAMETOOLONG);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_023": {
        "vulnerable_code": {
            "Code": "check_and_drop(void *_data){\n\tstruct detach_data *data = _data;\n\n\tif (!data->mountpoint && !data->select.found)\n\t\t__d_drop(data->select.start);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "check_and_drop(void *_data){\n\tstruct detach_data *data = _data;\n\n\tif (!data->mountpoint && !data->select.found)\n\t\t__d_drop(data->select.start);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_024": {
        "vulnerable_code": {
            "Code": "d_alloc(struct dentry * parent, const struct qstr *name){\n\tstruct dentry *dentry = __d_alloc(parent->d_sb, name);\n\tif (!dentry)\n\t\treturn NULL;\n\n\tspin_lock(&parent->d_lock);\n\t/*\n\t * don't need child lock because it is not subject\n\t * to concurrency here\n\t */\n\t__dget_dlock(parent);\n\tdentry->d_parent = parent;\n\tlist_add(&dentry->d_u.d_child, &parent->d_subdirs);\n\tspin_unlock(&parent->d_lock);\n\n\treturn dentry;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_alloc(struct dentry * parent, const struct qstr *name){\n\tstruct dentry *dentry = __d_alloc(parent->d_sb, name);\n\tif (!dentry)\n\t\treturn NULL;\n\n\tspin_lock(&parent->d_lock);\n\t/*\n\t * don't need child lock because it is not subject\n\t * to concurrency here\n\t */\n\t__dget_dlock(parent);\n\tdentry->d_parent = parent;\n\tlist_add(&dentry->d_child, &parent->d_subdirs);\n\tspin_unlock(&parent->d_lock);\n\n\treturn dentry;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_025": {
        "vulnerable_code": {
            "Code": "d_hash_and_lookup(struct dentry *dir, struct qstr *name){\n\t/*\n\t * Check for a fs-specific hash function. Note that we must\n\t * calculate the standard hash first, as the d_op->d_hash()\n\t * routine may choose to leave the hash value unchanged.\n\t */\n\tname->hash = full_name_hash(name->name, name->len);\n\tif (dir->d_flags & DCACHE_OP_HASH) {\n\t\tint err = dir->d_op->d_hash(dir, name);\n\t\tif (unlikely(err < 0))\n\t\t\treturn ERR_PTR(err);\n\t}\n\treturn d_lookup(dir, name);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_hash_and_lookup(struct dentry *dir, struct qstr *name){\n\t/*\n\t * Check for a fs-specific hash function. Note that we must\n\t * calculate the standard hash first, as the d_op->d_hash()\n\t * routine may choose to leave the hash value unchanged.\n\t */\n\tname->hash = full_name_hash(name->name, name->len);\n\tif (dir->d_flags & DCACHE_OP_HASH) {\n\t\tint err = dir->d_op->d_hash(dir, name);\n\t\tif (unlikely(err < 0))\n\t\t\treturn ERR_PTR(err);\n\t}\n\treturn d_lookup(dir, name);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_026": {
        "vulnerable_code": {
            "Code": "d_instantiate(struct dentry *entry, struct inode * inode){\n\tBUG_ON(!hlist_unhashed(&entry->d_alias));\n\tif (inode)\n\t\tspin_lock(&inode->i_lock);\n\t__d_instantiate(entry, inode);\n\tif (inode)\n\t\tspin_unlock(&inode->i_lock);\n\tsecurity_d_instantiate(entry, inode);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_instantiate(struct dentry *entry, struct inode * inode){\n\tBUG_ON(!hlist_unhashed(&entry->d_u.d_alias));\n\tif (inode)\n\t\tspin_lock(&inode->i_lock);\n\t__d_instantiate(entry, inode);\n\tif (inode)\n\t\tspin_unlock(&inode->i_lock);\n\tsecurity_d_instantiate(entry, inode);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_027": {
        "vulnerable_code": {
            "Code": "d_instantiate_no_diralias(struct dentry *entry, struct inode *inode){\n\tBUG_ON(!hlist_unhashed(&entry->d_alias));\n\n\tspin_lock(&inode->i_lock);\n\tif (S_ISDIR(inode->i_mode) && !hlist_empty(&inode->i_dentry)) {\n\t\tspin_unlock(&inode->i_lock);\n\t\tiput(inode);\n\t\treturn -EBUSY;\n\t}\n\t__d_instantiate(entry, inode);\n\tspin_unlock(&inode->i_lock);\n\tsecurity_d_instantiate(entry, inode);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_instantiate_no_diralias(struct dentry *entry, struct inode *inode){\n\tBUG_ON(!hlist_unhashed(&entry->d_u.d_alias));\n\n\tspin_lock(&inode->i_lock);\n\tif (S_ISDIR(inode->i_mode) && !hlist_empty(&inode->i_dentry)) {\n\t\tspin_unlock(&inode->i_lock);\n\t\tiput(inode);\n\t\treturn -EBUSY;\n\t}\n\t__d_instantiate(entry, inode);\n\tspin_unlock(&inode->i_lock);\n\tsecurity_d_instantiate(entry, inode);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_028": {
        "vulnerable_code": {
            "Code": "d_instantiate_unique(struct dentry *entry, struct inode *inode){\n\tstruct dentry *result;\n\n\tBUG_ON(!hlist_unhashed(&entry->d_alias));\n\n\tif (inode)\n\t\tspin_lock(&inode->i_lock);\n\tresult = __d_instantiate_unique(entry, inode);\n\tif (inode)\n\t\tspin_unlock(&inode->i_lock);\n\n\tif (!result) {\n\t\tsecurity_d_instantiate(entry, inode);\n\t\treturn NULL;\n\t}\n\n\tBUG_ON(!d_unhashed(result));\n\tiput(inode);\n\treturn result;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_instantiate_unique(struct dentry *entry, struct inode *inode){\n\tstruct dentry *result;\n\n\tBUG_ON(!hlist_unhashed(&entry->d_u.d_alias));\n\n\tif (inode)\n\t\tspin_lock(&inode->i_lock);\n\tresult = __d_instantiate_unique(entry, inode);\n\tif (inode)\n\t\tspin_unlock(&inode->i_lock);\n\n\tif (!result) {\n\t\tsecurity_d_instantiate(entry, inode);\n\t\treturn NULL;\n\t}\n\n\tBUG_ON(!d_unhashed(result));\n\tiput(inode);\n\treturn result;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_029": {
        "vulnerable_code": {
            "Code": "d_lru_shrink_move(struct dentry *dentry, struct list_head *list){\n\tD_FLAG_VERIFY(dentry, DCACHE_LRU_LIST);\n\tdentry->d_flags |= DCACHE_SHRINK_LIST;\n\tlist_move_tail(&dentry->d_lru, list);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_lru_shrink_move(struct dentry *dentry, struct list_head *list){\n\tD_FLAG_VERIFY(dentry, DCACHE_LRU_LIST);\n\tdentry->d_flags |= DCACHE_SHRINK_LIST;\n\tlist_move_tail(&dentry->d_lru, list);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_030": {
        "vulnerable_code": {
            "Code": "d_prune_aliases(struct inode *inode){\n\tstruct dentry *dentry;\nrestart:\n\tspin_lock(&inode->i_lock);\n\thlist_for_each_entry(dentry, &inode->i_dentry, d_alias) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!dentry->d_lockref.count) {\n\t\t\tstruct dentry *parent = lock_parent(dentry);\n\t\t\tif (likely(!dentry->d_lockref.count)) {\n\t\t\t\t__dentry_kill(dentry);\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t\tif (parent)\n\t\t\t\tspin_unlock(&parent->d_lock);\n\t\t}\n\t\tspin_unlock(&dentry->d_lock);\n\t}\n\tspin_unlock(&inode->i_lock);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_prune_aliases(struct inode *inode){\n\tstruct dentry *dentry;\nrestart:\n\tspin_lock(&inode->i_lock);\n\thlist_for_each_entry(dentry, &inode->i_dentry, d_u.d_alias) {\n\t\tspin_lock(&dentry->d_lock);\n\t\tif (!dentry->d_lockref.count) {\n\t\t\tstruct dentry *parent = lock_parent(dentry);\n\t\t\tif (likely(!dentry->d_lockref.count)) {\n\t\t\t\t__dentry_kill(dentry);\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t\tif (parent)\n\t\t\t\tspin_unlock(&parent->d_lock);\n\t\t}\n\t\tspin_unlock(&dentry->d_lock);\n\t}\n\tspin_unlock(&inode->i_lock);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_031": {
        "vulnerable_code": {
            "Code": "d_shrink_add(struct dentry *dentry, struct list_head *list){\n\tD_FLAG_VERIFY(dentry, 0);\n\tlist_add(&dentry->d_lru, list);\n\tdentry->d_flags |= DCACHE_SHRINK_LIST | DCACHE_LRU_LIST;\n\tthis_cpu_inc(nr_dentry_unused);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_shrink_add(struct dentry *dentry, struct list_head *list){\n\tD_FLAG_VERIFY(dentry, 0);\n\tlist_add(&dentry->d_lru, list);\n\tdentry->d_flags |= DCACHE_SHRINK_LIST | DCACHE_LRU_LIST;\n\tthis_cpu_inc(nr_dentry_unused);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_032": {
        "vulnerable_code": {
            "Code": "d_tmpfile(struct dentry *dentry, struct inode *inode){\n\tinode_dec_link_count(inode);\n\tBUG_ON(dentry->d_name.name != dentry->d_iname ||\n\t\t!hlist_unhashed(&dentry->d_alias) ||\n\t\t!d_unlinked(dentry));\n\tspin_lock(&dentry->d_parent->d_lock);\n\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\tdentry->d_name.len = sprintf(dentry->d_iname, \"#%llu\",\n\t\t\t\t(unsigned long long)inode->i_ino);\n\tspin_unlock(&dentry->d_lock);\n\tspin_unlock(&dentry->d_parent->d_lock);\n\td_instantiate(dentry, inode);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_tmpfile(struct dentry *dentry, struct inode *inode){\n\tinode_dec_link_count(inode);\n\tBUG_ON(dentry->d_name.name != dentry->d_iname ||\n\t\t!hlist_unhashed(&dentry->d_u.d_alias) ||\n\t\t!d_unlinked(dentry));\n\tspin_lock(&dentry->d_parent->d_lock);\n\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\tdentry->d_name.len = sprintf(dentry->d_iname, \"#%llu\",\n\t\t\t\t(unsigned long long)inode->i_ino);\n\tspin_unlock(&dentry->d_lock);\n\tspin_unlock(&dentry->d_parent->d_lock);\n\td_instantiate(dentry, inode);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_033": {
        "vulnerable_code": {
            "Code": "d_validate(struct dentry *dentry, struct dentry *dparent){\n\tstruct dentry *child;\n\n\tspin_lock(&dparent->d_lock);\n\tlist_for_each_entry(child, &dparent->d_subdirs, d_u.d_child) {\n\t\tif (dentry == child) {\n\t\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\t\t\t__dget_dlock(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tspin_unlock(&dparent->d_lock);\n\t\t\treturn 1;\n\t\t}\n\t}\n\tspin_unlock(&dparent->d_lock);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "d_validate(struct dentry *dentry, struct dentry *dparent){\n\tstruct dentry *child;\n\n\tspin_lock(&dparent->d_lock);\n\tlist_for_each_entry(child, &dparent->d_subdirs, d_child) {\n\t\tif (dentry == child) {\n\t\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\t\t\t__dget_dlock(dentry);\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tspin_unlock(&dparent->d_lock);\n\t\t\treturn 1;\n\t\t}\n\t}\n\tspin_unlock(&dparent->d_lock);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_034": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_035": {
        "vulnerable_code": {
            "Code": "dentry_free(struct dentry *dentry){\n\tif (unlikely(dname_external(dentry))) {\n\t\tstruct external_name *p = external_name(dentry);\n\t\tif (likely(atomic_dec_and_test(&p->u.count))) {\n\t\t\tcall_rcu(&dentry->d_u.d_rcu, __d_free_external);\n\t\t\treturn;\n\t\t}\n\t}\n\t/* if dentry was never visible to RCU, immediate free is OK */\n\tif (!(dentry->d_flags & DCACHE_RCUACCESS))\n\t\t__d_free(&dentry->d_u.d_rcu);\n\telse\n\t\tcall_rcu(&dentry->d_u.d_rcu, __d_free);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "dentry_free(struct dentry *dentry){\n\tWARN_ON(!hlist_unhashed(&dentry->d_u.d_alias));\n\tif (unlikely(dname_external(dentry))) {\n\t\tstruct external_name *p = external_name(dentry);\n\t\tif (likely(atomic_dec_and_test(&p->u.count))) {\n\t\t\tcall_rcu(&dentry->d_u.d_rcu, __d_free_external);\n\t\t\treturn;\n\t\t}\n\t}\n\t/* if dentry was never visible to RCU, immediate free is OK */\n\tif (!(dentry->d_flags & DCACHE_RCUACCESS))\n\t\t__d_free(&dentry->d_u.d_rcu);\n\telse\n\t\tcall_rcu(&dentry->d_u.d_rcu, __d_free);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_036": {
        "vulnerable_code": {
            "Code": "dentry_lru_isolate(struct list_head *item, spinlock_t *lru_lock, void *arg){\n\tstruct list_head *freeable = arg;\n\tstruct dentry\t*dentry = container_of(item, struct dentry, d_lru);\n\n\n\t/*\n\t * we are inverting the lru lock/dentry->d_lock here,\n\t * so use a trylock. If we fail to get the lock, just skip\n\t * it\n\t */\n\tif (!spin_trylock(&dentry->d_lock))\n\t\treturn LRU_SKIP;\n\n\t/*\n\t * Referenced dentries are still in use. If they have active\n\t * counts, just remove them from the LRU. Otherwise give them\n\t * another pass through the LRU.\n\t */\n\tif (dentry->d_lockref.count) {\n\t\td_lru_isolate(dentry);\n\t\tspin_unlock(&dentry->d_lock);\n\t\treturn LRU_REMOVED;\n\t}\n\n\tif (dentry->d_flags & DCACHE_REFERENCED) {\n\t\tdentry->d_flags &= ~DCACHE_REFERENCED;\n\t\tspin_unlock(&dentry->d_lock);\n\n\t\t/*\n\t\t * The list move itself will be made by the common LRU code. At\n\t\t * this point, we've dropped the dentry->d_lock but keep the\n\t\t * lru lock. This is safe to do, since every list movement is\n\t\t * protected by the lru lock even if both locks are held.\n\t\t *\n\t\t * This is guaranteed by the fact that all LRU management\n\t\t * functions are intermediated by the LRU API calls like\n\t\t * list_lru_add and list_lru_del. List movement in this file\n\t\t * only ever occur through this functions or through callbacks\n\t\t * like this one, that are called from the LRU API.\n\t\t *\n\t\t * The only exceptions to this are functions like\n\t\t * shrink_dentry_list, and code that first checks for the\n\t\t * DCACHE_SHRINK_LIST flag.  Those are guaranteed to be\n\t\t * operating only with stack provided lists after they are\n\t\t * properly isolated from the main list.  It is thus, always a\n\t\t * local access.\n\t\t */\n\t\treturn LRU_ROTATE;\n\t}\n\n\td_lru_shrink_move(dentry, freeable);\n\tspin_unlock(&dentry->d_lock);\n\n\treturn LRU_REMOVED;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "dentry_lru_isolate(struct list_head *item, spinlock_t *lru_lock, void *arg){\n\tstruct list_head *freeable = arg;\n\tstruct dentry\t*dentry = container_of(item, struct dentry, d_lru);\n\n\n\t/*\n\t * we are inverting the lru lock/dentry->d_lock here,\n\t * so use a trylock. If we fail to get the lock, just skip\n\t * it\n\t */\n\tif (!spin_trylock(&dentry->d_lock))\n\t\treturn LRU_SKIP;\n\n\t/*\n\t * Referenced dentries are still in use. If they have active\n\t * counts, just remove them from the LRU. Otherwise give them\n\t * another pass through the LRU.\n\t */\n\tif (dentry->d_lockref.count) {\n\t\td_lru_isolate(dentry);\n\t\tspin_unlock(&dentry->d_lock);\n\t\treturn LRU_REMOVED;\n\t}\n\n\tif (dentry->d_flags & DCACHE_REFERENCED) {\n\t\tdentry->d_flags &= ~DCACHE_REFERENCED;\n\t\tspin_unlock(&dentry->d_lock);\n\n\t\t/*\n\t\t * The list move itself will be made by the common LRU code. At\n\t\t * this point, we've dropped the dentry->d_lock but keep the\n\t\t * lru lock. This is safe to do, since every list movement is\n\t\t * protected by the lru lock even if both locks are held.\n\t\t *\n\t\t * This is guaranteed by the fact that all LRU management\n\t\t * functions are intermediated by the LRU API calls like\n\t\t * list_lru_add and list_lru_del. List movement in this file\n\t\t * only ever occur through this functions or through callbacks\n\t\t * like this one, that are called from the LRU API.\n\t\t *\n\t\t * The only exceptions to this are functions like\n\t\t * shrink_dentry_list, and code that first checks for the\n\t\t * DCACHE_SHRINK_LIST flag.  Those are guaranteed to be\n\t\t * operating only with stack provided lists after they are\n\t\t * properly isolated from the main list.  It is thus, always a\n\t\t * local access.\n\t\t */\n\t\treturn LRU_ROTATE;\n\t}\n\n\td_lru_shrink_move(dentry, freeable);\n\tspin_unlock(&dentry->d_lock);\n\n\treturn LRU_REMOVED;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_037": {
        "vulnerable_code": {
            "Code": "external_name(struct dentry *dentry){\n\treturn container_of(dentry->d_name.name, struct external_name, name[0]);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "external_name(struct dentry *dentry){\n\treturn container_of(dentry->d_name.name, struct external_name, name[0]);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_038": {
        "vulnerable_code": {
            "Code": "lock_parent(struct dentry *dentry){\n\tstruct dentry *parent = dentry->d_parent;\n\tif (IS_ROOT(dentry))\n\t\treturn NULL;\n\tif (unlikely((int)dentry->d_lockref.count < 0))\n\t\treturn NULL;\n\tif (likely(spin_trylock(&parent->d_lock)))\n\t\treturn parent;\n\trcu_read_lock();\n\tspin_unlock(&dentry->d_lock);\nagain:\n\tparent = ACCESS_ONCE(dentry->d_parent);\n\tspin_lock(&parent->d_lock);\n\t/*\n\t * We can't blindly lock dentry until we are sure\n\t * that we won't violate the locking order.\n\t * Any changes of dentry->d_parent must have\n\t * been done with parent->d_lock held, so\n\t * spin_lock() above is enough of a barrier\n\t * for checking if it's still our child.\n\t */\n\tif (unlikely(parent != dentry->d_parent)) {\n\t\tspin_unlock(&parent->d_lock);\n\t\tgoto again;\n\t}\n\trcu_read_unlock();\n\tif (parent != dentry)\n\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\telse\n\t\tparent = NULL;\n\treturn parent;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "lock_parent(struct dentry *dentry){\n\tstruct dentry *parent = dentry->d_parent;\n\tif (IS_ROOT(dentry))\n\t\treturn NULL;\n\tif (unlikely((int)dentry->d_lockref.count < 0))\n\t\treturn NULL;\n\tif (likely(spin_trylock(&parent->d_lock)))\n\t\treturn parent;\n\trcu_read_lock();\n\tspin_unlock(&dentry->d_lock);\nagain:\n\tparent = ACCESS_ONCE(dentry->d_parent);\n\tspin_lock(&parent->d_lock);\n\t/*\n\t * We can't blindly lock dentry until we are sure\n\t * that we won't violate the locking order.\n\t * Any changes of dentry->d_parent must have\n\t * been done with parent->d_lock held, so\n\t * spin_lock() above is enough of a barrier\n\t * for checking if it's still our child.\n\t */\n\tif (unlikely(parent != dentry->d_parent)) {\n\t\tspin_unlock(&parent->d_lock);\n\t\tgoto again;\n\t}\n\trcu_read_unlock();\n\tif (parent != dentry)\n\t\tspin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);\n\telse\n\t\tparent = NULL;\n\treturn parent;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_039": {
        "vulnerable_code": {
            "Code": "simple_dname(struct dentry *dentry, char *buffer, int buflen){\n\tchar *end = buffer + buflen;\n\t/* these dentries are never renamed, so d_lock is not needed */\n\tif (prepend(&end, &buflen, \" (deleted)\", 11) ||\n\t    prepend(&end, &buflen, dentry->d_name.name, dentry->d_name.len) ||\n\t    prepend(&end, &buflen, \"/\", 1))  \n\t\tend = ERR_PTR(-ENAMETOOLONG);\n\treturn end;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "simple_dname(struct dentry *dentry, char *buffer, int buflen){\n\tchar *end = buffer + buflen;\n\t/* these dentries are never renamed, so d_lock is not needed */\n\tif (prepend(&end, &buflen, \" (deleted)\", 11) ||\n\t    prepend(&end, &buflen, dentry->d_name.name, dentry->d_name.len) ||\n\t    prepend(&end, &buflen, \"/\", 1))  \n\t\tend = ERR_PTR(-ENAMETOOLONG);\n\treturn end;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_040": {
        "vulnerable_code": {
            "Code": "snd_timer_close(struct snd_timer_instance *timeri){\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\tmutex_lock(&register_mutex);\n\tlist_del(&timeri->open_list);\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\ttimer = timeri->timer;\n\tif (timer) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\n\t\t/* remove slave links */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\tspin_lock(&timer->lock);\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tlist_del_init(&slave->ack_list);\n\t\t\tlist_del_init(&slave->active_list);\n\t\t}\n\t\tspin_unlock(&timer->lock);\n\t\tspin_unlock_irq(&slave_active_lock);\n\n\t\t/* slave doesn't need to release timer resources below */\n\t\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)\n\t\t\ttimer = NULL;\n\t}\n\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\n\tif (timer) {\n\t\tif (list_empty(&timer->open_list_head) && timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* release a card refcount for safe disconnection */\n\t\tif (timer->card)\n\t\t\tput_device(&timer->card->card_dev);\n\t\tmodule_put(timer->module);\n\t}\n\n\tmutex_unlock(&register_mutex);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_close(struct snd_timer_instance *timeri){\n\tstruct snd_timer *timer = NULL;\n\tstruct snd_timer_instance *slave, *tmp;\n\n\tif (snd_BUG_ON(!timeri))\n\t\treturn -ENXIO;\n\n\tmutex_lock(&register_mutex);\n\tlist_del(&timeri->open_list);\n\n\t/* force to stop the timer */\n\tsnd_timer_stop(timeri);\n\n\ttimer = timeri->timer;\n\tif (timer) {\n\t\t/* wait, until the active callback is finished */\n\t\tspin_lock_irq(&timer->lock);\n\t\twhile (timeri->flags & SNDRV_TIMER_IFLG_CALLBACK) {\n\t\t\tspin_unlock_irq(&timer->lock);\n\t\t\tudelay(10);\n\t\t\tspin_lock_irq(&timer->lock);\n\t\t}\n\t\tspin_unlock_irq(&timer->lock);\n\n\t\t/* remove slave links */\n\t\tspin_lock_irq(&slave_active_lock);\n\t\tspin_lock(&timer->lock);\n\t\tlist_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,\n\t\t\t\t\t open_list) {\n\t\t\tlist_move_tail(&slave->open_list, &snd_timer_slave_list);\n\t\t\tslave->master = NULL;\n\t\t\tslave->timer = NULL;\n\t\t\tlist_del_init(&slave->ack_list);\n\t\t\tlist_del_init(&slave->active_list);\n\t\t}\n\t\tspin_unlock(&timer->lock);\n\t\tspin_unlock_irq(&slave_active_lock);\n\n\t\t/* slave doesn't need to release timer resources below */\n\t\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)\n\t\t\ttimer = NULL;\n\t}\n\n\tif (timeri->private_free)\n\t\ttimeri->private_free(timeri);\n\tkfree(timeri->owner);\n\tkfree(timeri);\n\n\tif (timer) {\n\t\tif (list_empty(&timer->open_list_head) && timer->hw.close)\n\t\t\ttimer->hw.close(timer);\n\t\t/* release a card refcount for safe disconnection */\n\t\tif (timer->card)\n\t\t\tput_device(&timer->card->card_dev);\n\t\tmodule_put(timer->module);\n\t}\n\n\tmutex_unlock(&register_mutex);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_041": {
        "vulnerable_code": {
            "Code": "snd_timer_dev_disconnect(struct snd_device *device){\n\tstruct snd_timer *timer = device->device_data;\n\tstruct snd_timer_instance *ti;\n\n\tmutex_lock(&register_mutex);\n\tlist_del_init(&timer->device_list);\n\t/* wake up pending sleepers */\n\tlist_for_each_entry(ti, &timer->open_list_head, open_list) {\n\t\tif (ti->disconnect)\n\t\t\tti->disconnect(ti);\n\t}\n\tmutex_unlock(&register_mutex);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_dev_disconnect(struct snd_device *device){\n\tstruct snd_timer *timer = device->device_data;\n\tstruct snd_timer_instance *ti;\n\n\tmutex_lock(&register_mutex);\n\tlist_del_init(&timer->device_list);\n\t/* wake up pending sleepers */\n\tlist_for_each_entry(ti, &timer->open_list_head, open_list) {\n\t\tif (ti->disconnect)\n\t\t\tti->disconnect(ti);\n\t}\n\tmutex_unlock(&register_mutex);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_042": {
        "vulnerable_code": {
            "Code": "snd_timer_global_free(struct snd_timer *timer){\n\treturn snd_timer_free(timer);\n}",
            "Size": 4,
            "Code Complexity": 1,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_global_free(struct snd_timer *timer){\n\treturn snd_timer_free(timer);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_043": {
        "vulnerable_code": {
            "Code": "snd_timer_instance_new(char *owner,\n\t\t\t\t\t\t\t struct snd_timer *timer){\n\tstruct snd_timer_instance *timeri;\n\ttimeri = kzalloc(sizeof(*timeri), GFP_KERNEL);\n\tif (timeri == NULL)\n\t\treturn NULL;\n\ttimeri->owner = kstrdup(owner, GFP_KERNEL);\n\tif (! timeri->owner) {\n\t\tkfree(timeri);\n\t\treturn NULL;\n\t}\n\tINIT_LIST_HEAD(&timeri->open_list);\n\tINIT_LIST_HEAD(&timeri->active_list);\n\tINIT_LIST_HEAD(&timeri->ack_list);\n\tINIT_LIST_HEAD(&timeri->slave_list_head);\n\tINIT_LIST_HEAD(&timeri->slave_active_head);\n\n\ttimeri->timer = timer;\n\tif (timer && !try_module_get(timer->module)) {\n\t\tkfree(timeri->owner);\n\t\tkfree(timeri);\n\t\treturn NULL;\n\t}\n\n\treturn timeri;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_instance_new(char *owner,\n\t\t\t\t\t\t\t struct snd_timer *timer){\n\tstruct snd_timer_instance *timeri;\n\ttimeri = kzalloc(sizeof(*timeri), GFP_KERNEL);\n\tif (timeri == NULL)\n\t\treturn NULL;\n\ttimeri->owner = kstrdup(owner, GFP_KERNEL);\n\tif (! timeri->owner) {\n\t\tkfree(timeri);\n\t\treturn NULL;\n\t}\n\tINIT_LIST_HEAD(&timeri->open_list);\n\tINIT_LIST_HEAD(&timeri->active_list);\n\tINIT_LIST_HEAD(&timeri->ack_list);\n\tINIT_LIST_HEAD(&timeri->slave_list_head);\n\tINIT_LIST_HEAD(&timeri->slave_active_head);\n\n\ttimeri->timer = timer;\n\tif (timer && !try_module_get(timer->module)) {\n\t\tkfree(timeri->owner);\n\t\tkfree(timeri);\n\t\treturn NULL;\n\t}\n\n\treturn timeri;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_044": {
        "vulnerable_code": {
            "Code": "snd_timer_new(struct snd_card *card, char *id, struct snd_timer_id *tid,\n\t\t  struct snd_timer **rtimer){\n\tstruct snd_timer *timer;\n\tint err;\n\tstatic struct snd_device_ops ops = {\n\t\t.dev_free = snd_timer_dev_free,\n\t\t.dev_register = snd_timer_dev_register,\n\t\t.dev_disconnect = snd_timer_dev_disconnect,\n\t};\n\n\tif (snd_BUG_ON(!tid))\n\t\treturn -EINVAL;\n\tif (rtimer)\n\t\t*rtimer = NULL;\n\ttimer = kzalloc(sizeof(*timer), GFP_KERNEL);\n\tif (!timer)\n\t\treturn -ENOMEM;\n\ttimer->tmr_class = tid->dev_class;\n\ttimer->card = card;\n\ttimer->tmr_device = tid->device;\n\ttimer->tmr_subdevice = tid->subdevice;\n\tif (id)\n\t\tstrlcpy(timer->id, id, sizeof(timer->id));\n\tINIT_LIST_HEAD(&timer->device_list);\n\tINIT_LIST_HEAD(&timer->open_list_head);\n\tINIT_LIST_HEAD(&timer->active_list_head);\n\tINIT_LIST_HEAD(&timer->ack_list_head);\n\tINIT_LIST_HEAD(&timer->sack_list_head);\n\tspin_lock_init(&timer->lock);\n\ttasklet_init(&timer->task_queue, snd_timer_tasklet,\n\t\t     (unsigned long)timer);\n\tif (card != NULL) {\n\t\ttimer->module = card->module;\n\t\terr = snd_device_new(card, SNDRV_DEV_TIMER, timer, &ops);\n\t\tif (err < 0) {\n\t\t\tsnd_timer_free(timer);\n\t\t\treturn err;\n\t\t}\n\t}\n\tif (rtimer)\n\t\t*rtimer = timer;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_new(struct snd_card *card, char *id, struct snd_timer_id *tid,\n\t\t  struct snd_timer **rtimer){\n\tstruct snd_timer *timer;\n\tint err;\n\tstatic struct snd_device_ops ops = {\n\t\t.dev_free = snd_timer_dev_free,\n\t\t.dev_register = snd_timer_dev_register,\n\t\t.dev_disconnect = snd_timer_dev_disconnect,\n\t};\n\n\tif (snd_BUG_ON(!tid))\n\t\treturn -EINVAL;\n\tif (rtimer)\n\t\t*rtimer = NULL;\n\ttimer = kzalloc(sizeof(*timer), GFP_KERNEL);\n\tif (!timer)\n\t\treturn -ENOMEM;\n\ttimer->tmr_class = tid->dev_class;\n\ttimer->card = card;\n\ttimer->tmr_device = tid->device;\n\ttimer->tmr_subdevice = tid->subdevice;\n\tif (id)\n\t\tstrlcpy(timer->id, id, sizeof(timer->id));\n\tINIT_LIST_HEAD(&timer->device_list);\n\tINIT_LIST_HEAD(&timer->open_list_head);\n\tINIT_LIST_HEAD(&timer->active_list_head);\n\tINIT_LIST_HEAD(&timer->ack_list_head);\n\tINIT_LIST_HEAD(&timer->sack_list_head);\n\tspin_lock_init(&timer->lock);\n\ttasklet_init(&timer->task_queue, snd_timer_tasklet,\n\t\t     (unsigned long)timer);\n\tif (card != NULL) {\n\t\ttimer->module = card->module;\n\t\terr = snd_device_new(card, SNDRV_DEV_TIMER, timer, &ops);\n\t\tif (err < 0) {\n\t\t\tsnd_timer_free(timer);\n\t\t\treturn err;\n\t\t}\n\t}\n\tif (rtimer)\n\t\t*rtimer = timer;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_045": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_notify(struct snd_timer *timer, int event, struct timespec *tstamp){\n\tunsigned long flags;\n\tunsigned long resolution = 0;\n\tstruct snd_timer_instance *ti, *ts;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\tif (! (timer->hw.flags & SNDRV_TIMER_HW_SLAVE))\n\t\treturn;\n\tif (snd_BUG_ON(event < SNDRV_TIMER_EVENT_MSTART ||\n\t\t       event > SNDRV_TIMER_EVENT_MRESUME))\n\t\treturn;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tif (event == SNDRV_TIMER_EVENT_MSTART ||\n\t    event == SNDRV_TIMER_EVENT_MCONTINUE ||\n\t    event == SNDRV_TIMER_EVENT_MRESUME) {\n\t\tif (timer->hw.c_resolution)\n\t\t\tresolution = timer->hw.c_resolution(timer);\n\t\telse\n\t\t\tresolution = timer->hw.resolution;\n\t}\n\tlist_for_each_entry(ti, &timer->active_list_head, active_list) {\n\t\tif (ti->ccallback)\n\t\t\tti->ccallback(ti, event, tstamp, resolution);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list)\n\t\t\tif (ts->ccallback)\n\t\t\t\tts->ccallback(ts, event, tstamp, resolution);\n\t}\n\tspin_unlock_irqrestore(&timer->lock, flags);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_046": {
        "vulnerable_code": {
            "Code": "snd_timer_open(struct snd_timer_instance **ti,\n\t\t   char *owner, struct snd_timer_id *tid,\n\t\t   unsigned int slave_id){\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *timeri = NULL;\n\n\tif (tid->dev_class == SNDRV_TIMER_CLASS_SLAVE) {\n\t\t/* open a slave instance */\n\t\tif (tid->dev_sclass <= SNDRV_TIMER_SCLASS_NONE ||\n\t\t    tid->dev_sclass > SNDRV_TIMER_SCLASS_OSS_SEQUENCER) {\n\t\t\tpr_debug(\"ALSA: timer: invalid slave class %i\\n\",\n\t\t\t\t tid->dev_sclass);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmutex_lock(&register_mutex);\n\t\ttimeri = snd_timer_instance_new(owner, NULL);\n\t\tif (!timeri) {\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\ttimeri->slave_class = tid->dev_sclass;\n\t\ttimeri->slave_id = tid->device;\n\t\ttimeri->flags |= SNDRV_TIMER_IFLG_SLAVE;\n\t\tlist_add_tail(&timeri->open_list, &snd_timer_slave_list);\n\t\tsnd_timer_check_slave(timeri);\n\t\tmutex_unlock(&register_mutex);\n\t\t*ti = timeri;\n\t\treturn 0;\n\t}\n\n\t/* open a master instance */\n\tmutex_lock(&register_mutex);\n\ttimer = snd_timer_find(tid);\n#ifdef CONFIG_MODULES\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\tsnd_timer_request(tid);\n\t\tmutex_lock(&register_mutex);\n\t\ttimer = snd_timer_find(tid);\n\t}\n#endif\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\treturn -ENODEV;\n\t}\n\tif (!list_empty(&timer->open_list_head)) {\n\t\ttimeri = list_entry(timer->open_list_head.next,\n\t\t\t\t    struct snd_timer_instance, open_list);\n\t\tif (timeri->flags & SNDRV_TIMER_IFLG_EXCLUSIVE) {\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\ttimeri = snd_timer_instance_new(owner, timer);\n\tif (!timeri) {\n\t\tmutex_unlock(&register_mutex);\n\t\treturn -ENOMEM;\n\t}\n\t/* take a card refcount for safe disconnection */\n\tif (timer->card)\n\t\tget_device(&timer->card->card_dev);\n\ttimeri->slave_class = tid->dev_sclass;\n\ttimeri->slave_id = slave_id;\n\tif (list_empty(&timer->open_list_head) && timer->hw.open)\n\t\ttimer->hw.open(timer);\n\tlist_add_tail(&timeri->open_list, &timer->open_list_head);\n\tsnd_timer_check_master(timeri);\n\tmutex_unlock(&register_mutex);\n\t*ti = timeri;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_open(struct snd_timer_instance **ti,\n\t\t   char *owner, struct snd_timer_id *tid,\n\t\t   unsigned int slave_id){\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *timeri = NULL;\n\n\tif (tid->dev_class == SNDRV_TIMER_CLASS_SLAVE) {\n\t\t/* open a slave instance */\n\t\tif (tid->dev_sclass <= SNDRV_TIMER_SCLASS_NONE ||\n\t\t    tid->dev_sclass > SNDRV_TIMER_SCLASS_OSS_SEQUENCER) {\n\t\t\tpr_debug(\"ALSA: timer: invalid slave class %i\\n\",\n\t\t\t\t tid->dev_sclass);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmutex_lock(&register_mutex);\n\t\ttimeri = snd_timer_instance_new(owner, NULL);\n\t\tif (!timeri) {\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\ttimeri->slave_class = tid->dev_sclass;\n\t\ttimeri->slave_id = tid->device;\n\t\ttimeri->flags |= SNDRV_TIMER_IFLG_SLAVE;\n\t\tlist_add_tail(&timeri->open_list, &snd_timer_slave_list);\n\t\tsnd_timer_check_slave(timeri);\n\t\tmutex_unlock(&register_mutex);\n\t\t*ti = timeri;\n\t\treturn 0;\n\t}\n\n\t/* open a master instance */\n\tmutex_lock(&register_mutex);\n\ttimer = snd_timer_find(tid);\n#ifdef CONFIG_MODULES\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\tsnd_timer_request(tid);\n\t\tmutex_lock(&register_mutex);\n\t\ttimer = snd_timer_find(tid);\n\t}\n#endif\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\treturn -ENODEV;\n\t}\n\tif (!list_empty(&timer->open_list_head)) {\n\t\ttimeri = list_entry(timer->open_list_head.next,\n\t\t\t\t    struct snd_timer_instance, open_list);\n\t\tif (timeri->flags & SNDRV_TIMER_IFLG_EXCLUSIVE) {\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\ttimeri = snd_timer_instance_new(owner, timer);\n\tif (!timeri) {\n\t\tmutex_unlock(&register_mutex);\n\t\treturn -ENOMEM;\n\t}\n\t/* take a card refcount for safe disconnection */\n\tif (timer->card)\n\t\tget_device(&timer->card->card_dev);\n\ttimeri->slave_class = tid->dev_sclass;\n\ttimeri->slave_id = slave_id;\n\tif (list_empty(&timer->open_list_head) && timer->hw.open)\n\t\ttimer->hw.open(timer);\n\tlist_add_tail(&timeri->open_list, &timer->open_list_head);\n\tsnd_timer_check_master(timeri);\n\tmutex_unlock(&register_mutex);\n\t*ti = timeri;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_047": {
        "vulnerable_code": {
            "Code": "snd_timer_user_continue(struct file *file){\n\tint err;\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\ttu->timeri->lost = 0;\n\treturn (err = snd_timer_continue(tu->timeri)) < 0 ? err : 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_continue(struct file *file){\n\tint err;\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\ttu->timeri->lost = 0;\n\treturn (err = snd_timer_continue(tu->timeri)) < 0 ? err : 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_048": {
        "vulnerable_code": {
            "Code": "snd_timer_user_copy_id(struct snd_timer_id *id, struct snd_timer *timer){\n\tid->dev_class = timer->tmr_class;\n\tid->dev_sclass = SNDRV_TIMER_SCLASS_NONE;\n\tid->card = timer->card ? timer->card->number : -1;\n\tid->device = timer->tmr_device;\n\tid->subdevice = timer->tmr_subdevice;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_copy_id(struct snd_timer_id *id, struct snd_timer *timer){\n\tid->dev_class = timer->tmr_class;\n\tid->dev_sclass = SNDRV_TIMER_SCLASS_NONE;\n\tid->card = timer->card ? timer->card->number : -1;\n\tid->device = timer->tmr_device;\n\tid->subdevice = timer->tmr_subdevice;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_049": {
        "vulnerable_code": {
            "Code": "snd_timer_user_info(struct file *file,\n\t\t\t       struct snd_timer_info __user *_info){\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_info *info;\n\tstruct snd_timer *t;\n\tint err = 0;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\tt = tu->timeri->timer;\n\tif (!t)\n\t\treturn -EBADFD;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (! info)\n\t\treturn -ENOMEM;\n\tinfo->card = t->card ? t->card->number : -1;\n\tif (t->hw.flags & SNDRV_TIMER_HW_SLAVE)\n\t\tinfo->flags |= SNDRV_TIMER_FLG_SLAVE;\n\tstrlcpy(info->id, t->id, sizeof(info->id));\n\tstrlcpy(info->name, t->name, sizeof(info->name));\n\tinfo->resolution = t->hw.resolution;\n\tif (copy_to_user(_info, info, sizeof(*_info)))\n\t\terr = -EFAULT;\n\tkfree(info);\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_info(struct file *file,\n\t\t\t       struct snd_timer_info __user *_info){\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_info *info;\n\tstruct snd_timer *t;\n\tint err = 0;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\tt = tu->timeri->timer;\n\tif (!t)\n\t\treturn -EBADFD;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\tif (! info)\n\t\treturn -ENOMEM;\n\tinfo->card = t->card ? t->card->number : -1;\n\tif (t->hw.flags & SNDRV_TIMER_HW_SLAVE)\n\t\tinfo->flags |= SNDRV_TIMER_FLG_SLAVE;\n\tstrlcpy(info->id, t->id, sizeof(info->id));\n\tstrlcpy(info->name, t->name, sizeof(info->name));\n\tinfo->resolution = t->hw.resolution;\n\tif (copy_to_user(_info, info, sizeof(*_info)))\n\t\terr = -EFAULT;\n\tkfree(info);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_050": {
        "vulnerable_code": {
            "Code": "snd_timer_user_params(struct file *file,\n\t\t\t\t struct snd_timer_params __user *_params){\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_params params;\n\tstruct snd_timer *t;\n\tstruct snd_timer_read *tr;\n\tstruct snd_timer_tread *ttr;\n\tint err;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\tt = tu->timeri->timer;\n\tif (!t)\n\t\treturn -EBADFD;\n\tif (copy_from_user(&params, _params, sizeof(params)))\n\t\treturn -EFAULT;\n\tif (!(t->hw.flags & SNDRV_TIMER_HW_SLAVE) && params.ticks < 1) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tif (params.queue_size > 0 &&\n\t    (params.queue_size < 32 || params.queue_size > 1024)) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tif (params.filter & ~((1<<SNDRV_TIMER_EVENT_RESOLUTION)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_TICK)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_START)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_STOP)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_CONTINUE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_PAUSE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_SUSPEND)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_RESUME)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSTART)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSTOP)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MCONTINUE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MPAUSE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSUSPEND)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MRESUME))) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tsnd_timer_stop(tu->timeri);\n\tspin_lock_irq(&t->lock);\n\ttu->timeri->flags &= ~(SNDRV_TIMER_IFLG_AUTO|\n\t\t\t       SNDRV_TIMER_IFLG_EXCLUSIVE|\n\t\t\t       SNDRV_TIMER_IFLG_EARLY_EVENT);\n\tif (params.flags & SNDRV_TIMER_PSFLG_AUTO)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_AUTO;\n\tif (params.flags & SNDRV_TIMER_PSFLG_EXCLUSIVE)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_EXCLUSIVE;\n\tif (params.flags & SNDRV_TIMER_PSFLG_EARLY_EVENT)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_EARLY_EVENT;\n\tspin_unlock_irq(&t->lock);\n\tif (params.queue_size > 0 &&\n\t    (unsigned int)tu->queue_size != params.queue_size) {\n\t\tif (tu->tread) {\n\t\t\tttr = kmalloc(params.queue_size * sizeof(*ttr),\n\t\t\t\t      GFP_KERNEL);\n\t\t\tif (ttr) {\n\t\t\t\tkfree(tu->tqueue);\n\t\t\t\ttu->queue_size = params.queue_size;\n\t\t\t\ttu->tqueue = ttr;\n\t\t\t}\n\t\t} else {\n\t\t\ttr = kmalloc(params.queue_size * sizeof(*tr),\n\t\t\t\t     GFP_KERNEL);\n\t\t\tif (tr) {\n\t\t\t\tkfree(tu->queue);\n\t\t\t\ttu->queue_size = params.queue_size;\n\t\t\t\ttu->queue = tr;\n\t\t\t}\n\t\t}\n\t}\n\ttu->qhead = tu->qtail = tu->qused = 0;\n\tif (tu->timeri->flags & SNDRV_TIMER_IFLG_EARLY_EVENT) {\n\t\tif (tu->tread) {\n\t\t\tstruct snd_timer_tread tread;\n\t\t\ttread.event = SNDRV_TIMER_EVENT_EARLY;\n\t\t\ttread.tstamp.tv_sec = 0;\n\t\t\ttread.tstamp.tv_nsec = 0;\n\t\t\ttread.val = 0;\n\t\t\tsnd_timer_user_append_to_tqueue(tu, &tread);\n\t\t} else {\n\t\t\tstruct snd_timer_read *r = &tu->queue[0];\n\t\t\tr->resolution = 0;\n\t\t\tr->ticks = 0;\n\t\t\ttu->qused++;\n\t\t\ttu->qtail++;\n\t\t}\n\t}\n\ttu->filter = params.filter;\n\ttu->ticks = params.ticks;\n\terr = 0;\n _end:\n\tif (copy_to_user(_params, &params, sizeof(params)))\n\t\treturn -EFAULT;\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_params(struct file *file,\n\t\t\t\t struct snd_timer_params __user *_params){\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_params params;\n\tstruct snd_timer *t;\n\tstruct snd_timer_read *tr;\n\tstruct snd_timer_tread *ttr;\n\tint err;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\tt = tu->timeri->timer;\n\tif (!t)\n\t\treturn -EBADFD;\n\tif (copy_from_user(&params, _params, sizeof(params)))\n\t\treturn -EFAULT;\n\tif (!(t->hw.flags & SNDRV_TIMER_HW_SLAVE) && params.ticks < 1) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tif (params.queue_size > 0 &&\n\t    (params.queue_size < 32 || params.queue_size > 1024)) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tif (params.filter & ~((1<<SNDRV_TIMER_EVENT_RESOLUTION)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_TICK)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_START)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_STOP)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_CONTINUE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_PAUSE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_SUSPEND)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_RESUME)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSTART)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSTOP)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MCONTINUE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MPAUSE)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MSUSPEND)|\n\t\t\t      (1<<SNDRV_TIMER_EVENT_MRESUME))) {\n\t\terr = -EINVAL;\n\t\tgoto _end;\n\t}\n\tsnd_timer_stop(tu->timeri);\n\tspin_lock_irq(&t->lock);\n\ttu->timeri->flags &= ~(SNDRV_TIMER_IFLG_AUTO|\n\t\t\t       SNDRV_TIMER_IFLG_EXCLUSIVE|\n\t\t\t       SNDRV_TIMER_IFLG_EARLY_EVENT);\n\tif (params.flags & SNDRV_TIMER_PSFLG_AUTO)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_AUTO;\n\tif (params.flags & SNDRV_TIMER_PSFLG_EXCLUSIVE)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_EXCLUSIVE;\n\tif (params.flags & SNDRV_TIMER_PSFLG_EARLY_EVENT)\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_EARLY_EVENT;\n\tspin_unlock_irq(&t->lock);\n\tif (params.queue_size > 0 &&\n\t    (unsigned int)tu->queue_size != params.queue_size) {\n\t\tif (tu->tread) {\n\t\t\tttr = kmalloc(params.queue_size * sizeof(*ttr),\n\t\t\t\t      GFP_KERNEL);\n\t\t\tif (ttr) {\n\t\t\t\tkfree(tu->tqueue);\n\t\t\t\ttu->queue_size = params.queue_size;\n\t\t\t\ttu->tqueue = ttr;\n\t\t\t}\n\t\t} else {\n\t\t\ttr = kmalloc(params.queue_size * sizeof(*tr),\n\t\t\t\t     GFP_KERNEL);\n\t\t\tif (tr) {\n\t\t\t\tkfree(tu->queue);\n\t\t\t\ttu->queue_size = params.queue_size;\n\t\t\t\ttu->queue = tr;\n\t\t\t}\n\t\t}\n\t}\n\ttu->qhead = tu->qtail = tu->qused = 0;\n\tif (tu->timeri->flags & SNDRV_TIMER_IFLG_EARLY_EVENT) {\n\t\tif (tu->tread) {\n\t\t\tstruct snd_timer_tread tread;\n\t\t\tmemset(&tread, 0, sizeof(tread));\n\t\t\ttread.event = SNDRV_TIMER_EVENT_EARLY;\n\t\t\ttread.tstamp.tv_sec = 0;\n\t\t\ttread.tstamp.tv_nsec = 0;\n\t\t\ttread.val = 0;\n\t\t\tsnd_timer_user_append_to_tqueue(tu, &tread);\n\t\t} else {\n\t\t\tstruct snd_timer_read *r = &tu->queue[0];\n\t\t\tr->resolution = 0;\n\t\t\tr->ticks = 0;\n\t\t\ttu->qused++;\n\t\t\ttu->qtail++;\n\t\t}\n\t}\n\ttu->filter = params.filter;\n\ttu->ticks = params.ticks;\n\terr = 0;\n _end:\n\tif (copy_to_user(_params, &params, sizeof(params)))\n\t\treturn -EFAULT;\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_051": {
        "vulnerable_code": {
            "Code": "snd_timer_user_stop(struct file *file){\n\tint err;\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\treturn (err = snd_timer_stop(tu->timeri)) < 0 ? err : 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_stop(struct file *file){\n\tint err;\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\treturn (err = snd_timer_stop(tu->timeri)) < 0 ? err : 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_052": {
        "vulnerable_code": {
            "Code": "snd_timer_user_tselect(struct file *file,\n\t\t\t\t  struct snd_timer_select __user *_tselect){\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_select tselect;\n\tchar str[32];\n\tint err = 0;\n\n\ttu = file->private_data;\n\tif (tu->timeri) {\n\t\tsnd_timer_close(tu->timeri);\n\t\ttu->timeri = NULL;\n\t}\n\tif (copy_from_user(&tselect, _tselect, sizeof(tselect))) {\n\t\terr = -EFAULT;\n\t\tgoto __err;\n\t}\n\tsprintf(str, \"application %i\", current->pid);\n\tif (tselect.id.dev_class != SNDRV_TIMER_CLASS_SLAVE)\n\t\ttselect.id.dev_sclass = SNDRV_TIMER_SCLASS_APPLICATION;\n\terr = snd_timer_open(&tu->timeri, str, &tselect.id, current->pid);\n\tif (err < 0)\n\t\tgoto __err;\n\n\tkfree(tu->queue);\n\ttu->queue = NULL;\n\tkfree(tu->tqueue);\n\ttu->tqueue = NULL;\n\tif (tu->tread) {\n\t\ttu->tqueue = kmalloc(tu->queue_size * sizeof(struct snd_timer_tread),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (tu->tqueue == NULL)\n\t\t\terr = -ENOMEM;\n\t} else {\n\t\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (tu->queue == NULL)\n\t\t\terr = -ENOMEM;\n\t}\n\n      \tif (err < 0) {\n\t\tsnd_timer_close(tu->timeri);\n      \t\ttu->timeri = NULL;\n      \t} else {\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_FAST;\n\t\ttu->timeri->callback = tu->tread\n\t\t\t? snd_timer_user_tinterrupt : snd_timer_user_interrupt;\n\t\ttu->timeri->ccallback = snd_timer_user_ccallback;\n\t\ttu->timeri->callback_data = (void *)tu;\n\t\ttu->timeri->disconnect = snd_timer_user_disconnect;\n\t}\n\n      __err:\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_tselect(struct file *file,\n\t\t\t\t  struct snd_timer_select __user *_tselect){\n\tstruct snd_timer_user *tu;\n\tstruct snd_timer_select tselect;\n\tchar str[32];\n\tint err = 0;\n\n\ttu = file->private_data;\n\tif (tu->timeri) {\n\t\tsnd_timer_close(tu->timeri);\n\t\ttu->timeri = NULL;\n\t}\n\tif (copy_from_user(&tselect, _tselect, sizeof(tselect))) {\n\t\terr = -EFAULT;\n\t\tgoto __err;\n\t}\n\tsprintf(str, \"application %i\", current->pid);\n\tif (tselect.id.dev_class != SNDRV_TIMER_CLASS_SLAVE)\n\t\ttselect.id.dev_sclass = SNDRV_TIMER_SCLASS_APPLICATION;\n\terr = snd_timer_open(&tu->timeri, str, &tselect.id, current->pid);\n\tif (err < 0)\n\t\tgoto __err;\n\n\tkfree(tu->queue);\n\ttu->queue = NULL;\n\tkfree(tu->tqueue);\n\ttu->tqueue = NULL;\n\tif (tu->tread) {\n\t\ttu->tqueue = kmalloc(tu->queue_size * sizeof(struct snd_timer_tread),\n\t\t\t\t     GFP_KERNEL);\n\t\tif (tu->tqueue == NULL)\n\t\t\terr = -ENOMEM;\n\t} else {\n\t\ttu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),\n\t\t\t\t    GFP_KERNEL);\n\t\tif (tu->queue == NULL)\n\t\t\terr = -ENOMEM;\n\t}\n\n      \tif (err < 0) {\n\t\tsnd_timer_close(tu->timeri);\n      \t\ttu->timeri = NULL;\n      \t} else {\n\t\ttu->timeri->flags |= SNDRV_TIMER_IFLG_FAST;\n\t\ttu->timeri->callback = tu->tread\n\t\t\t? snd_timer_user_tinterrupt : snd_timer_user_interrupt;\n\t\ttu->timeri->ccallback = snd_timer_user_ccallback;\n\t\ttu->timeri->callback_data = (void *)tu;\n\t\ttu->timeri->disconnect = snd_timer_user_disconnect;\n\t}\n\n      __err:\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_053": {
        "vulnerable_code": {
            "Code": "snd_timer_check_slave(struct snd_timer_instance *slave){\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *master;\n\n\t/* FIXME: it's really dumb to look up all entries.. */\n\tlist_for_each_entry(timer, &snd_timer_list, device_list) {\n\t\tlist_for_each_entry(master, &timer->open_list_head, open_list) {\n\t\t\tif (slave->slave_class == master->slave_class &&\n\t\t\t    slave->slave_id == master->slave_id) {\n\t\t\t\tlist_move_tail(&slave->open_list,\n\t\t\t\t\t       &master->slave_list_head);\n\t\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\t\tslave->master = master;\n\t\t\t\tslave->timer = master->timer;\n\t\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_check_slave(struct snd_timer_instance *slave){\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *master;\n\n\t/* FIXME: it's really dumb to look up all entries.. */\n\tlist_for_each_entry(timer, &snd_timer_list, device_list) {\n\t\tlist_for_each_entry(master, &timer->open_list_head, open_list) {\n\t\t\tif (slave->slave_class == master->slave_class &&\n\t\t\t    slave->slave_id == master->slave_id) {\n\t\t\t\tlist_move_tail(&slave->open_list,\n\t\t\t\t\t       &master->slave_list_head);\n\t\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\t\tslave->master = master;\n\t\t\t\tslave->timer = master->timer;\n\t\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_054": {
        "vulnerable_code": {
            "Code": "snd_timer_check_slave(struct snd_timer_instance *slave){\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *master;\n\n\t/* FIXME: it's really dumb to look up all entries.. */\n\tlist_for_each_entry(timer, &snd_timer_list, device_list) {\n\t\tlist_for_each_entry(master, &timer->open_list_head, open_list) {\n\t\t\tif (slave->slave_class == master->slave_class &&\n\t\t\t    slave->slave_id == master->slave_id) {\n\t\t\t\tlist_move_tail(&slave->open_list,\n\t\t\t\t\t       &master->slave_list_head);\n\t\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\t\tslave->master = master;\n\t\t\t\tslave->timer = master->timer;\n\t\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_check_slave(struct snd_timer_instance *slave){\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *master;\n\n\t/* FIXME: it's really dumb to look up all entries.. */\n\tlist_for_each_entry(timer, &snd_timer_list, device_list) {\n\t\tlist_for_each_entry(master, &timer->open_list_head, open_list) {\n\t\t\tif (slave->slave_class == master->slave_class &&\n\t\t\t    slave->slave_id == master->slave_id) {\n\t\t\t\tlist_move_tail(&slave->open_list,\n\t\t\t\t\t       &master->slave_list_head);\n\t\t\t\tspin_lock_irq(&slave_active_lock);\n\t\t\t\tslave->master = master;\n\t\t\t\tslave->timer = master->timer;\n\t\t\t\tspin_unlock_irq(&slave_active_lock);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_055": {
        "vulnerable_code": {
            "Code": "snd_timer_global_free(struct snd_timer *timer){\n\treturn snd_timer_free(timer);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_global_free(struct snd_timer *timer){\n\treturn snd_timer_free(timer);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_056": {
        "vulnerable_code": {
            "Code": "snd_timer_global_free(struct snd_timer *timer){\n\treturn snd_timer_free(timer);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_global_free(struct snd_timer *timer){\n\treturn snd_timer_free(timer);\n}",
            "Size": 3,
            "Code Complexity": 1,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_057": {
        "vulnerable_code": {
            "Code": "snd_timer_notify(struct snd_timer *timer, int event, struct timespec *tstamp){\n\tunsigned long flags;\n\tunsigned long resolution = 0;\n\tstruct snd_timer_instance *ti, *ts;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\tif (! (timer->hw.flags & SNDRV_TIMER_HW_SLAVE))\n\t\treturn;\n\tif (snd_BUG_ON(event < SNDRV_TIMER_EVENT_MSTART ||\n\t\t       event > SNDRV_TIMER_EVENT_MRESUME))\n\t\treturn;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tif (event == SNDRV_TIMER_EVENT_MSTART ||\n\t    event == SNDRV_TIMER_EVENT_MCONTINUE ||\n\t    event == SNDRV_TIMER_EVENT_MRESUME) {\n\t\tif (timer->hw.c_resolution)\n\t\t\tresolution = timer->hw.c_resolution(timer);\n\t\telse\n\t\t\tresolution = timer->hw.resolution;\n\t}\n\tlist_for_each_entry(ti, &timer->active_list_head, active_list) {\n\t\tif (ti->ccallback)\n\t\t\tti->ccallback(ti, event, tstamp, resolution);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list)\n\t\t\tif (ts->ccallback)\n\t\t\t\tts->ccallback(ts, event, tstamp, resolution);\n\t}\n\tspin_unlock_irqrestore(&timer->lock, flags);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_notify(struct snd_timer *timer, int event, struct timespec *tstamp){\n\tunsigned long flags;\n\tunsigned long resolution = 0;\n\tstruct snd_timer_instance *ti, *ts;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\tif (! (timer->hw.flags & SNDRV_TIMER_HW_SLAVE))\n\t\treturn;\n\tif (snd_BUG_ON(event < SNDRV_TIMER_EVENT_MSTART ||\n\t\t       event > SNDRV_TIMER_EVENT_MRESUME))\n\t\treturn;\n\tspin_lock_irqsave(&timer->lock, flags);\n\tif (event == SNDRV_TIMER_EVENT_MSTART ||\n\t    event == SNDRV_TIMER_EVENT_MCONTINUE ||\n\t    event == SNDRV_TIMER_EVENT_MRESUME) {\n\t\tif (timer->hw.c_resolution)\n\t\t\tresolution = timer->hw.c_resolution(timer);\n\t\telse\n\t\t\tresolution = timer->hw.resolution;\n\t}\n\tlist_for_each_entry(ti, &timer->active_list_head, active_list) {\n\t\tif (ti->ccallback)\n\t\t\tti->ccallback(ti, event, tstamp, resolution);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list)\n\t\t\tif (ts->ccallback)\n\t\t\t\tts->ccallback(ts, event, tstamp, resolution);\n\t}\n\tspin_unlock_irqrestore(&timer->lock, flags);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_058": {
        "vulnerable_code": {
            "Code": "snd_timer_open(struct snd_timer_instance **ti,\n\t\t   char *owner, struct snd_timer_id *tid,\n\t\t   unsigned int slave_id){\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *timeri = NULL;\n\n\tif (tid->dev_class == SNDRV_TIMER_CLASS_SLAVE) {\n\t\t/* open a slave instance */\n\t\tif (tid->dev_sclass <= SNDRV_TIMER_SCLASS_NONE ||\n\t\t    tid->dev_sclass > SNDRV_TIMER_SCLASS_OSS_SEQUENCER) {\n\t\t\tpr_debug(\"ALSA: timer: invalid slave class %i\\n\",\n\t\t\t\t tid->dev_sclass);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmutex_lock(&register_mutex);\n\t\ttimeri = snd_timer_instance_new(owner, NULL);\n\t\tif (!timeri) {\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\ttimeri->slave_class = tid->dev_sclass;\n\t\ttimeri->slave_id = tid->device;\n\t\ttimeri->flags |= SNDRV_TIMER_IFLG_SLAVE;\n\t\tlist_add_tail(&timeri->open_list, &snd_timer_slave_list);\n\t\tsnd_timer_check_slave(timeri);\n\t\tmutex_unlock(&register_mutex);\n\t\t*ti = timeri;\n\t\treturn 0;\n\t}\n\n\t/* open a master instance */\n\tmutex_lock(&register_mutex);\n\ttimer = snd_timer_find(tid);\n#ifdef CONFIG_MODULES\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\tsnd_timer_request(tid);\n\t\tmutex_lock(&register_mutex);\n\t\ttimer = snd_timer_find(tid);\n\t}\n#endif\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\treturn -ENODEV;\n\t}\n\tif (!list_empty(&timer->open_list_head)) {\n\t\ttimeri = list_entry(timer->open_list_head.next,\n\t\t\t\t    struct snd_timer_instance, open_list);\n\t\tif (timeri->flags & SNDRV_TIMER_IFLG_EXCLUSIVE) {\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\ttimeri = snd_timer_instance_new(owner, timer);\n\tif (!timeri) {\n\t\tmutex_unlock(&register_mutex);\n\t\treturn -ENOMEM;\n\t}\n\t/* take a card refcount for safe disconnection */\n\tif (timer->card)\n\t\tget_device(&timer->card->card_dev);\n\ttimeri->slave_class = tid->dev_sclass;\n\ttimeri->slave_id = slave_id;\n\tif (list_empty(&timer->open_list_head) && timer->hw.open)\n\t\ttimer->hw.open(timer);\n\tlist_add_tail(&timeri->open_list, &timer->open_list_head);\n\tsnd_timer_check_master(timeri);\n\tmutex_unlock(&register_mutex);\n\t*ti = timeri;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_open(struct snd_timer_instance **ti,\n\t\t   char *owner, struct snd_timer_id *tid,\n\t\t   unsigned int slave_id){\n\tstruct snd_timer *timer;\n\tstruct snd_timer_instance *timeri = NULL;\n\n\tif (tid->dev_class == SNDRV_TIMER_CLASS_SLAVE) {\n\t\t/* open a slave instance */\n\t\tif (tid->dev_sclass <= SNDRV_TIMER_SCLASS_NONE ||\n\t\t    tid->dev_sclass > SNDRV_TIMER_SCLASS_OSS_SEQUENCER) {\n\t\t\tpr_debug(\"ALSA: timer: invalid slave class %i\\n\",\n\t\t\t\t tid->dev_sclass);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmutex_lock(&register_mutex);\n\t\ttimeri = snd_timer_instance_new(owner, NULL);\n\t\tif (!timeri) {\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\ttimeri->slave_class = tid->dev_sclass;\n\t\ttimeri->slave_id = tid->device;\n\t\ttimeri->flags |= SNDRV_TIMER_IFLG_SLAVE;\n\t\tlist_add_tail(&timeri->open_list, &snd_timer_slave_list);\n\t\tsnd_timer_check_slave(timeri);\n\t\tmutex_unlock(&register_mutex);\n\t\t*ti = timeri;\n\t\treturn 0;\n\t}\n\n\t/* open a master instance */\n\tmutex_lock(&register_mutex);\n\ttimer = snd_timer_find(tid);\n#ifdef CONFIG_MODULES\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\tsnd_timer_request(tid);\n\t\tmutex_lock(&register_mutex);\n\t\ttimer = snd_timer_find(tid);\n\t}\n#endif\n\tif (!timer) {\n\t\tmutex_unlock(&register_mutex);\n\t\treturn -ENODEV;\n\t}\n\tif (!list_empty(&timer->open_list_head)) {\n\t\ttimeri = list_entry(timer->open_list_head.next,\n\t\t\t\t    struct snd_timer_instance, open_list);\n\t\tif (timeri->flags & SNDRV_TIMER_IFLG_EXCLUSIVE) {\n\t\t\tmutex_unlock(&register_mutex);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\ttimeri = snd_timer_instance_new(owner, timer);\n\tif (!timeri) {\n\t\tmutex_unlock(&register_mutex);\n\t\treturn -ENOMEM;\n\t}\n\t/* take a card refcount for safe disconnection */\n\tif (timer->card)\n\t\tget_device(&timer->card->card_dev);\n\ttimeri->slave_class = tid->dev_sclass;\n\ttimeri->slave_id = slave_id;\n\tif (list_empty(&timer->open_list_head) && timer->hw.open)\n\t\ttimer->hw.open(timer);\n\tlist_add_tail(&timeri->open_list, &timer->open_list_head);\n\tsnd_timer_check_master(timeri);\n\tmutex_unlock(&register_mutex);\n\t*ti = timeri;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_059": {
        "vulnerable_code": {
            "Code": "snd_timer_start(struct snd_timer_instance *timeri, unsigned int ticks){\n\tif (timeri == NULL || ticks < 1)\n\t\treturn -EINVAL;\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)\n\t\treturn snd_timer_start_slave(timeri, true);\n\telse\n\t\treturn snd_timer_start1(timeri, true, ticks);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_start(struct snd_timer_instance *timeri, unsigned int ticks){\n\tif (timeri == NULL || ticks < 1)\n\t\treturn -EINVAL;\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)\n\t\treturn snd_timer_start_slave(timeri, true);\n\telse\n\t\treturn snd_timer_start1(timeri, true, ticks);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_060": {
        "vulnerable_code": {
            "Code": "snd_timer_start(struct snd_timer_instance *timeri, unsigned int ticks){\n\tif (timeri == NULL || ticks < 1)\n\t\treturn -EINVAL;\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)\n\t\treturn snd_timer_start_slave(timeri, true);\n\telse\n\t\treturn snd_timer_start1(timeri, true, ticks);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_start(struct snd_timer_instance *timeri, unsigned int ticks){\n\tif (timeri == NULL || ticks < 1)\n\t\treturn -EINVAL;\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)\n\t\treturn snd_timer_start_slave(timeri, true);\n\telse\n\t\treturn snd_timer_start1(timeri, true, ticks);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_061": {
        "vulnerable_code": {
            "Code": "snd_timer_tasklet(unsigned long arg){\n\tstruct snd_timer *timer = (struct snd_timer *) arg;\n\tstruct snd_timer_instance *ti;\n\tstruct list_head *p;\n\tunsigned long resolution, ticks;\n\tunsigned long flags;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\t/* now process all callbacks */\n\twhile (!list_empty(&timer->sack_list_head)) {\n\t\tp = timer->sack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\t\tresolution = ti->resolution;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\tspin_unlock_irqrestore(&timer->lock, flags);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_tasklet(unsigned long arg){\n\tstruct snd_timer *timer = (struct snd_timer *) arg;\n\tstruct snd_timer_instance *ti;\n\tstruct list_head *p;\n\tunsigned long resolution, ticks;\n\tunsigned long flags;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\t/* now process all callbacks */\n\twhile (!list_empty(&timer->sack_list_head)) {\n\t\tp = timer->sack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\t\tresolution = ti->resolution;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\tspin_unlock_irqrestore(&timer->lock, flags);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_062": {
        "vulnerable_code": {
            "Code": "snd_timer_tasklet(unsigned long arg){\n\tstruct snd_timer *timer = (struct snd_timer *) arg;\n\tstruct snd_timer_instance *ti;\n\tstruct list_head *p;\n\tunsigned long resolution, ticks;\n\tunsigned long flags;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\t/* now process all callbacks */\n\twhile (!list_empty(&timer->sack_list_head)) {\n\t\tp = timer->sack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\t\tresolution = ti->resolution;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\tspin_unlock_irqrestore(&timer->lock, flags);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_tasklet(unsigned long arg){\n\tstruct snd_timer *timer = (struct snd_timer *) arg;\n\tstruct snd_timer_instance *ti;\n\tstruct list_head *p;\n\tunsigned long resolution, ticks;\n\tunsigned long flags;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\t/* now process all callbacks */\n\twhile (!list_empty(&timer->sack_list_head)) {\n\t\tp = timer->sack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\t\tresolution = ti->resolution;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\tspin_unlock_irqrestore(&timer->lock, flags);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_063": {
        "vulnerable_code": {
            "Code": "snd_timer_user_append_to_tqueue(struct snd_timer_user *tu,\n\t\t\t\t\t    struct snd_timer_tread *tread){\n\tif (tu->qused >= tu->queue_size) {\n\t\ttu->overrun++;\n\t} else {\n\t\tmemcpy(&tu->tqueue[tu->qtail++], tread, sizeof(*tread));\n\t\ttu->qtail %= tu->queue_size;\n\t\ttu->qused++;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_append_to_tqueue(struct snd_timer_user *tu,\n\t\t\t\t\t    struct snd_timer_tread *tread){\n\tif (tu->qused >= tu->queue_size) {\n\t\ttu->overrun++;\n\t} else {\n\t\tmemcpy(&tu->tqueue[tu->qtail++], tread, sizeof(*tread));\n\t\ttu->qtail %= tu->queue_size;\n\t\ttu->qused++;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_064": {
        "vulnerable_code": {
            "Code": "snd_timer_user_continue(struct file *file){\n\tint err;\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\ttu->timeri->lost = 0;\n\treturn (err = snd_timer_continue(tu->timeri)) < 0 ? err : 0;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_continue(struct file *file){\n\tint err;\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\ttu->timeri->lost = 0;\n\treturn (err = snd_timer_continue(tu->timeri)) < 0 ? err : 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_065": {
        "vulnerable_code": {
            "Code": "snd_timer_user_gstatus(struct file *file,\n\t\t\t\t  struct snd_timer_gstatus __user *_gstatus){\n\tstruct snd_timer_gstatus gstatus;\n\tstruct snd_timer_id tid;\n\tstruct snd_timer *t;\n\tint err = 0;\n\n\tif (copy_from_user(&gstatus, _gstatus, sizeof(gstatus)))\n\t\treturn -EFAULT;\n\ttid = gstatus.tid;\n\tmemset(&gstatus, 0, sizeof(gstatus));\n\tgstatus.tid = tid;\n\tmutex_lock(&register_mutex);\n\tt = snd_timer_find(&tid);\n\tif (t != NULL) {\n\t\tif (t->hw.c_resolution)\n\t\t\tgstatus.resolution = t->hw.c_resolution(t);\n\t\telse\n\t\t\tgstatus.resolution = t->hw.resolution;\n\t\tif (t->hw.precise_resolution) {\n\t\t\tt->hw.precise_resolution(t, &gstatus.resolution_num,\n\t\t\t\t\t\t &gstatus.resolution_den);\n\t\t} else {\n\t\t\tgstatus.resolution_num = gstatus.resolution;\n\t\t\tgstatus.resolution_den = 1000000000uL;\n\t\t}\n\t} else {\n\t\terr = -ENODEV;\n\t}\n\tmutex_unlock(&register_mutex);\n\tif (err >= 0 && copy_to_user(_gstatus, &gstatus, sizeof(gstatus)))\n\t\terr = -EFAULT;\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_gstatus(struct file *file,\n\t\t\t\t  struct snd_timer_gstatus __user *_gstatus){\n\tstruct snd_timer_gstatus gstatus;\n\tstruct snd_timer_id tid;\n\tstruct snd_timer *t;\n\tint err = 0;\n\n\tif (copy_from_user(&gstatus, _gstatus, sizeof(gstatus)))\n\t\treturn -EFAULT;\n\ttid = gstatus.tid;\n\tmemset(&gstatus, 0, sizeof(gstatus));\n\tgstatus.tid = tid;\n\tmutex_lock(&register_mutex);\n\tt = snd_timer_find(&tid);\n\tif (t != NULL) {\n\t\tif (t->hw.c_resolution)\n\t\t\tgstatus.resolution = t->hw.c_resolution(t);\n\t\telse\n\t\t\tgstatus.resolution = t->hw.resolution;\n\t\tif (t->hw.precise_resolution) {\n\t\t\tt->hw.precise_resolution(t, &gstatus.resolution_num,\n\t\t\t\t\t\t &gstatus.resolution_den);\n\t\t} else {\n\t\t\tgstatus.resolution_num = gstatus.resolution;\n\t\t\tgstatus.resolution_den = 1000000000uL;\n\t\t}\n\t} else {\n\t\terr = -ENODEV;\n\t}\n\tmutex_unlock(&register_mutex);\n\tif (err >= 0 && copy_to_user(_gstatus, &gstatus, sizeof(gstatus)))\n\t\terr = -EFAULT;\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_066": {
        "vulnerable_code": {
            "Code": "snd_timer_user_interrupt(struct snd_timer_instance *timeri,\n\t\t\t\t     unsigned long resolution,\n\t\t\t\t     unsigned long ticks){\n\tstruct snd_timer_user *tu = timeri->callback_data;\n\tstruct snd_timer_read *r;\n\tint prev;\n\n\tspin_lock(&tu->qlock);\n\tif (tu->qused > 0) {\n\t\tprev = tu->qtail == 0 ? tu->queue_size - 1 : tu->qtail - 1;\n\t\tr = &tu->queue[prev];\n\t\tif (r->resolution == resolution) {\n\t\t\tr->ticks += ticks;\n\t\t\tgoto __wake;\n\t\t}\n\t}\n\tif (tu->qused >= tu->queue_size) {\n\t\ttu->overrun++;\n\t} else {\n\t\tr = &tu->queue[tu->qtail++];\n\t\ttu->qtail %= tu->queue_size;\n\t\tr->resolution = resolution;\n\t\tr->ticks = ticks;\n\t\ttu->qused++;\n\t}\n      __wake:\n\tspin_unlock(&tu->qlock);\n\tkill_fasync(&tu->fasync, SIGIO, POLL_IN);\n\twake_up(&tu->qchange_sleep);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_interrupt(struct snd_timer_instance *timeri,\n\t\t\t\t     unsigned long resolution,\n\t\t\t\t     unsigned long ticks){\n\tstruct snd_timer_user *tu = timeri->callback_data;\n\tstruct snd_timer_read *r;\n\tint prev;\n\n\tspin_lock(&tu->qlock);\n\tif (tu->qused > 0) {\n\t\tprev = tu->qtail == 0 ? tu->queue_size - 1 : tu->qtail - 1;\n\t\tr = &tu->queue[prev];\n\t\tif (r->resolution == resolution) {\n\t\t\tr->ticks += ticks;\n\t\t\tgoto __wake;\n\t\t}\n\t}\n\tif (tu->qused >= tu->queue_size) {\n\t\ttu->overrun++;\n\t} else {\n\t\tr = &tu->queue[tu->qtail++];\n\t\ttu->qtail %= tu->queue_size;\n\t\tr->resolution = resolution;\n\t\tr->ticks = ticks;\n\t\ttu->qused++;\n\t}\n      __wake:\n\tspin_unlock(&tu->qlock);\n\tkill_fasync(&tu->fasync, SIGIO, POLL_IN);\n\twake_up(&tu->qchange_sleep);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_067": {
        "vulnerable_code": {
            "Code": "snd_timer_user_stop(struct file *file){\n\tint err;\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\treturn (err = snd_timer_stop(tu->timeri)) < 0 ? err : 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_stop(struct file *file){\n\tint err;\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\tif (!tu->timeri)\n\t\treturn -EBADFD;\n\treturn (err = snd_timer_stop(tu->timeri)) < 0 ? err : 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_068": {
        "vulnerable_code": {
            "Code": "snd_timer_dev_free(struct snd_device *device){\n\tstruct snd_timer *timer = device->device_data;\n\treturn snd_timer_free(timer);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_dev_free(struct snd_device *device){\n\tstruct snd_timer *timer = device->device_data;\n\treturn snd_timer_free(timer);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_069": {
        "vulnerable_code": {
            "Code": "snd_timer_free_system(struct snd_timer *timer){\n\tkfree(timer->private_data);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_free_system(struct snd_timer *timer){\n\tkfree(timer->private_data);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_070": {
        "vulnerable_code": {
            "Code": "snd_timer_global_new(char *id, int device, struct snd_timer **rtimer){\n\tstruct snd_timer_id tid;\n\n\ttid.dev_class = SNDRV_TIMER_CLASS_GLOBAL;\n\ttid.dev_sclass = SNDRV_TIMER_SCLASS_NONE;\n\ttid.card = -1;\n\ttid.device = device;\n\ttid.subdevice = 0;\n\treturn snd_timer_new(NULL, id, &tid, rtimer);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_global_new(char *id, int device, struct snd_timer **rtimer){\n\tstruct snd_timer_id tid;\n\n\ttid.dev_class = SNDRV_TIMER_CLASS_GLOBAL;\n\ttid.dev_sclass = SNDRV_TIMER_SCLASS_NONE;\n\ttid.card = -1;\n\ttid.device = device;\n\ttid.subdevice = 0;\n\treturn snd_timer_new(NULL, id, &tid, rtimer);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_071": {
        "vulnerable_code": {
            "Code": "snd_timer_resolution(struct snd_timer_instance *timeri){\n\tstruct snd_timer * timer;\n\n\tif (timeri == NULL)\n\t\treturn 0;\n\tif ((timer = timeri->timer) != NULL) {\n\t\tif (timer->hw.c_resolution)\n\t\t\treturn timer->hw.c_resolution(timer);\n\t\treturn timer->hw.resolution;\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_resolution(struct snd_timer_instance *timeri){\n\tstruct snd_timer * timer;\n\n\tif (timeri == NULL)\n\t\treturn 0;\n\tif ((timer = timeri->timer) != NULL) {\n\t\tif (timer->hw.c_resolution)\n\t\t\treturn timer->hw.c_resolution(timer);\n\t\treturn timer->hw.resolution;\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_072": {
        "vulnerable_code": {
            "Code": "snd_timer_stop(struct snd_timer_instance *timeri){\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)\n\t\treturn snd_timer_stop_slave(timeri, true);\n\telse\n\t\treturn snd_timer_stop1(timeri, true);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_stop(struct snd_timer_instance *timeri){\n\tif (timeri->flags & SNDRV_TIMER_IFLG_SLAVE)\n\t\treturn snd_timer_stop_slave(timeri, true);\n\telse\n\t\treturn snd_timer_stop1(timeri, true);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_073": {
        "vulnerable_code": {
            "Code": "snd_timer_user_ccallback(struct snd_timer_instance *timeri,\n\t\t\t\t     int event,\n\t\t\t\t     struct timespec *tstamp,\n\t\t\t\t     unsigned long resolution){\n\tstruct snd_timer_user *tu = timeri->callback_data;\n\tstruct snd_timer_tread r1;\n\tunsigned long flags;\n\n\tif (event >= SNDRV_TIMER_EVENT_START &&\n\t    event <= SNDRV_TIMER_EVENT_PAUSE)\n\t\ttu->tstamp = *tstamp;\n\tif ((tu->filter & (1 << event)) == 0 || !tu->tread)\n\t\treturn;\n\tr1.event = event;\n\tr1.tstamp = *tstamp;\n\tr1.val = resolution;\n\tspin_lock_irqsave(&tu->qlock, flags);\n\tsnd_timer_user_append_to_tqueue(tu, &r1);\n\tspin_unlock_irqrestore(&tu->qlock, flags);\n\tkill_fasync(&tu->fasync, SIGIO, POLL_IN);\n\twake_up(&tu->qchange_sleep);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_ccallback(struct snd_timer_instance *timeri,\n\t\t\t\t     int event,\n\t\t\t\t     struct timespec *tstamp,\n\t\t\t\t     unsigned long resolution){\n\tstruct snd_timer_user *tu = timeri->callback_data;\n\tstruct snd_timer_tread r1;\n\tunsigned long flags;\n\n\tif (event >= SNDRV_TIMER_EVENT_START &&\n\t    event <= SNDRV_TIMER_EVENT_PAUSE)\n\t\ttu->tstamp = *tstamp;\n\tif ((tu->filter & (1 << event)) == 0 || !tu->tread)\n\t\treturn;\n\tmemset(&r1, 0, sizeof(r1));\n\tr1.event = event;\n\tr1.tstamp = *tstamp;\n\tr1.val = resolution;\n\tspin_lock_irqsave(&tu->qlock, flags);\n\tsnd_timer_user_append_to_tqueue(tu, &r1);\n\tspin_unlock_irqrestore(&tu->qlock, flags);\n\tkill_fasync(&tu->fasync, SIGIO, POLL_IN);\n\twake_up(&tu->qchange_sleep);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_074": {
        "vulnerable_code": {
            "Code": "snd_timer_user_fasync(int fd, struct file * file, int on){\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\treturn fasync_helper(fd, file, on, &tu->fasync);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_fasync(int fd, struct file * file, int on){\n\tstruct snd_timer_user *tu;\n\n\ttu = file->private_data;\n\treturn fasync_helper(fd, file, on, &tu->fasync);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_075": {
        "vulnerable_code": {
            "Code": "snd_timer_user_poll(struct file *file, poll_table * wait){\n        unsigned int mask;\n        struct snd_timer_user *tu;\n\n        tu = file->private_data;\n\n        poll_wait(file, &tu->qchange_sleep, wait);\n\n\tmask = 0;\n\tif (tu->qused)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (tu->disconnected)\n\t\tmask |= POLLERR;\n\n\treturn mask;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_poll(struct file *file, poll_table * wait){\n        unsigned int mask;\n        struct snd_timer_user *tu;\n\n        tu = file->private_data;\n\n        poll_wait(file, &tu->qchange_sleep, wait);\n\n\tmask = 0;\n\tif (tu->qused)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (tu->disconnected)\n\t\tmask |= POLLERR;\n\n\treturn mask;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_076": {
        "vulnerable_code": {
            "Code": "snd_timer_user_release(struct inode *inode, struct file *file){\n\tstruct snd_timer_user *tu;\n\n\tif (file->private_data) {\n\t\ttu = file->private_data;\n\t\tfile->private_data = NULL;\n\t\tmutex_lock(&tu->ioctl_lock);\n\t\tif (tu->timeri)\n\t\t\tsnd_timer_close(tu->timeri);\n\t\tmutex_unlock(&tu->ioctl_lock);\n\t\tkfree(tu->queue);\n\t\tkfree(tu->tqueue);\n\t\tkfree(tu);\n\t}\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_release(struct inode *inode, struct file *file){\n\tstruct snd_timer_user *tu;\n\n\tif (file->private_data) {\n\t\ttu = file->private_data;\n\t\tfile->private_data = NULL;\n\t\tmutex_lock(&tu->ioctl_lock);\n\t\tif (tu->timeri)\n\t\t\tsnd_timer_close(tu->timeri);\n\t\tmutex_unlock(&tu->ioctl_lock);\n\t\tkfree(tu->queue);\n\t\tkfree(tu->tqueue);\n\t\tkfree(tu);\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_077": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_078": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_079": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_080": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_081": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_082": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_083": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_084": {
        "vulnerable_code": {
            "Code": "cp2112_gpio_direction_input(struct gpio_chip *chip, unsigned offset){\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_GET_REPORT);\n\tif (ret != CP2112_GPIO_CONFIG_LENGTH) {\n\t\thid_err(hdev, \"error requesting GPIO config: %d\\n\", ret);\n\t\tgoto exit;\n\t}\n\n\tbuf[1] &= ~(1 << offset);\n\tbuf[2] = gpio_push_pull;\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_SET_REPORT);\n\tif (ret < 0) {\n\t\thid_err(hdev, \"error setting GPIO config: %d\\n\", ret);\n\t\tgoto exit;\n\t}\n\n\tret = 0;\n\nexit:\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\treturn ret <= 0 ? ret : -EIO;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cp2112_gpio_direction_input(struct gpio_chip *chip, unsigned offset){\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tint ret;\n\n\tmutex_lock(&dev->lock);\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_GET_REPORT);\n\tif (ret != CP2112_GPIO_CONFIG_LENGTH) {\n\t\thid_err(hdev, \"error requesting GPIO config: %d\\n\", ret);\n\t\tgoto exit;\n\t}\n\n\tbuf[1] &= ~(1 << offset);\n\tbuf[2] = gpio_push_pull;\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_SET_REPORT);\n\tif (ret < 0) {\n\t\thid_err(hdev, \"error setting GPIO config: %d\\n\", ret);\n\t\tgoto exit;\n\t}\n\n\tret = 0;\n\nexit:\n\tmutex_unlock(&dev->lock);\n\treturn ret <= 0 ? ret : -EIO;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_085": {
        "vulnerable_code": {
            "Code": "cp2112_gpio_direction_output(struct gpio_chip *chip,\n\t\t\t\t\tunsigned offset, int value){\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_GET_REPORT);\n\tif (ret != CP2112_GPIO_CONFIG_LENGTH) {\n\t\thid_err(hdev, \"error requesting GPIO config: %d\\n\", ret);\n\t\tgoto fail;\n\t}\n\n\tbuf[1] |= 1 << offset;\n\tbuf[2] = gpio_push_pull;\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_SET_REPORT);\n\tif (ret < 0) {\n\t\thid_err(hdev, \"error setting GPIO config: %d\\n\", ret);\n\t\tgoto fail;\n\t}\n\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\t/*\n\t * Set gpio value when output direction is already set,\n\t * as specified in AN495, Rev. 0.2, cpt. 4.4\n\t */\n\tcp2112_gpio_set(chip, offset, value);\n\n\treturn 0;\n\nfail:\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\treturn ret < 0 ? ret : -EIO;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cp2112_gpio_direction_output(struct gpio_chip *chip,\n\t\t\t\t\tunsigned offset, int value){\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tint ret;\n\n\tmutex_lock(&dev->lock);\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_GET_REPORT);\n\tif (ret != CP2112_GPIO_CONFIG_LENGTH) {\n\t\thid_err(hdev, \"error requesting GPIO config: %d\\n\", ret);\n\t\tgoto fail;\n\t}\n\n\tbuf[1] |= 1 << offset;\n\tbuf[2] = gpio_push_pull;\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,\n\t\t\t\t CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_SET_REPORT);\n\tif (ret < 0) {\n\t\thid_err(hdev, \"error setting GPIO config: %d\\n\", ret);\n\t\tgoto fail;\n\t}\n\n\tmutex_unlock(&dev->lock);\n\n\t/*\n\t * Set gpio value when output direction is already set,\n\t * as specified in AN495, Rev. 0.2, cpt. 4.4\n\t */\n\tcp2112_gpio_set(chip, offset, value);\n\n\treturn 0;\n\nfail:\n\tmutex_unlock(&dev->lock);\n\treturn ret < 0 ? ret : -EIO;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_086": {
        "vulnerable_code": {
            "Code": "cp2112_gpio_get(struct gpio_chip *chip, unsigned int offset){\n\tint ret;\n\n\tret = cp2112_gpio_get_all(chip);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn (ret >> offset) & 1;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cp2112_gpio_get(struct gpio_chip *chip, unsigned int offset){\n\tint ret;\n\n\tret = cp2112_gpio_get_all(chip);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn (ret >> offset) & 1;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_087": {
        "vulnerable_code": {
            "Code": "cp2112_gpio_get_all(struct gpio_chip *chip){\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_GET, buf,\n\t\t\t\t CP2112_GPIO_GET_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_GET_REPORT);\n\tif (ret != CP2112_GPIO_GET_LENGTH) {\n\t\thid_err(hdev, \"error requesting GPIO values: %d\\n\", ret);\n\t\tret = ret < 0 ? ret : -EIO;\n\t\tgoto exit;\n\t}\n\n\tret = buf[1];\n\nexit:\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cp2112_gpio_get_all(struct gpio_chip *chip){\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tint ret;\n\n\tmutex_lock(&dev->lock);\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_GET, buf,\n\t\t\t\t CP2112_GPIO_GET_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_GET_REPORT);\n\tif (ret != CP2112_GPIO_GET_LENGTH) {\n\t\thid_err(hdev, \"error requesting GPIO values: %d\\n\", ret);\n\t\tret = ret < 0 ? ret : -EIO;\n\t\tgoto exit;\n\t}\n\n\tret = buf[1];\n\nexit:\n\tmutex_unlock(&dev->lock);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_088": {
        "vulnerable_code": {
            "Code": "cp2112_gpio_set(struct gpio_chip *chip, unsigned offset, int value){\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\n\tbuf[0] = CP2112_GPIO_SET;\n\tbuf[1] = value ? 0xff : 0;\n\tbuf[2] = 1 << offset;\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_SET, buf,\n\t\t\t\t CP2112_GPIO_SET_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_SET_REPORT);\n\tif (ret < 0)\n\t\thid_err(hdev, \"error setting GPIO values: %d\\n\", ret);\n\n\tspin_unlock_irqrestore(&dev->lock, flags);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cp2112_gpio_set(struct gpio_chip *chip, unsigned offset, int value){\n\tstruct cp2112_device *dev = gpiochip_get_data(chip);\n\tstruct hid_device *hdev = dev->hdev;\n\tu8 *buf = dev->in_out_buffer;\n\tint ret;\n\n\tmutex_lock(&dev->lock);\n\n\tbuf[0] = CP2112_GPIO_SET;\n\tbuf[1] = value ? 0xff : 0;\n\tbuf[2] = 1 << offset;\n\n\tret = hid_hw_raw_request(hdev, CP2112_GPIO_SET, buf,\n\t\t\t\t CP2112_GPIO_SET_LENGTH, HID_FEATURE_REPORT,\n\t\t\t\t HID_REQ_SET_REPORT);\n\tif (ret < 0)\n\t\thid_err(hdev, \"error setting GPIO values: %d\\n\", ret);\n\n\tmutex_unlock(&dev->lock);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_089": {
        "vulnerable_code": {
            "Code": "cp2112_hid_output(struct hid_device *hdev, u8 *data, size_t count,\n\t\t\t     unsigned char report_type){\n\tu8 *buf;\n\tint ret;\n\n\tbuf = kmemdup(data, count, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tif (report_type == HID_OUTPUT_REPORT)\n\t\tret = hid_hw_output_report(hdev, buf, count);\n\telse\n\t\tret = hid_hw_raw_request(hdev, buf[0], buf, count, report_type,\n\t\t\t\tHID_REQ_SET_REPORT);\n\n\tkfree(buf);\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cp2112_hid_output(struct hid_device *hdev, u8 *data, size_t count,\n\t\t\t     unsigned char report_type){\n\tu8 *buf;\n\tint ret;\n\n\tbuf = kmemdup(data, count, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tif (report_type == HID_OUTPUT_REPORT)\n\t\tret = hid_hw_output_report(hdev, buf, count);\n\telse\n\t\tret = hid_hw_raw_request(hdev, buf[0], buf, count, report_type,\n\t\t\t\tHID_REQ_SET_REPORT);\n\n\tkfree(buf);\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_090": {
        "vulnerable_code": {
            "Code": "cp2112_set_usb_config(struct hid_device *hdev,\n\t\t\t\t struct cp2112_usb_config_report *cfg){\n\tint ret;\n\n\tBUG_ON(cfg->report != CP2112_USB_CONFIG);\n\n\tret = cp2112_hid_output(hdev, (u8 *)cfg, sizeof(*cfg),\n\t\t\t\tHID_FEATURE_REPORT);\n\tif (ret != sizeof(*cfg)) {\n\t\thid_err(hdev, \"error writing usb config: %d\\n\", ret);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cp2112_set_usb_config(struct hid_device *hdev,\n\t\t\t\t struct cp2112_usb_config_report *cfg){\n\tint ret;\n\n\tBUG_ON(cfg->report != CP2112_USB_CONFIG);\n\n\tret = cp2112_hid_output(hdev, (u8 *)cfg, sizeof(*cfg),\n\t\t\t\tHID_FEATURE_REPORT);\n\tif (ret != sizeof(*cfg)) {\n\t\thid_err(hdev, \"error writing usb config: %d\\n\", ret);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_091": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_092": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_093": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_094": {
        "vulnerable_code": {
            "Code": "__build_skb(void *data, unsigned int frag_size){\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tunsigned int size = frag_size ? : ksize(data);\n\n\tskb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\tsize -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->truesize = SKB_TRUESIZE(size);\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\treturn skb;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__build_skb(void *data, unsigned int frag_size){\n\tstruct skb_shared_info *shinfo;\n\tstruct sk_buff *skb;\n\tunsigned int size = frag_size ? : ksize(data);\n\n\tskb = kmem_cache_alloc(skbuff_head_cache, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\tsize -= SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tmemset(skb, 0, offsetof(struct sk_buff, tail));\n\tskb->truesize = SKB_TRUESIZE(size);\n\tatomic_set(&skb->users, 1);\n\tskb->head = data;\n\tskb->data = data;\n\tskb_reset_tail_pointer(skb);\n\tskb->end = skb->tail + size;\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n\tskb->transport_header = (typeof(skb->transport_header))~0U;\n\n\t/* make sure we initialize shinfo sequentially */\n\tshinfo = skb_shinfo(skb);\n\tmemset(shinfo, 0, offsetof(struct skb_shared_info, dataref));\n\tatomic_set(&shinfo->dataref, 1);\n\tkmemcheck_annotate_variable(shinfo->destructor_arg);\n\n\treturn skb;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_095": {
        "vulnerable_code": {
            "Code": "__kfree_skb_defer(struct sk_buff *skb){\n\t_kfree_skb_defer(skb);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 2,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__kfree_skb_defer(struct sk_buff *skb){\n\t_kfree_skb_defer(skb);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_096": {
        "vulnerable_code": {
            "Code": "__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone){\n\tunsigned int size = skb_headlen(skb) + headroom;\n\tint flags = skb_alloc_rx_flag(skb) | (fclone ? SKB_ALLOC_FCLONE : 0);\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask, flags, NUMA_NO_NODE);\n\n\tif (!n)\n\t\tgoto out;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headroom);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb_headlen(skb));\n\t/* Copy the bytes */\n\tskb_copy_from_linear_data(skb, n->data, n->len);\n\n\tn->truesize += skb->data_len;\n\tn->data_len  = skb->data_len;\n\tn->len\t     = skb->len;\n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\tint i;\n\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree_skb(n);\n\t\t\tn = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_shinfo(n)->frags[i] = skb_shinfo(skb)->frags[i];\n\t\t\tskb_frag_ref(skb, i);\n\t\t}\n\t\tskb_shinfo(n)->nr_frags = i;\n\t}\n\n\tif (skb_has_frag_list(skb)) {\n\t\tskb_shinfo(n)->frag_list = skb_shinfo(skb)->frag_list;\n\t\tskb_clone_fraglist(n);\n\t}\n\n\tcopy_skb_header(n, skb);\nout:\n\treturn n;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone){\n\tunsigned int size = skb_headlen(skb) + headroom;\n\tint flags = skb_alloc_rx_flag(skb) | (fclone ? SKB_ALLOC_FCLONE : 0);\n\tstruct sk_buff *n = __alloc_skb(size, gfp_mask, flags, NUMA_NO_NODE);\n\n\tif (!n)\n\t\tgoto out;\n\n\t/* Set the data pointer */\n\tskb_reserve(n, headroom);\n\t/* Set the tail pointer and length */\n\tskb_put(n, skb_headlen(skb));\n\t/* Copy the bytes */\n\tskb_copy_from_linear_data(skb, n->data, n->len);\n\n\tn->truesize += skb->data_len;\n\tn->data_len  = skb->data_len;\n\tn->len\t     = skb->len;\n\n\tif (skb_shinfo(skb)->nr_frags) {\n\t\tint i;\n\n\t\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\t\tkfree_skb(n);\n\t\t\tn = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\t\tskb_shinfo(n)->frags[i] = skb_shinfo(skb)->frags[i];\n\t\t\tskb_frag_ref(skb, i);\n\t\t}\n\t\tskb_shinfo(n)->nr_frags = i;\n\t}\n\n\tif (skb_has_frag_list(skb)) {\n\t\tskb_shinfo(n)->frag_list = skb_shinfo(skb)->frag_list;\n\t\tskb_clone_fraglist(n);\n\t}\n\n\tcopy_skb_header(n, skb);\nout:\n\treturn n;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_097": {
        "vulnerable_code": {
            "Code": "__skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci){\n\tstruct vlan_hdr *vhdr;\n\tint offset = skb->data - skb_mac_header(skb);\n\tint err;\n\n\tif (WARN_ONCE(offset,\n\t\t      \"__skb_vlan_pop got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t      offset)) {\n\t\treturn -EINVAL;\n\t}\n\n\terr = skb_ensure_writable(skb, VLAN_ETH_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tskb_postpull_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\n\tvhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);\n\t*vlan_tci = ntohs(vhdr->h_vlan_TCI);\n\n\tmemmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);\n\t__skb_pull(skb, VLAN_HLEN);\n\n\tvlan_set_encap_proto(skb, vhdr);\n\tskb->mac_header += VLAN_HLEN;\n\n\tif (skb_network_offset(skb) < ETH_HLEN)\n\t\tskb_set_network_header(skb, ETH_HLEN);\n\n\tskb_reset_mac_len(skb);\n\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci){\n\tstruct vlan_hdr *vhdr;\n\tint offset = skb->data - skb_mac_header(skb);\n\tint err;\n\n\tif (WARN_ONCE(offset,\n\t\t      \"__skb_vlan_pop got skb with skb->data not at mac header (offset %d)\\n\",\n\t\t      offset)) {\n\t\treturn -EINVAL;\n\t}\n\n\terr = skb_ensure_writable(skb, VLAN_ETH_HLEN);\n\tif (unlikely(err))\n\t\treturn err;\n\n\tskb_postpull_rcsum(skb, skb->data + (2 * ETH_ALEN), VLAN_HLEN);\n\n\tvhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);\n\t*vlan_tci = ntohs(vhdr->h_vlan_TCI);\n\n\tmemmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);\n\t__skb_pull(skb, VLAN_HLEN);\n\n\tvlan_set_encap_proto(skb, vhdr);\n\tskb->mac_header += VLAN_HLEN;\n\n\tif (skb_network_offset(skb) < ETH_HLEN)\n\t\tskb_set_network_header(skb, ETH_HLEN);\n\n\tskb_reset_mac_len(skb);\n\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_098": {
        "vulnerable_code": {
            "Code": "_kfree_skb_defer(struct sk_buff *skb){\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\t/* drop skb->head and call any destructors for packet */\n\tskb_release_all(skb);\n\n\t/* record skb to CPU local list */\n\tnc->skb_cache[nc->skb_count++] = skb;\n\n#ifdef CONFIG_SLUB\n\t/* SLUB writes into objects when freeing */\n\tprefetchw(skb);\n#endif\n\n\t/* flush skb_cache if it is filled */\n\tif (unlikely(nc->skb_count == NAPI_SKB_CACHE_SIZE)) {\n\t\tkmem_cache_free_bulk(skbuff_head_cache, NAPI_SKB_CACHE_SIZE,\n\t\t\t\t     nc->skb_cache);\n\t\tnc->skb_count = 0;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "_kfree_skb_defer(struct sk_buff *skb){\n\tstruct napi_alloc_cache *nc = this_cpu_ptr(&napi_alloc_cache);\n\n\t/* drop skb->head and call any destructors for packet */\n\tskb_release_all(skb);\n\n\t/* record skb to CPU local list */\n\tnc->skb_cache[nc->skb_count++] = skb;\n\n#ifdef CONFIG_SLUB\n\t/* SLUB writes into objects when freeing */\n\tprefetchw(skb);\n#endif\n\n\t/* flush skb_cache if it is filled */\n\tif (unlikely(nc->skb_count == NAPI_SKB_CACHE_SIZE)) {\n\t\tkmem_cache_free_bulk(skbuff_head_cache, NAPI_SKB_CACHE_SIZE,\n\t\t\t\t     nc->skb_cache);\n\t\tnc->skb_count = 0;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_099": {
        "vulnerable_code": {
            "Code": "pskb_carve(struct sk_buff *skb, const u32 len, gfp_t gfp){\n\tint headlen = skb_headlen(skb);\n\n\tif (len < headlen)\n\t\treturn pskb_carve_inside_header(skb, len, headlen, gfp);\n\telse\n\t\treturn pskb_carve_inside_nonlinear(skb, len, headlen, gfp);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "pskb_carve(struct sk_buff *skb, const u32 len, gfp_t gfp){\n\tint headlen = skb_headlen(skb);\n\n\tif (len < headlen)\n\t\treturn pskb_carve_inside_header(skb, len, headlen, gfp);\n\telse\n\t\treturn pskb_carve_inside_nonlinear(skb, len, headlen, gfp);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_100": {
        "vulnerable_code": {
            "Code": "pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,\n\t\t\t\t       int pos, gfp_t gfp_mask){\n\tint i, k = 0;\n\tint size = skb_end_offset(skb);\n\tu8 *data;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\tstruct skb_shared_info *shinfo;\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size +\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb), offsetof(struct skb_shared_info,\n\t\t\t\t\t frags[skb_shinfo(skb)->nr_frags]));\n\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\tshinfo = (struct skb_shared_info *)(data + size);\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint fsize = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + fsize > off) {\n\t\t\tshinfo->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < off) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tshinfo->frags[0].page_offset += off - pos;\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[0], off - pos);\n\t\t\t}\n\t\t\tskb_frag_ref(skb, i);\n\t\t\tk++;\n\t\t}\n\t\tpos += fsize;\n\t}\n\tshinfo->nr_frags = k;\n\tif (skb_has_frag_list(skb))\n\t\tskb_clone_fraglist(skb);\n\n\tif (k == 0) {\n\t\t/* split line is in frag list */\n\t\tpskb_carve_frag_list(skb, shinfo, off - pos, gfp_mask);\n\t}\n\tskb_release_data(skb);\n\n\tskb->head = data;\n\tskb->head_frag = 0;\n\tskb->data = data;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end = size;\n#else\n\tskb->end = skb->head + size;\n#endif\n\tskb_reset_tail_pointer(skb);\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tskb->len -= off;\n\tskb->data_len = skb->len;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "pskb_carve_inside_nonlinear(struct sk_buff *skb, const u32 off,\n\t\t\t\t       int pos, gfp_t gfp_mask){\n\tint i, k = 0;\n\tint size = skb_end_offset(skb);\n\tu8 *data;\n\tconst int nfrags = skb_shinfo(skb)->nr_frags;\n\tstruct skb_shared_info *shinfo;\n\n\tsize = SKB_DATA_ALIGN(size);\n\n\tif (skb_pfmemalloc(skb))\n\t\tgfp_mask |= __GFP_MEMALLOC;\n\tdata = kmalloc_reserve(size +\n\t\t\t       SKB_DATA_ALIGN(sizeof(struct skb_shared_info)),\n\t\t\t       gfp_mask, NUMA_NO_NODE, NULL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsize = SKB_WITH_OVERHEAD(ksize(data));\n\n\tmemcpy((struct skb_shared_info *)(data + size),\n\t       skb_shinfo(skb), offsetof(struct skb_shared_info,\n\t\t\t\t\t frags[skb_shinfo(skb)->nr_frags]));\n\tif (skb_orphan_frags(skb, gfp_mask)) {\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\tshinfo = (struct skb_shared_info *)(data + size);\n\tfor (i = 0; i < nfrags; i++) {\n\t\tint fsize = skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\n\t\tif (pos + fsize > off) {\n\t\t\tshinfo->frags[k] = skb_shinfo(skb)->frags[i];\n\n\t\t\tif (pos < off) {\n\t\t\t\t/* Split frag.\n\t\t\t\t * We have two variants in this case:\n\t\t\t\t * 1. Move all the frag to the second\n\t\t\t\t *    part, if it is possible. F.e.\n\t\t\t\t *    this approach is mandatory for TUX,\n\t\t\t\t *    where splitting is expensive.\n\t\t\t\t * 2. Split is accurately. We make this.\n\t\t\t\t */\n\t\t\t\tshinfo->frags[0].page_offset += off - pos;\n\t\t\t\tskb_frag_size_sub(&shinfo->frags[0], off - pos);\n\t\t\t}\n\t\t\tskb_frag_ref(skb, i);\n\t\t\tk++;\n\t\t}\n\t\tpos += fsize;\n\t}\n\tshinfo->nr_frags = k;\n\tif (skb_has_frag_list(skb))\n\t\tskb_clone_fraglist(skb);\n\n\tif (k == 0) {\n\t\t/* split line is in frag list */\n\t\tpskb_carve_frag_list(skb, shinfo, off - pos, gfp_mask);\n\t}\n\tskb_release_data(skb);\n\n\tskb->head = data;\n\tskb->head_frag = 0;\n\tskb->data = data;\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\n\tskb->end = size;\n#else\n\tskb->end = skb->head + size;\n#endif\n\tskb_reset_tail_pointer(skb);\n\tskb_headers_offset_update(skb, 0);\n\tskb->cloned   = 0;\n\tskb->hdr_len  = 0;\n\tskb->nohdr    = 0;\n\tskb->len -= off;\n\tskb->data_len = skb->len;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_101": {
        "vulnerable_code": {
            "Code": "pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len){\n\tif (tail != skb) {\n\t\tskb->data_len += len;\n\t\tskb->len += len;\n\t}\n\treturn skb_put(tail, len);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len){\n\tif (tail != skb) {\n\t\tskb->data_len += len;\n\t\tskb->len += len;\n\t}\n\treturn skb_put(tail, len);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_102": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_103": {
        "vulnerable_code": {
            "Code": "skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps){\n\tstruct sock *sk = skb->sk;\n\n\tif (!skb_may_tx_timestamp(sk, false))\n\t\treturn;\n\n\t/* Take a reference to prevent skb_orphan() from freeing the socket,\n\t * but only if the socket refcount is not zero.\n\t */\n\tif (likely(atomic_inc_not_zero(&sk->sk_refcnt))) {\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\t\t__skb_complete_tx_timestamp(skb, sk, SCM_TSTAMP_SND);\n\t\tsock_put(sk);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps){\n\tstruct sock *sk = skb->sk;\n\n\tif (!skb_may_tx_timestamp(sk, false))\n\t\treturn;\n\n\t/* Take a reference to prevent skb_orphan() from freeing the socket,\n\t * but only if the socket refcount is not zero.\n\t */\n\tif (likely(atomic_inc_not_zero(&sk->sk_refcnt))) {\n\t\t*skb_hwtstamps(skb) = *hwtstamps;\n\t\t__skb_complete_tx_timestamp(skb, sk, SCM_TSTAMP_SND);\n\t\tsock_put(sk);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_104": {
        "vulnerable_code": {
            "Code": "skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer){\n\tint copyflag;\n\tint elt;\n\tstruct sk_buff *skb1, **skb_p;\n\n\t/* If skb is cloned or its head is paged, reallocate\n\t * head pulling out all the pages (pages are considered not writable\n\t * at the moment even if they are anonymous).\n\t */\n\tif ((skb_cloned(skb) || skb_shinfo(skb)->nr_frags) &&\n\t    __pskb_pull_tail(skb, skb_pagelen(skb)-skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\t/* Easy case. Most of packets will go this way. */\n\tif (!skb_has_frag_list(skb)) {\n\t\t/* A little of trouble, not enough of space for trailer.\n\t\t * This should not happen, when stack is tuned to generate\n\t\t * good frames. OK, on miss we reallocate and reserve even more\n\t\t * space, 128 bytes is fair. */\n\n\t\tif (skb_tailroom(skb) < tailbits &&\n\t\t    pskb_expand_head(skb, 0, tailbits-skb_tailroom(skb)+128, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\t/* Voila! */\n\t\t*trailer = skb;\n\t\treturn 1;\n\t}\n\n\t/* Misery. We are in troubles, going to mincer fragments... */\n\n\telt = 1;\n\tskb_p = &skb_shinfo(skb)->frag_list;\n\tcopyflag = 0;\n\n\twhile ((skb1 = *skb_p) != NULL) {\n\t\tint ntail = 0;\n\n\t\t/* The fragment is partially pulled by someone,\n\t\t * this can happen on input. Copy it and everything\n\t\t * after it. */\n\n\t\tif (skb_shared(skb1))\n\t\t\tcopyflag = 1;\n\n\t\t/* If the skb is the last, worry about trailer. */\n\n\t\tif (skb1->next == NULL && tailbits) {\n\t\t\tif (skb_shinfo(skb1)->nr_frags ||\n\t\t\t    skb_has_frag_list(skb1) ||\n\t\t\t    skb_tailroom(skb1) < tailbits)\n\t\t\t\tntail = tailbits + 128;\n\t\t}\n\n\t\tif (copyflag ||\n\t\t    skb_cloned(skb1) ||\n\t\t    ntail ||\n\t\t    skb_shinfo(skb1)->nr_frags ||\n\t\t    skb_has_frag_list(skb1)) {\n\t\t\tstruct sk_buff *skb2;\n\n\t\t\t/* Fuck, we are miserable poor guys... */\n\t\t\tif (ntail == 0)\n\t\t\t\tskb2 = skb_copy(skb1, GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tskb2 = skb_copy_expand(skb1,\n\t\t\t\t\t\t       skb_headroom(skb1),\n\t\t\t\t\t\t       ntail,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (unlikely(skb2 == NULL))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (skb1->sk)\n\t\t\t\tskb_set_owner_w(skb2, skb1->sk);\n\n\t\t\t/* Looking around. Are we still alive?\n\t\t\t * OK, link new skb, drop old one */\n\n\t\t\tskb2->next = skb1->next;\n\t\t\t*skb_p = skb2;\n\t\t\tkfree_skb(skb1);\n\t\t\tskb1 = skb2;\n\t\t}\n\t\telt++;\n\t\t*trailer = skb1;\n\t\tskb_p = &skb1->next;\n\t}\n\n\treturn elt;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer){\n\tint copyflag;\n\tint elt;\n\tstruct sk_buff *skb1, **skb_p;\n\n\t/* If skb is cloned or its head is paged, reallocate\n\t * head pulling out all the pages (pages are considered not writable\n\t * at the moment even if they are anonymous).\n\t */\n\tif ((skb_cloned(skb) || skb_shinfo(skb)->nr_frags) &&\n\t    __pskb_pull_tail(skb, skb_pagelen(skb)-skb_headlen(skb)) == NULL)\n\t\treturn -ENOMEM;\n\n\t/* Easy case. Most of packets will go this way. */\n\tif (!skb_has_frag_list(skb)) {\n\t\t/* A little of trouble, not enough of space for trailer.\n\t\t * This should not happen, when stack is tuned to generate\n\t\t * good frames. OK, on miss we reallocate and reserve even more\n\t\t * space, 128 bytes is fair. */\n\n\t\tif (skb_tailroom(skb) < tailbits &&\n\t\t    pskb_expand_head(skb, 0, tailbits-skb_tailroom(skb)+128, GFP_ATOMIC))\n\t\t\treturn -ENOMEM;\n\n\t\t/* Voila! */\n\t\t*trailer = skb;\n\t\treturn 1;\n\t}\n\n\t/* Misery. We are in troubles, going to mincer fragments... */\n\n\telt = 1;\n\tskb_p = &skb_shinfo(skb)->frag_list;\n\tcopyflag = 0;\n\n\twhile ((skb1 = *skb_p) != NULL) {\n\t\tint ntail = 0;\n\n\t\t/* The fragment is partially pulled by someone,\n\t\t * this can happen on input. Copy it and everything\n\t\t * after it. */\n\n\t\tif (skb_shared(skb1))\n\t\t\tcopyflag = 1;\n\n\t\t/* If the skb is the last, worry about trailer. */\n\n\t\tif (skb1->next == NULL && tailbits) {\n\t\t\tif (skb_shinfo(skb1)->nr_frags ||\n\t\t\t    skb_has_frag_list(skb1) ||\n\t\t\t    skb_tailroom(skb1) < tailbits)\n\t\t\t\tntail = tailbits + 128;\n\t\t}\n\n\t\tif (copyflag ||\n\t\t    skb_cloned(skb1) ||\n\t\t    ntail ||\n\t\t    skb_shinfo(skb1)->nr_frags ||\n\t\t    skb_has_frag_list(skb1)) {\n\t\t\tstruct sk_buff *skb2;\n\n\t\t\t/* Fuck, we are miserable poor guys... */\n\t\t\tif (ntail == 0)\n\t\t\t\tskb2 = skb_copy(skb1, GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tskb2 = skb_copy_expand(skb1,\n\t\t\t\t\t\t       skb_headroom(skb1),\n\t\t\t\t\t\t       ntail,\n\t\t\t\t\t\t       GFP_ATOMIC);\n\t\t\tif (unlikely(skb2 == NULL))\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (skb1->sk)\n\t\t\t\tskb_set_owner_w(skb2, skb1->sk);\n\n\t\t\t/* Looking around. Are we still alive?\n\t\t\t * OK, link new skb, drop old one */\n\n\t\t\tskb2->next = skb1->next;\n\t\t\t*skb_p = skb2;\n\t\t\tkfree_skb(skb1);\n\t\t\tskb1 = skb2;\n\t\t}\n\t\telt++;\n\t\t*trailer = skb1;\n\t\tskb_p = &skb1->next;\n\t}\n\n\treturn elt;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_105": {
        "vulnerable_code": {
            "Code": "skb_headers_offset_update(struct sk_buff *skb, int off){\n\t/* Only adjust this if it actually is csum_start rather than csum */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tskb->csum_start += off;\n\t/* {transport,network,mac}_header and tail are relative to skb->head */\n\tskb->transport_header += off;\n\tskb->network_header   += off;\n\tif (skb_mac_header_was_set(skb))\n\t\tskb->mac_header += off;\n\tskb->inner_transport_header += off;\n\tskb->inner_network_header += off;\n\tskb->inner_mac_header += off;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_headers_offset_update(struct sk_buff *skb, int off){\n\t/* Only adjust this if it actually is csum_start rather than csum */\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tskb->csum_start += off;\n\t/* {transport,network,mac}_header and tail are relative to skb->head */\n\tskb->transport_header += off;\n\tskb->network_header   += off;\n\tif (skb_mac_header_was_set(skb))\n\t\tskb->mac_header += off;\n\tskb->inner_transport_header += off;\n\tskb->inner_network_header += off;\n\tskb->inner_mac_header += off;\n}",
            "Size": 4,
            "Code Complexity": 0,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_106": {
        "vulnerable_code": {
            "Code": "skb_over_panic(struct sk_buff *skb, unsigned int sz, void *addr){\n\tskb_panic(skb, sz, addr, __func__);\n}",
            "Size": 3,
            "Code Complexity": 0,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_over_panic(struct sk_buff *skb, unsigned int sz, void *addr){\n\tskb_panic(skb, sz, addr, __func__);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_107": {
        "vulnerable_code": {
            "Code": "skb_prepare_for_shift(struct sk_buff *skb){\n\treturn skb_cloned(skb) && pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_prepare_for_shift(struct sk_buff *skb){\n\treturn skb_cloned(skb) && pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_108": {
        "vulnerable_code": {
            "Code": "skb_queue_purge(struct sk_buff_head *list){\n\tstruct sk_buff *skb;\n\twhile ((skb = skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_queue_purge(struct sk_buff_head *list){\n\tstruct sk_buff *skb;\n\twhile ((skb = skb_dequeue(list)) != NULL)\n\t\tkfree_skb(skb);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_109": {
        "vulnerable_code": {
            "Code": "skb_rbtree_purge(struct rb_root *root){\n\tstruct sk_buff *skb, *next;\n\n\trbtree_postorder_for_each_entry_safe(skb, next, root, rbnode)\n\t\tkfree_skb(skb);\n\n\t*root = RB_ROOT;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_rbtree_purge(struct rb_root *root){\n\tstruct sk_buff *skb, *next;\n\n\trbtree_postorder_for_each_entry_safe(skb, next, root, rbnode)\n\t\tkfree_skb(skb);\n\n\t*root = RB_ROOT;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_110": {
        "vulnerable_code": {
            "Code": "skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen){\n\tint from, to, merge, todo;\n\tstruct skb_frag_struct *fragfrom, *fragto;\n\n\tBUG_ON(shiftlen > skb->len);\n\n\tif (skb_headlen(skb))\n\t\treturn 0;\n\n\ttodo = shiftlen;\n\tfrom = 0;\n\tto = skb_shinfo(tgt)->nr_frags;\n\tfragfrom = &skb_shinfo(skb)->frags[from];\n\n\t/* Actual merge is delayed until the point when we know we can\n\t * commit all, so that we don't have to undo partial changes\n\t */\n\tif (!to ||\n\t    !skb_can_coalesce(tgt, to, skb_frag_page(fragfrom),\n\t\t\t      fragfrom->page_offset)) {\n\t\tmerge = -1;\n\t} else {\n\t\tmerge = to - 1;\n\n\t\ttodo -= skb_frag_size(fragfrom);\n\t\tif (todo < 0) {\n\t\t\tif (skb_prepare_for_shift(skb) ||\n\t\t\t    skb_prepare_for_shift(tgt))\n\t\t\t\treturn 0;\n\n\t\t\t/* All previous frag pointers might be stale! */\n\t\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\t\tskb_frag_size_add(fragto, shiftlen);\n\t\t\tskb_frag_size_sub(fragfrom, shiftlen);\n\t\t\tfragfrom->page_offset += shiftlen;\n\n\t\t\tgoto onlymerged;\n\t\t}\n\n\t\tfrom++;\n\t}\n\n\t/* Skip full, not-fitting skb to avoid expensive operations */\n\tif ((shiftlen == skb->len) &&\n\t    (skb_shinfo(skb)->nr_frags - from) > (MAX_SKB_FRAGS - to))\n\t\treturn 0;\n\n\tif (skb_prepare_for_shift(skb) || skb_prepare_for_shift(tgt))\n\t\treturn 0;\n\n\twhile ((todo > 0) && (from < skb_shinfo(skb)->nr_frags)) {\n\t\tif (to == MAX_SKB_FRAGS)\n\t\t\treturn 0;\n\n\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\tfragto = &skb_shinfo(tgt)->frags[to];\n\n\t\tif (todo >= skb_frag_size(fragfrom)) {\n\t\t\t*fragto = *fragfrom;\n\t\t\ttodo -= skb_frag_size(fragfrom);\n\t\t\tfrom++;\n\t\t\tto++;\n\n\t\t} else {\n\t\t\t__skb_frag_ref(fragfrom);\n\t\t\tfragto->page = fragfrom->page;\n\t\t\tfragto->page_offset = fragfrom->page_offset;\n\t\t\tskb_frag_size_set(fragto, todo);\n\n\t\t\tfragfrom->page_offset += todo;\n\t\t\tskb_frag_size_sub(fragfrom, todo);\n\t\t\ttodo = 0;\n\n\t\t\tto++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ready to \"commit\" this state change to tgt */\n\tskb_shinfo(tgt)->nr_frags = to;\n\n\tif (merge >= 0) {\n\t\tfragfrom = &skb_shinfo(skb)->frags[0];\n\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\tskb_frag_size_add(fragto, skb_frag_size(fragfrom));\n\t\t__skb_frag_unref(fragfrom);\n\t}\n\n\t/* Reposition in the original skb */\n\tto = 0;\n\twhile (from < skb_shinfo(skb)->nr_frags)\n\t\tskb_shinfo(skb)->frags[to++] = skb_shinfo(skb)->frags[from++];\n\tskb_shinfo(skb)->nr_frags = to;\n\n\tBUG_ON(todo > 0 && !skb_shinfo(skb)->nr_frags);\n\nonlymerged:\n\t/* Most likely the tgt won't ever need its checksum anymore, skb on\n\t * the other hand might need it if it needs to be resent\n\t */\n\ttgt->ip_summed = CHECKSUM_PARTIAL;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t/* Yak, is it really working this way? Some helper please? */\n\tskb->len -= shiftlen;\n\tskb->data_len -= shiftlen;\n\tskb->truesize -= shiftlen;\n\ttgt->len += shiftlen;\n\ttgt->data_len += shiftlen;\n\ttgt->truesize += shiftlen;\n\n\treturn shiftlen;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen){\n\tint from, to, merge, todo;\n\tstruct skb_frag_struct *fragfrom, *fragto;\n\n\tBUG_ON(shiftlen > skb->len);\n\n\tif (skb_headlen(skb))\n\t\treturn 0;\n\n\ttodo = shiftlen;\n\tfrom = 0;\n\tto = skb_shinfo(tgt)->nr_frags;\n\tfragfrom = &skb_shinfo(skb)->frags[from];\n\n\t/* Actual merge is delayed until the point when we know we can\n\t * commit all, so that we don't have to undo partial changes\n\t */\n\tif (!to ||\n\t    !skb_can_coalesce(tgt, to, skb_frag_page(fragfrom),\n\t\t\t      fragfrom->page_offset)) {\n\t\tmerge = -1;\n\t} else {\n\t\tmerge = to - 1;\n\n\t\ttodo -= skb_frag_size(fragfrom);\n\t\tif (todo < 0) {\n\t\t\tif (skb_prepare_for_shift(skb) ||\n\t\t\t    skb_prepare_for_shift(tgt))\n\t\t\t\treturn 0;\n\n\t\t\t/* All previous frag pointers might be stale! */\n\t\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\t\tskb_frag_size_add(fragto, shiftlen);\n\t\t\tskb_frag_size_sub(fragfrom, shiftlen);\n\t\t\tfragfrom->page_offset += shiftlen;\n\n\t\t\tgoto onlymerged;\n\t\t}\n\n\t\tfrom++;\n\t}\n\n\t/* Skip full, not-fitting skb to avoid expensive operations */\n\tif ((shiftlen == skb->len) &&\n\t    (skb_shinfo(skb)->nr_frags - from) > (MAX_SKB_FRAGS - to))\n\t\treturn 0;\n\n\tif (skb_prepare_for_shift(skb) || skb_prepare_for_shift(tgt))\n\t\treturn 0;\n\n\twhile ((todo > 0) && (from < skb_shinfo(skb)->nr_frags)) {\n\t\tif (to == MAX_SKB_FRAGS)\n\t\t\treturn 0;\n\n\t\tfragfrom = &skb_shinfo(skb)->frags[from];\n\t\tfragto = &skb_shinfo(tgt)->frags[to];\n\n\t\tif (todo >= skb_frag_size(fragfrom)) {\n\t\t\t*fragto = *fragfrom;\n\t\t\ttodo -= skb_frag_size(fragfrom);\n\t\t\tfrom++;\n\t\t\tto++;\n\n\t\t} else {\n\t\t\t__skb_frag_ref(fragfrom);\n\t\t\tfragto->page = fragfrom->page;\n\t\t\tfragto->page_offset = fragfrom->page_offset;\n\t\t\tskb_frag_size_set(fragto, todo);\n\n\t\t\tfragfrom->page_offset += todo;\n\t\t\tskb_frag_size_sub(fragfrom, todo);\n\t\t\ttodo = 0;\n\n\t\t\tto++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/* Ready to \"commit\" this state change to tgt */\n\tskb_shinfo(tgt)->nr_frags = to;\n\n\tif (merge >= 0) {\n\t\tfragfrom = &skb_shinfo(skb)->frags[0];\n\t\tfragto = &skb_shinfo(tgt)->frags[merge];\n\n\t\tskb_frag_size_add(fragto, skb_frag_size(fragfrom));\n\t\t__skb_frag_unref(fragfrom);\n\t}\n\n\t/* Reposition in the original skb */\n\tto = 0;\n\twhile (from < skb_shinfo(skb)->nr_frags)\n\t\tskb_shinfo(skb)->frags[to++] = skb_shinfo(skb)->frags[from++];\n\tskb_shinfo(skb)->nr_frags = to;\n\n\tBUG_ON(todo > 0 && !skb_shinfo(skb)->nr_frags);\n\nonlymerged:\n\t/* Most likely the tgt won't ever need its checksum anymore, skb on\n\t * the other hand might need it if it needs to be resent\n\t */\n\ttgt->ip_summed = CHECKSUM_PARTIAL;\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t/* Yak, is it really working this way? Some helper please? */\n\tskb->len -= shiftlen;\n\tskb->data_len -= shiftlen;\n\tskb->truesize -= shiftlen;\n\ttgt->len += shiftlen;\n\ttgt->data_len += shiftlen;\n\ttgt->truesize += shiftlen;\n\n\treturn shiftlen;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_111": {
        "vulnerable_code": {
            "Code": "skb_trim(struct sk_buff *skb, unsigned int len){\n\tif (skb->len > len)\n\t\t__skb_trim(skb, len);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_trim(struct sk_buff *skb, unsigned int len){\n\tif (skb->len > len)\n\t\t__skb_trim(skb, len);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_112": {
        "vulnerable_code": {
            "Code": "skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr){\n\tskb_panic(skb, sz, addr, __func__);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_under_panic(struct sk_buff *skb, unsigned int sz, void *addr){\n\tskb_panic(skb, sz, addr, __func__);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_113": {
        "vulnerable_code": {
            "Code": "skb_zerocopy(struct sk_buff *to, struct sk_buff *from, int len, int hlen){\n\tint i, j = 0;\n\tint plen = 0; /* length of skb->head fragment */\n\tint ret;\n\tstruct page *page;\n\tunsigned int offset;\n\n\tBUG_ON(!from->head_frag && !hlen);\n\n\t/* dont bother with small payloads */\n\tif (len <= skb_tailroom(to))\n\t\treturn skb_copy_bits(from, 0, skb_put(to, len), len);\n\n\tif (hlen) {\n\t\tret = skb_copy_bits(from, 0, skb_put(to, hlen), hlen);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\tlen -= hlen;\n\t} else {\n\t\tplen = min_t(int, skb_headlen(from), len);\n\t\tif (plen) {\n\t\t\tpage = virt_to_head_page(from->head);\n\t\t\toffset = from->data - (unsigned char *)page_address(page);\n\t\t\t__skb_fill_page_desc(to, 0, page, offset, plen);\n\t\t\tget_page(page);\n\t\t\tj = 1;\n\t\t\tlen -= plen;\n\t\t}\n\t}\n\n\tto->truesize += len + plen;\n\tto->len += len + plen;\n\tto->data_len += len + plen;\n\n\tif (unlikely(skb_orphan_frags(from, GFP_ATOMIC))) {\n\t\tskb_tx_error(from);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++) {\n\t\tif (!len)\n\t\t\tbreak;\n\t\tskb_shinfo(to)->frags[j] = skb_shinfo(from)->frags[i];\n\t\tskb_shinfo(to)->frags[j].size = min_t(int, skb_shinfo(to)->frags[j].size, len);\n\t\tlen -= skb_shinfo(to)->frags[j].size;\n\t\tskb_frag_ref(to, j);\n\t\tj++;\n\t}\n\tskb_shinfo(to)->nr_frags = j;\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skb_zerocopy(struct sk_buff *to, struct sk_buff *from, int len, int hlen){\n\tint i, j = 0;\n\tint plen = 0; /* length of skb->head fragment */\n\tint ret;\n\tstruct page *page;\n\tunsigned int offset;\n\n\tBUG_ON(!from->head_frag && !hlen);\n\n\t/* dont bother with small payloads */\n\tif (len <= skb_tailroom(to))\n\t\treturn skb_copy_bits(from, 0, skb_put(to, len), len);\n\n\tif (hlen) {\n\t\tret = skb_copy_bits(from, 0, skb_put(to, hlen), hlen);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\tlen -= hlen;\n\t} else {\n\t\tplen = min_t(int, skb_headlen(from), len);\n\t\tif (plen) {\n\t\t\tpage = virt_to_head_page(from->head);\n\t\t\toffset = from->data - (unsigned char *)page_address(page);\n\t\t\t__skb_fill_page_desc(to, 0, page, offset, plen);\n\t\t\tget_page(page);\n\t\t\tj = 1;\n\t\t\tlen -= plen;\n\t\t}\n\t}\n\n\tto->truesize += len + plen;\n\tto->len += len + plen;\n\tto->data_len += len + plen;\n\n\tif (unlikely(skb_orphan_frags(from, GFP_ATOMIC))) {\n\t\tskb_tx_error(from);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(from)->nr_frags; i++) {\n\t\tif (!len)\n\t\t\tbreak;\n\t\tskb_shinfo(to)->frags[j] = skb_shinfo(from)->frags[i];\n\t\tskb_shinfo(to)->frags[j].size = min_t(int, skb_shinfo(to)->frags[j].size, len);\n\t\tlen -= skb_shinfo(to)->frags[j].size;\n\t\tskb_frag_ref(to, j);\n\t\tj++;\n\t}\n\tskb_shinfo(to)->nr_frags = j;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_114": {
        "vulnerable_code": {
            "Code": "sock_queue_err_skb(struct sock *sk, struct sk_buff *skb){\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned int)sk->sk_rcvbuf)\n\t\treturn -ENOMEM;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rmem_free;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\n\t/* before exiting rcu section, make sure dst is refcounted */\n\tskb_dst_force(skb);\n\n\tskb_queue_tail(&sk->sk_error_queue, skb);\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sock_queue_err_skb(struct sock *sk, struct sk_buff *skb){\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned int)sk->sk_rcvbuf)\n\t\treturn -ENOMEM;\n\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rmem_free;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tskb_set_err_queue(skb);\n\n\t/* before exiting rcu section, make sure dst is refcounted */\n\tskb_dst_force(skb);\n\n\tskb_queue_tail(&sk->sk_error_queue, skb);\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_115": {
        "vulnerable_code": {
            "Code": "sock_rmem_free(struct sk_buff *skb){\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n}",
            "Size": 5,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sock_rmem_free(struct sk_buff *skb){\n\tstruct sock *sk = skb->sk;\n\n\tatomic_sub(skb->truesize, &sk->sk_rmem_alloc);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_116": {
        "vulnerable_code": {
            "Code": "sock_spd_release(struct splice_pipe_desc *spd, unsigned int i){\n\tput_page(spd->pages[i]);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sock_spd_release(struct splice_pipe_desc *spd, unsigned int i){\n\tput_page(spd->pages[i]);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_117": {
        "vulnerable_code": {
            "Code": "__sock_recv_timestamp(struct msghdr *msg, struct sock *sk,\n\tstruct sk_buff *skb){\n\tint need_software_tstamp = sock_flag(sk, SOCK_RCVTSTAMP);\n\tstruct scm_timestamping tss;\n\tint empty = 1;\n\tstruct skb_shared_hwtstamps *shhwtstamps =\n\t\tskb_hwtstamps(skb);\n\n\t/* Race occurred between timestamp enabling and packet\n\t   receiving.  Fill in the current time for now. */\n\tif (need_software_tstamp && skb->tstamp == 0)\n\t\t__net_timestamp(skb);\n\n\tif (need_software_tstamp) {\n\t\tif (!sock_flag(sk, SOCK_RCVTSTAMPNS)) {\n\t\t\tstruct timeval tv;\n\t\t\tskb_get_timestamp(skb, &tv);\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMP,\n\t\t\t\t sizeof(tv), &tv);\n\t\t} else {\n\t\t\tstruct timespec ts;\n\t\t\tskb_get_timestampns(skb, &ts);\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPNS,\n\t\t\t\t sizeof(ts), &ts);\n\t\t}\n\t}\n\n\tmemset(&tss, 0, sizeof(tss));\n\tif ((sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE) &&\n\t    ktime_to_timespec_cond(skb->tstamp, tss.ts + 0))\n\t\tempty = 0;\n\tif (shhwtstamps &&\n\t    (sk->sk_tsflags & SOF_TIMESTAMPING_RAW_HARDWARE) &&\n\t    ktime_to_timespec_cond(shhwtstamps->hwtstamp, tss.ts + 2))\n\t\tempty = 0;\n\tif (!empty) {\n\t\tput_cmsg(msg, SOL_SOCKET,\n\t\t\t SCM_TIMESTAMPING, sizeof(tss), &tss);\n\n\t\tif (skb->len && (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS))\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPING_OPT_STATS,\n\t\t\t\t skb->len, skb->data);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__sock_recv_timestamp(struct msghdr *msg, struct sock *sk,\n\tstruct sk_buff *skb){\n\tint need_software_tstamp = sock_flag(sk, SOCK_RCVTSTAMP);\n\tstruct scm_timestamping tss;\n\tint empty = 1;\n\tstruct skb_shared_hwtstamps *shhwtstamps =\n\t\tskb_hwtstamps(skb);\n\n\t/* Race occurred between timestamp enabling and packet\n\t   receiving.  Fill in the current time for now. */\n\tif (need_software_tstamp && skb->tstamp == 0)\n\t\t__net_timestamp(skb);\n\n\tif (need_software_tstamp) {\n\t\tif (!sock_flag(sk, SOCK_RCVTSTAMPNS)) {\n\t\t\tstruct timeval tv;\n\t\t\tskb_get_timestamp(skb, &tv);\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMP,\n\t\t\t\t sizeof(tv), &tv);\n\t\t} else {\n\t\t\tstruct timespec ts;\n\t\t\tskb_get_timestampns(skb, &ts);\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPNS,\n\t\t\t\t sizeof(ts), &ts);\n\t\t}\n\t}\n\n\tmemset(&tss, 0, sizeof(tss));\n\tif ((sk->sk_tsflags & SOF_TIMESTAMPING_SOFTWARE) &&\n\t    ktime_to_timespec_cond(skb->tstamp, tss.ts + 0))\n\t\tempty = 0;\n\tif (shhwtstamps &&\n\t    (sk->sk_tsflags & SOF_TIMESTAMPING_RAW_HARDWARE) &&\n\t    ktime_to_timespec_cond(shhwtstamps->hwtstamp, tss.ts + 2))\n\t\tempty = 0;\n\tif (!empty) {\n\t\tput_cmsg(msg, SOL_SOCKET,\n\t\t\t SCM_TIMESTAMPING, sizeof(tss), &tss);\n\n\t\tif (skb_is_err_queue(skb) && skb->len &&\n\t\t    (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS))\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPING_OPT_STATS,\n\t\t\t\t skb->len, skb->data);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_118": {
        "vulnerable_code": {
            "Code": "__sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags){\n\tu8 flags = *tx_flags;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_HARDWARE)\n\t\tflags |= SKBTX_HW_TSTAMP;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_SOFTWARE)\n\t\tflags |= SKBTX_SW_TSTAMP;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_SCHED)\n\t\tflags |= SKBTX_SCHED_TSTAMP;\n\n\t*tx_flags = flags;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags){\n\tu8 flags = *tx_flags;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_HARDWARE)\n\t\tflags |= SKBTX_HW_TSTAMP;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_SOFTWARE)\n\t\tflags |= SKBTX_SW_TSTAMP;\n\n\tif (tsflags & SOF_TIMESTAMPING_TX_SCHED)\n\t\tflags |= SKBTX_SCHED_TSTAMP;\n\n\t*tx_flags = flags;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_119": {
        "vulnerable_code": {
            "Code": "kernel_sock_ioctl(struct socket *sock, int cmd, unsigned long arg){\n\tmm_segment_t oldfs = get_fs();\n\tint err;\n\n\tset_fs(KERNEL_DS);\n\terr = sock->ops->ioctl(sock, cmd, arg);\n\tset_fs(oldfs);\n\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "kernel_sock_ioctl(struct socket *sock, int cmd, unsigned long arg){\n\tmm_segment_t oldfs = get_fs();\n\tint err;\n\n\tset_fs(KERNEL_DS);\n\terr = sock->ops->ioctl(sock, cmd, arg);\n\tset_fs(oldfs);\n\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_120": {
        "vulnerable_code": {
            "Code": "sock_alloc(void){\n\tstruct inode *inode;\n\tstruct socket *sock;\n\n\tinode = new_inode_pseudo(sock_mnt->mnt_sb);\n\tif (!inode)\n\t\treturn NULL;\n\n\tsock = SOCKET_I(inode);\n\n\tkmemcheck_annotate_bitfield(sock, type);\n\tinode->i_ino = get_next_ino();\n\tinode->i_mode = S_IFSOCK | S_IRWXUGO;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_op = &sockfs_inode_ops;\n\n\tthis_cpu_add(sockets_in_use, 1);\n\treturn sock;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sock_alloc(void){\n\tstruct inode *inode;\n\tstruct socket *sock;\n\n\tinode = new_inode_pseudo(sock_mnt->mnt_sb);\n\tif (!inode)\n\t\treturn NULL;\n\n\tsock = SOCKET_I(inode);\n\n\tkmemcheck_annotate_bitfield(sock, type);\n\tinode->i_ino = get_next_ino();\n\tinode->i_mode = S_IFSOCK | S_IRWXUGO;\n\tinode->i_uid = current_fsuid();\n\tinode->i_gid = current_fsgid();\n\tinode->i_op = &sockfs_inode_ops;\n\n\tthis_cpu_add(sockets_in_use, 1);\n\treturn sock;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_121": {
        "vulnerable_code": {
            "Code": "sock_alloc_inode(struct super_block *sb){\n\tstruct socket_alloc *ei;\n\tstruct socket_wq *wq;\n\n\tei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL);\n\tif (!ei)\n\t\treturn NULL;\n\twq = kmalloc(sizeof(*wq), GFP_KERNEL);\n\tif (!wq) {\n\t\tkmem_cache_free(sock_inode_cachep, ei);\n\t\treturn NULL;\n\t}\n\tinit_waitqueue_head(&wq->wait);\n\twq->fasync_list = NULL;\n\twq->flags = 0;\n\tRCU_INIT_POINTER(ei->socket.wq, wq);\n\n\tei->socket.state = SS_UNCONNECTED;\n\tei->socket.flags = 0;\n\tei->socket.ops = NULL;\n\tei->socket.sk = NULL;\n\tei->socket.file = NULL;\n\n\treturn &ei->vfs_inode;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sock_alloc_inode(struct super_block *sb){\n\tstruct socket_alloc *ei;\n\tstruct socket_wq *wq;\n\n\tei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL);\n\tif (!ei)\n\t\treturn NULL;\n\twq = kmalloc(sizeof(*wq), GFP_KERNEL);\n\tif (!wq) {\n\t\tkmem_cache_free(sock_inode_cachep, ei);\n\t\treturn NULL;\n\t}\n\tinit_waitqueue_head(&wq->wait);\n\twq->fasync_list = NULL;\n\twq->flags = 0;\n\tRCU_INIT_POINTER(ei->socket.wq, wq);\n\n\tei->socket.state = SS_UNCONNECTED;\n\tei->socket.flags = 0;\n\tei->socket.ops = NULL;\n\tei->socket.sk = NULL;\n\tei->socket.file = NULL;\n\n\treturn &ei->vfs_inode;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_122": {
        "vulnerable_code": {
            "Code": "sock_do_ioctl(struct net *net, struct socket *sock,\n\t\t\t\t unsigned int cmd, unsigned long arg){\n\tint err;\n\tvoid __user *argp = (void __user *)arg;\n\n\terr = sock->ops->ioctl(sock, cmd, arg);\n\n\t/*\n\t * If this ioctl is unknown try to hand it down\n\t * to the NIC driver.\n\t */\n\tif (err == -ENOIOCTLCMD)\n\t\terr = dev_ioctl(net, cmd, argp);\n\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sock_do_ioctl(struct net *net, struct socket *sock,\n\t\t\t\t unsigned int cmd, unsigned long arg){\n\tint err;\n\tvoid __user *argp = (void __user *)arg;\n\n\terr = sock->ops->ioctl(sock, cmd, arg);\n\n\t/*\n\t * If this ioctl is unknown try to hand it down\n\t * to the NIC driver.\n\t */\n\tif (err == -ENOIOCTLCMD)\n\t\terr = dev_ioctl(net, cmd, argp);\n\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_123": {
        "vulnerable_code": {
            "Code": "sock_map_fd(struct socket *sock, int flags){\n\tstruct file *newfile;\n\tint fd = get_unused_fd_flags(flags);\n\tif (unlikely(fd < 0))\n\t\treturn fd;\n\n\tnewfile = sock_alloc_file(sock, flags, NULL);\n\tif (likely(!IS_ERR(newfile))) {\n\t\tfd_install(fd, newfile);\n\t\treturn fd;\n\t}\n\n\tput_unused_fd(fd);\n\treturn PTR_ERR(newfile);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sock_map_fd(struct socket *sock, int flags){\n\tstruct file *newfile;\n\tint fd = get_unused_fd_flags(flags);\n\tif (unlikely(fd < 0))\n\t\treturn fd;\n\n\tnewfile = sock_alloc_file(sock, flags, NULL);\n\tif (likely(!IS_ERR(newfile))) {\n\t\tfd_install(fd, newfile);\n\t\treturn fd;\n\t}\n\n\tput_unused_fd(fd);\n\treturn PTR_ERR(newfile);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_124": {
        "vulnerable_code": {
            "Code": "sockfd_lookup(int fd, int *err){\n\tstruct file *file;\n\tstruct socket *sock;\n\n\tfile = fget(fd);\n\tif (!file) {\n\t\t*err = -EBADF;\n\t\treturn NULL;\n\t}\n\n\tsock = sock_from_file(file, err);\n\tif (!sock)\n\t\tfput(file);\n\treturn sock;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sockfd_lookup(int fd, int *err){\n\tstruct file *file;\n\tstruct socket *sock;\n\n\tfile = fget(fd);\n\tif (!file) {\n\t\t*err = -EBADF;\n\t\treturn NULL;\n\t}\n\n\tsock = sock_from_file(file, err);\n\tif (!sock)\n\t\tfput(file);\n\treturn sock;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_125": {
        "vulnerable_code": {
            "Code": "attach_auth(struct xfrm_algo_auth **algpp, u8 *props,\n\t\t       struct nlattr *rta){\n\tstruct xfrm_algo *ualg;\n\tstruct xfrm_algo_auth *p;\n\tstruct xfrm_algo_desc *algo;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tualg = nla_data(rta);\n\n\talgo = xfrm_aalg_get_byname(ualg->alg_name, 1);\n\tif (!algo)\n\t\treturn -ENOSYS;\n\t*props = algo->desc.sadb_alg_id;\n\n\tp = kmalloc(sizeof(*p) + (ualg->alg_key_len + 7) / 8, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tstrcpy(p->alg_name, algo->name);\n\tp->alg_key_len = ualg->alg_key_len;\n\tp->alg_trunc_len = algo->uinfo.auth.icv_truncbits;\n\tmemcpy(p->alg_key, ualg->alg_key, (ualg->alg_key_len + 7) / 8);\n\n\t*algpp = p;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "attach_auth(struct xfrm_algo_auth **algpp, u8 *props,\n\t\t       struct nlattr *rta){\n\tstruct xfrm_algo *ualg;\n\tstruct xfrm_algo_auth *p;\n\tstruct xfrm_algo_desc *algo;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tualg = nla_data(rta);\n\n\talgo = xfrm_aalg_get_byname(ualg->alg_name, 1);\n\tif (!algo)\n\t\treturn -ENOSYS;\n\t*props = algo->desc.sadb_alg_id;\n\n\tp = kmalloc(sizeof(*p) + (ualg->alg_key_len + 7) / 8, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tstrcpy(p->alg_name, algo->name);\n\tp->alg_key_len = ualg->alg_key_len;\n\tp->alg_trunc_len = algo->uinfo.auth.icv_truncbits;\n\tmemcpy(p->alg_key, ualg->alg_key, (ualg->alg_key_len + 7) / 8);\n\n\t*algpp = p;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_126": {
        "vulnerable_code": {
            "Code": "attach_auth_trunc(struct xfrm_algo_auth **algpp, u8 *props,\n\t\t\t     struct nlattr *rta){\n\tstruct xfrm_algo_auth *p, *ualg;\n\tstruct xfrm_algo_desc *algo;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tualg = nla_data(rta);\n\n\talgo = xfrm_aalg_get_byname(ualg->alg_name, 1);\n\tif (!algo)\n\t\treturn -ENOSYS;\n\tif (ualg->alg_trunc_len > algo->uinfo.auth.icv_fullbits)\n\t\treturn -EINVAL;\n\t*props = algo->desc.sadb_alg_id;\n\n\tp = kmemdup(ualg, xfrm_alg_auth_len(ualg), GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tstrcpy(p->alg_name, algo->name);\n\tif (!p->alg_trunc_len)\n\t\tp->alg_trunc_len = algo->uinfo.auth.icv_truncbits;\n\n\t*algpp = p;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "attach_auth_trunc(struct xfrm_algo_auth **algpp, u8 *props,\n\t\t\t     struct nlattr *rta){\n\tstruct xfrm_algo_auth *p, *ualg;\n\tstruct xfrm_algo_desc *algo;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tualg = nla_data(rta);\n\n\talgo = xfrm_aalg_get_byname(ualg->alg_name, 1);\n\tif (!algo)\n\t\treturn -ENOSYS;\n\tif (ualg->alg_trunc_len > algo->uinfo.auth.icv_fullbits)\n\t\treturn -EINVAL;\n\t*props = algo->desc.sadb_alg_id;\n\n\tp = kmemdup(ualg, xfrm_alg_auth_len(ualg), GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tstrcpy(p->alg_name, algo->name);\n\tif (!p->alg_trunc_len)\n\t\tp->alg_trunc_len = algo->uinfo.auth.icv_truncbits;\n\n\t*algpp = p;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_127": {
        "vulnerable_code": {
            "Code": "attach_crypt(struct xfrm_state *x, struct nlattr *rta){\n\tstruct xfrm_algo *p, *ualg;\n\tstruct xfrm_algo_desc *algo;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tualg = nla_data(rta);\n\n\talgo = xfrm_ealg_get_byname(ualg->alg_name, 1);\n\tif (!algo)\n\t\treturn -ENOSYS;\n\tx->props.ealgo = algo->desc.sadb_alg_id;\n\n\tp = kmemdup(ualg, xfrm_alg_len(ualg), GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tstrcpy(p->alg_name, algo->name);\n\tx->ealg = p;\n\tx->geniv = algo->uinfo.encr.geniv;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "attach_crypt(struct xfrm_state *x, struct nlattr *rta){\n\tstruct xfrm_algo *p, *ualg;\n\tstruct xfrm_algo_desc *algo;\n\n\tif (!rta)\n\t\treturn 0;\n\n\tualg = nla_data(rta);\n\n\talgo = xfrm_ealg_get_byname(ualg->alg_name, 1);\n\tif (!algo)\n\t\treturn -ENOSYS;\n\tx->props.ealgo = algo->desc.sadb_alg_id;\n\n\tp = kmemdup(ualg, xfrm_alg_len(ualg), GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tstrcpy(p->alg_name, algo->name);\n\tx->ealg = p;\n\tx->geniv = algo->uinfo.encr.geniv;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_128": {
        "vulnerable_code": {
            "Code": "build_expire(struct sk_buff *skb, struct xfrm_state *x, const struct km_event *c){\n\tstruct xfrm_user_expire *ue;\n\tstruct nlmsghdr *nlh;\n\tint err;\n\n\tnlh = nlmsg_put(skb, c->portid, 0, XFRM_MSG_EXPIRE, sizeof(*ue), 0);\n\tif (nlh == NULL)\n\t\treturn -EMSGSIZE;\n\n\tue = nlmsg_data(nlh);\n\tcopy_to_user_state(x, &ue->state);\n\tue->hard = (c->data.hard != 0) ? 1 : 0;\n\n\terr = xfrm_mark_put(skb, &x->mark);\n\tif (err)\n\t\treturn err;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "build_expire(struct sk_buff *skb, struct xfrm_state *x, const struct km_event *c){\n\tstruct xfrm_user_expire *ue;\n\tstruct nlmsghdr *nlh;\n\tint err;\n\n\tnlh = nlmsg_put(skb, c->portid, 0, XFRM_MSG_EXPIRE, sizeof(*ue), 0);\n\tif (nlh == NULL)\n\t\treturn -EMSGSIZE;\n\n\tue = nlmsg_data(nlh);\n\tcopy_to_user_state(x, &ue->state);\n\tue->hard = (c->data.hard != 0) ? 1 : 0;\n\n\terr = xfrm_mark_put(skb, &x->mark);\n\tif (err)\n\t\treturn err;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_129": {
        "vulnerable_code": {
            "Code": "build_mapping(struct sk_buff *skb, struct xfrm_state *x,\n\t\t\t xfrm_address_t *new_saddr, __be16 new_sport){\n\tstruct xfrm_user_mapping *um;\n\tstruct nlmsghdr *nlh;\n\n\tnlh = nlmsg_put(skb, 0, 0, XFRM_MSG_MAPPING, sizeof(*um), 0);\n\tif (nlh == NULL)\n\t\treturn -EMSGSIZE;\n\n\tum = nlmsg_data(nlh);\n\n\tmemcpy(&um->id.daddr, &x->id.daddr, sizeof(um->id.daddr));\n\tum->id.spi = x->id.spi;\n\tum->id.family = x->props.family;\n\tum->id.proto = x->id.proto;\n\tmemcpy(&um->new_saddr, new_saddr, sizeof(um->new_saddr));\n\tmemcpy(&um->old_saddr, &x->props.saddr, sizeof(um->old_saddr));\n\tum->new_sport = new_sport;\n\tum->old_sport = x->encap->encap_sport;\n\tum->reqid = x->props.reqid;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "build_mapping(struct sk_buff *skb, struct xfrm_state *x,\n\t\t\t xfrm_address_t *new_saddr, __be16 new_sport){\n\tstruct xfrm_user_mapping *um;\n\tstruct nlmsghdr *nlh;\n\n\tnlh = nlmsg_put(skb, 0, 0, XFRM_MSG_MAPPING, sizeof(*um), 0);\n\tif (nlh == NULL)\n\t\treturn -EMSGSIZE;\n\n\tum = nlmsg_data(nlh);\n\n\tmemcpy(&um->id.daddr, &x->id.daddr, sizeof(um->id.daddr));\n\tum->id.spi = x->id.spi;\n\tum->id.family = x->props.family;\n\tum->id.proto = x->id.proto;\n\tmemcpy(&um->new_saddr, new_saddr, sizeof(um->new_saddr));\n\tmemcpy(&um->old_saddr, &x->props.saddr, sizeof(um->old_saddr));\n\tum->new_sport = new_sport;\n\tum->old_sport = x->encap->encap_sport;\n\tum->reqid = x->props.reqid;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_130": {
        "vulnerable_code": {
            "Code": "build_polexpire(struct sk_buff *skb, struct xfrm_policy *xp,\n\t\t\t   int dir, const struct km_event *c){\n\tstruct xfrm_user_polexpire *upe;\n\tint hard = c->data.hard;\n\tstruct nlmsghdr *nlh;\n\tint err;\n\n\tnlh = nlmsg_put(skb, c->portid, 0, XFRM_MSG_POLEXPIRE, sizeof(*upe), 0);\n\tif (nlh == NULL)\n\t\treturn -EMSGSIZE;\n\n\tupe = nlmsg_data(nlh);\n\tcopy_to_user_policy(xp, &upe->pol, dir);\n\terr = copy_to_user_tmpl(xp, skb);\n\tif (!err)\n\t\terr = copy_to_user_sec_ctx(xp, skb);\n\tif (!err)\n\t\terr = copy_to_user_policy_type(xp->type, skb);\n\tif (!err)\n\t\terr = xfrm_mark_put(skb, &xp->mark);\n\tif (err) {\n\t\tnlmsg_cancel(skb, nlh);\n\t\treturn err;\n\t}\n\tupe->hard = !!hard;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "build_polexpire(struct sk_buff *skb, struct xfrm_policy *xp,\n\t\t\t   int dir, const struct km_event *c){\n\tstruct xfrm_user_polexpire *upe;\n\tint hard = c->data.hard;\n\tstruct nlmsghdr *nlh;\n\tint err;\n\n\tnlh = nlmsg_put(skb, c->portid, 0, XFRM_MSG_POLEXPIRE, sizeof(*upe), 0);\n\tif (nlh == NULL)\n\t\treturn -EMSGSIZE;\n\n\tupe = nlmsg_data(nlh);\n\tcopy_to_user_policy(xp, &upe->pol, dir);\n\terr = copy_to_user_tmpl(xp, skb);\n\tif (!err)\n\t\terr = copy_to_user_sec_ctx(xp, skb);\n\tif (!err)\n\t\terr = copy_to_user_policy_type(xp->type, skb);\n\tif (!err)\n\t\terr = xfrm_mark_put(skb, &xp->mark);\n\tif (err) {\n\t\tnlmsg_cancel(skb, nlh);\n\t\treturn err;\n\t}\n\tupe->hard = !!hard;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_131": {
        "vulnerable_code": {
            "Code": "build_spdinfo(struct sk_buff *skb, struct net *net,\n\t\t\t u32 portid, u32 seq, u32 flags){\n\tstruct xfrmk_spdinfo si;\n\tstruct xfrmu_spdinfo spc;\n\tstruct xfrmu_spdhinfo sph;\n\tstruct xfrmu_spdhthresh spt4, spt6;\n\tstruct nlmsghdr *nlh;\n\tint err;\n\tu32 *f;\n\tunsigned lseq;\n\n\tnlh = nlmsg_put(skb, portid, seq, XFRM_MSG_NEWSPDINFO, sizeof(u32), 0);\n\tif (nlh == NULL) /* shouldn't really happen ... */\n\t\treturn -EMSGSIZE;\n\n\tf = nlmsg_data(nlh);\n\t*f = flags;\n\txfrm_spd_getinfo(net, &si);\n\tspc.incnt = si.incnt;\n\tspc.outcnt = si.outcnt;\n\tspc.fwdcnt = si.fwdcnt;\n\tspc.inscnt = si.inscnt;\n\tspc.outscnt = si.outscnt;\n\tspc.fwdscnt = si.fwdscnt;\n\tsph.spdhcnt = si.spdhcnt;\n\tsph.spdhmcnt = si.spdhmcnt;\n\n\tdo {\n\t\tlseq = read_seqbegin(&net->xfrm.policy_hthresh.lock);\n\n\t\tspt4.lbits = net->xfrm.policy_hthresh.lbits4;\n\t\tspt4.rbits = net->xfrm.policy_hthresh.rbits4;\n\t\tspt6.lbits = net->xfrm.policy_hthresh.lbits6;\n\t\tspt6.rbits = net->xfrm.policy_hthresh.rbits6;\n\t} while (read_seqretry(&net->xfrm.policy_hthresh.lock, lseq));\n\n\terr = nla_put(skb, XFRMA_SPD_INFO, sizeof(spc), &spc);\n\tif (!err)\n\t\terr = nla_put(skb, XFRMA_SPD_HINFO, sizeof(sph), &sph);\n\tif (!err)\n\t\terr = nla_put(skb, XFRMA_SPD_IPV4_HTHRESH, sizeof(spt4), &spt4);\n\tif (!err)\n\t\terr = nla_put(skb, XFRMA_SPD_IPV6_HTHRESH, sizeof(spt6), &spt6);\n\tif (err) {\n\t\tnlmsg_cancel(skb, nlh);\n\t\treturn err;\n\t}\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "build_spdinfo(struct sk_buff *skb, struct net *net,\n\t\t\t u32 portid, u32 seq, u32 flags){\n\tstruct xfrmk_spdinfo si;\n\tstruct xfrmu_spdinfo spc;\n\tstruct xfrmu_spdhinfo sph;\n\tstruct xfrmu_spdhthresh spt4, spt6;\n\tstruct nlmsghdr *nlh;\n\tint err;\n\tu32 *f;\n\tunsigned lseq;\n\n\tnlh = nlmsg_put(skb, portid, seq, XFRM_MSG_NEWSPDINFO, sizeof(u32), 0);\n\tif (nlh == NULL) /* shouldn't really happen ... */\n\t\treturn -EMSGSIZE;\n\n\tf = nlmsg_data(nlh);\n\t*f = flags;\n\txfrm_spd_getinfo(net, &si);\n\tspc.incnt = si.incnt;\n\tspc.outcnt = si.outcnt;\n\tspc.fwdcnt = si.fwdcnt;\n\tspc.inscnt = si.inscnt;\n\tspc.outscnt = si.outscnt;\n\tspc.fwdscnt = si.fwdscnt;\n\tsph.spdhcnt = si.spdhcnt;\n\tsph.spdhmcnt = si.spdhmcnt;\n\n\tdo {\n\t\tlseq = read_seqbegin(&net->xfrm.policy_hthresh.lock);\n\n\t\tspt4.lbits = net->xfrm.policy_hthresh.lbits4;\n\t\tspt4.rbits = net->xfrm.policy_hthresh.rbits4;\n\t\tspt6.lbits = net->xfrm.policy_hthresh.lbits6;\n\t\tspt6.rbits = net->xfrm.policy_hthresh.rbits6;\n\t} while (read_seqretry(&net->xfrm.policy_hthresh.lock, lseq));\n\n\terr = nla_put(skb, XFRMA_SPD_INFO, sizeof(spc), &spc);\n\tif (!err)\n\t\terr = nla_put(skb, XFRMA_SPD_HINFO, sizeof(sph), &sph);\n\tif (!err)\n\t\terr = nla_put(skb, XFRMA_SPD_IPV4_HTHRESH, sizeof(spt4), &spt4);\n\tif (!err)\n\t\terr = nla_put(skb, XFRMA_SPD_IPV6_HTHRESH, sizeof(spt6), &spt6);\n\tif (err) {\n\t\tnlmsg_cancel(skb, nlh);\n\t\treturn err;\n\t}\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_132": {
        "vulnerable_code": {
            "Code": "copy_to_user_auth(struct xfrm_algo_auth *auth, struct sk_buff *skb){\n\tstruct xfrm_algo *algo;\n\tstruct nlattr *nla;\n\n\tnla = nla_reserve(skb, XFRMA_ALG_AUTH,\n\t\t\t  sizeof(*algo) + (auth->alg_key_len + 7) / 8);\n\tif (!nla)\n\t\treturn -EMSGSIZE;\n\n\talgo = nla_data(nla);\n\tstrncpy(algo->alg_name, auth->alg_name, sizeof(algo->alg_name));\n\tmemcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8);\n\talgo->alg_key_len = auth->alg_key_len;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "copy_to_user_auth(struct xfrm_algo_auth *auth, struct sk_buff *skb){\n\tstruct xfrm_algo *algo;\n\tstruct nlattr *nla;\n\n\tnla = nla_reserve(skb, XFRMA_ALG_AUTH,\n\t\t\t  sizeof(*algo) + (auth->alg_key_len + 7) / 8);\n\tif (!nla)\n\t\treturn -EMSGSIZE;\n\n\talgo = nla_data(nla);\n\tstrncpy(algo->alg_name, auth->alg_name, sizeof(algo->alg_name));\n\tmemcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8);\n\talgo->alg_key_len = auth->alg_key_len;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_133": {
        "vulnerable_code": {
            "Code": "userpolicy_type_attrsize(void){\n#ifdef CONFIG_XFRM_SUB_POLICY\n\treturn nla_total_size(sizeof(struct xfrm_userpolicy_type));\n#else\n\treturn 0;\n#endif\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "userpolicy_type_attrsize(void){\n#ifdef CONFIG_XFRM_SUB_POLICY\n\treturn nla_total_size(sizeof(struct xfrm_userpolicy_type));\n#else\n\treturn 0;\n#endif\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_134": {
        "vulnerable_code": {
            "Code": "verify_one_addr(struct nlattr **attrs, enum xfrm_attr_type_t type,\n\t\t\t   xfrm_address_t **addrp){\n\tstruct nlattr *rt = attrs[type];\n\n\tif (rt && addrp)\n\t\t*addrp = nla_data(rt);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "verify_one_addr(struct nlattr **attrs, enum xfrm_attr_type_t type,\n\t\t\t   xfrm_address_t **addrp){\n\tstruct nlattr *rt = attrs[type];\n\n\tif (rt && addrp)\n\t\t*addrp = nla_data(rt);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_135": {
        "vulnerable_code": {
            "Code": "xfrm_acquire_msgsize(struct xfrm_state *x,\n\t\t\t\t\t  struct xfrm_policy *xp){\n\treturn NLMSG_ALIGN(sizeof(struct xfrm_user_acquire))\n\t       + nla_total_size(sizeof(struct xfrm_user_tmpl) * xp->xfrm_nr)\n\t       + nla_total_size(sizeof(struct xfrm_mark))\n\t       + nla_total_size(xfrm_user_sec_ctx_size(x->security))\n\t       + userpolicy_type_attrsize();\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "xfrm_acquire_msgsize(struct xfrm_state *x,\n\t\t\t\t\t  struct xfrm_policy *xp){\n\treturn NLMSG_ALIGN(sizeof(struct xfrm_user_acquire))\n\t       + nla_total_size(sizeof(struct xfrm_user_tmpl) * xp->xfrm_nr)\n\t       + nla_total_size(sizeof(struct xfrm_mark))\n\t       + nla_total_size(xfrm_user_sec_ctx_size(x->security))\n\t       + userpolicy_type_attrsize();\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_136": {
        "vulnerable_code": {
            "Code": "xfrm_add_policy(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\tstruct nlattr **attrs){\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_userpolicy_info *p = nlmsg_data(nlh);\n\tstruct xfrm_policy *xp;\n\tstruct km_event c;\n\tint err;\n\tint excl;\n\n\terr = verify_newpolicy_info(p);\n\tif (err)\n\t\treturn err;\n\terr = verify_sec_ctx_len(attrs);\n\tif (err)\n\t\treturn err;\n\n\txp = xfrm_policy_construct(net, p, attrs, &err);\n\tif (!xp)\n\t\treturn err;\n\n\t/* shouldn't excl be based on nlh flags??\n\t * Aha! this is anti-netlink really i.e  more pfkey derived\n\t * in netlink excl is a flag and you wouldnt need\n\t * a type XFRM_MSG_UPDPOLICY - JHS */\n\texcl = nlh->nlmsg_type == XFRM_MSG_NEWPOLICY;\n\terr = xfrm_policy_insert(p->dir, xp, excl);\n\txfrm_audit_policy_add(xp, err ? 0 : 1, true);\n\n\tif (err) {\n\t\tsecurity_xfrm_policy_free(xp->security);\n\t\tkfree(xp);\n\t\treturn err;\n\t}\n\n\tc.event = nlh->nlmsg_type;\n\tc.seq = nlh->nlmsg_seq;\n\tc.portid = nlh->nlmsg_pid;\n\tkm_policy_notify(xp, p->dir, &c);\n\n\txfrm_pol_put(xp);\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "xfrm_add_policy(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\tstruct nlattr **attrs){\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_userpolicy_info *p = nlmsg_data(nlh);\n\tstruct xfrm_policy *xp;\n\tstruct km_event c;\n\tint err;\n\tint excl;\n\n\terr = verify_newpolicy_info(p);\n\tif (err)\n\t\treturn err;\n\terr = verify_sec_ctx_len(attrs);\n\tif (err)\n\t\treturn err;\n\n\txp = xfrm_policy_construct(net, p, attrs, &err);\n\tif (!xp)\n\t\treturn err;\n\n\t/* shouldn't excl be based on nlh flags??\n\t * Aha! this is anti-netlink really i.e  more pfkey derived\n\t * in netlink excl is a flag and you wouldnt need\n\t * a type XFRM_MSG_UPDPOLICY - JHS */\n\texcl = nlh->nlmsg_type == XFRM_MSG_NEWPOLICY;\n\terr = xfrm_policy_insert(p->dir, xp, excl);\n\txfrm_audit_policy_add(xp, err ? 0 : 1, true);\n\n\tif (err) {\n\t\tsecurity_xfrm_policy_free(xp->security);\n\t\tkfree(xp);\n\t\treturn err;\n\t}\n\n\tc.event = nlh->nlmsg_type;\n\tc.seq = nlh->nlmsg_seq;\n\tc.portid = nlh->nlmsg_pid;\n\tkm_policy_notify(xp, p->dir, &c);\n\n\txfrm_pol_put(xp);\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_137": {
        "vulnerable_code": {
            "Code": "xfrm_dump_policy_done(struct netlink_callback *cb){\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n\tstruct net *net = sock_net(cb->skb->sk);\n\n\txfrm_policy_walk_done(walk, net);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "xfrm_dump_policy_done(struct netlink_callback *cb){\n\tstruct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];\n\tstruct net *net = sock_net(cb->skb->sk);\n\n\txfrm_policy_walk_done(walk, net);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_138": {
        "vulnerable_code": {
            "Code": "xfrm_exp_state_notify(struct xfrm_state *x, const struct km_event *c){\n\tstruct net *net = xs_net(x);\n\tstruct sk_buff *skb;\n\n\tskb = nlmsg_new(xfrm_expire_msgsize(), GFP_ATOMIC);\n\tif (skb == NULL)\n\t\treturn -ENOMEM;\n\n\tif (build_expire(skb, x, c) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\treturn xfrm_nlmsg_multicast(net, skb, 0, XFRMNLGRP_EXPIRE);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "xfrm_exp_state_notify(struct xfrm_state *x, const struct km_event *c){\n\tstruct net *net = xs_net(x);\n\tstruct sk_buff *skb;\n\n\tskb = nlmsg_new(xfrm_expire_msgsize(), GFP_ATOMIC);\n\tif (skb == NULL)\n\t\treturn -ENOMEM;\n\n\tif (build_expire(skb, x, c) < 0) {\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\treturn xfrm_nlmsg_multicast(net, skb, 0, XFRMNLGRP_EXPIRE);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_139": {
        "vulnerable_code": {
            "Code": "xfrm_get_ae(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\tstruct nlattr **attrs){\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_state *x;\n\tstruct sk_buff *r_skb;\n\tint err;\n\tstruct km_event c;\n\tu32 mark;\n\tstruct xfrm_mark m;\n\tstruct xfrm_aevent_id *p = nlmsg_data(nlh);\n\tstruct xfrm_usersa_id *id = &p->sa_id;\n\n\tmark = xfrm_mark_get(attrs, &m);\n\n\tx = xfrm_state_lookup(net, mark, &id->daddr, id->spi, id->proto, id->family);\n\tif (x == NULL)\n\t\treturn -ESRCH;\n\n\tr_skb = nlmsg_new(xfrm_aevent_msgsize(x), GFP_ATOMIC);\n\tif (r_skb == NULL) {\n\t\txfrm_state_put(x);\n\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * XXX: is this lock really needed - none of the other\n\t * gets lock (the concern is things getting updated\n\t * while we are still reading) - jhs\n\t*/\n\tspin_lock_bh(&x->lock);\n\tc.data.aevent = p->flags;\n\tc.seq = nlh->nlmsg_seq;\n\tc.portid = nlh->nlmsg_pid;\n\n\tif (build_aevent(r_skb, x, &c) < 0)\n\t\tBUG();\n\terr = nlmsg_unicast(net->xfrm.nlsk, r_skb, NETLINK_CB(skb).portid);\n\tspin_unlock_bh(&x->lock);\n\txfrm_state_put(x);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "xfrm_get_ae(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\tstruct nlattr **attrs){\n\tstruct net *net = sock_net(skb->sk);\n\tstruct xfrm_state *x;\n\tstruct sk_buff *r_skb;\n\tint err;\n\tstruct km_event c;\n\tu32 mark;\n\tstruct xfrm_mark m;\n\tstruct xfrm_aevent_id *p = nlmsg_data(nlh);\n\tstruct xfrm_usersa_id *id = &p->sa_id;\n\n\tmark = xfrm_mark_get(attrs, &m);\n\n\tx = xfrm_state_lookup(net, mark, &id->daddr, id->spi, id->proto, id->family);\n\tif (x == NULL)\n\t\treturn -ESRCH;\n\n\tr_skb = nlmsg_new(xfrm_aevent_msgsize(x), GFP_ATOMIC);\n\tif (r_skb == NULL) {\n\t\txfrm_state_put(x);\n\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * XXX: is this lock really needed - none of the other\n\t * gets lock (the concern is things getting updated\n\t * while we are still reading) - jhs\n\t*/\n\tspin_lock_bh(&x->lock);\n\tc.data.aevent = p->flags;\n\tc.seq = nlh->nlmsg_seq;\n\tc.portid = nlh->nlmsg_pid;\n\n\tif (build_aevent(r_skb, x, &c) < 0)\n\t\tBUG();\n\terr = nlmsg_unicast(net->xfrm.nlsk, r_skb, NETLINK_CB(skb).portid);\n\tspin_unlock_bh(&x->lock);\n\txfrm_state_put(x);\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_140": {
        "vulnerable_code": {
            "Code": "xfrm_replay_verify_len(struct xfrm_replay_state_esn *replay_esn,\n\t\t\t\t\t struct nlattr *rp){\n\tstruct xfrm_replay_state_esn *up;\n\tint ulen;\n\n\tif (!replay_esn || !rp)\n\t\treturn 0;\n\n\tup = nla_data(rp);\n\tulen = xfrm_replay_state_esn_len(up);\n\n\tif (nla_len(rp) < ulen || xfrm_replay_state_esn_len(replay_esn) != ulen)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "xfrm_replay_verify_len(struct xfrm_replay_state_esn *replay_esn,\n\t\t\t\t\t struct nlattr *rp){\n\tstruct xfrm_replay_state_esn *up;\n\tint ulen;\n\n\tif (!replay_esn || !rp)\n\t\treturn 0;\n\n\tup = nla_data(rp);\n\tulen = xfrm_replay_state_esn_len(up);\n\n\tif (nla_len(rp) < ulen || xfrm_replay_state_esn_len(replay_esn) != ulen)\n\t\treturn -EINVAL;\n\n\tif (up->replay_window > up->bmp_len * sizeof(__u32) * 8)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_141": {
        "vulnerable_code": {
            "Code": "xfrm_user_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh){\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *attrs[XFRMA_MAX+1];\n\tconst struct xfrm_link *link;\n\tint type, err;\n\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\treturn -EOPNOTSUPP;\n#endif\n\n\ttype = nlh->nlmsg_type;\n\tif (type > XFRM_MSG_MAX)\n\t\treturn -EINVAL;\n\n\ttype -= XFRM_MSG_BASE;\n\tlink = &xfrm_dispatch[type];\n\n\t/* All operations require privileges, even GET */\n\tif (!netlink_net_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((type == (XFRM_MSG_GETSA - XFRM_MSG_BASE) ||\n\t     type == (XFRM_MSG_GETPOLICY - XFRM_MSG_BASE)) &&\n\t    (nlh->nlmsg_flags & NLM_F_DUMP)) {\n\t\tif (link->dump == NULL)\n\t\t\treturn -EINVAL;\n\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump = link->dump,\n\t\t\t\t.done = link->done,\n\t\t\t};\n\t\t\treturn netlink_dump_start(net->xfrm.nlsk, skb, nlh, &c);\n\t\t}\n\t}\n\n\terr = nlmsg_parse(nlh, xfrm_msg_min[type], attrs,\n\t\t\t  link->nla_max ? : XFRMA_MAX,\n\t\t\t  link->nla_pol ? : xfrma_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (link->doit == NULL)\n\t\treturn -EINVAL;\n\n\treturn link->doit(skb, nlh, attrs);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "xfrm_user_rcv_msg(struct sk_buff *skb, struct nlmsghdr *nlh){\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *attrs[XFRMA_MAX+1];\n\tconst struct xfrm_link *link;\n\tint type, err;\n\n#ifdef CONFIG_COMPAT\n\tif (in_compat_syscall())\n\t\treturn -EOPNOTSUPP;\n#endif\n\n\ttype = nlh->nlmsg_type;\n\tif (type > XFRM_MSG_MAX)\n\t\treturn -EINVAL;\n\n\ttype -= XFRM_MSG_BASE;\n\tlink = &xfrm_dispatch[type];\n\n\t/* All operations require privileges, even GET */\n\tif (!netlink_net_capable(skb, CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tif ((type == (XFRM_MSG_GETSA - XFRM_MSG_BASE) ||\n\t     type == (XFRM_MSG_GETPOLICY - XFRM_MSG_BASE)) &&\n\t    (nlh->nlmsg_flags & NLM_F_DUMP)) {\n\t\tif (link->dump == NULL)\n\t\t\treturn -EINVAL;\n\n\t\t{\n\t\t\tstruct netlink_dump_control c = {\n\t\t\t\t.dump = link->dump,\n\t\t\t\t.done = link->done,\n\t\t\t};\n\t\t\treturn netlink_dump_start(net->xfrm.nlsk, skb, nlh, &c);\n\t\t}\n\t}\n\n\terr = nlmsg_parse(nlh, xfrm_msg_min[type], attrs,\n\t\t\t  link->nla_max ? : XFRMA_MAX,\n\t\t\t  link->nla_pol ? : xfrma_policy);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (link->doit == NULL)\n\t\treturn -EINVAL;\n\n\treturn link->doit(skb, nlh, attrs);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_142": {
        "vulnerable_code": {
            "Code": "xfrm_user_sec_ctx_size(struct xfrm_sec_ctx *xfrm_ctx){\n\tint len = 0;\n\n\tif (xfrm_ctx) {\n\t\tlen += sizeof(struct xfrm_user_sec_ctx);\n\t\tlen += xfrm_ctx->ctx_len;\n\t}\n\treturn len;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "xfrm_user_sec_ctx_size(struct xfrm_sec_ctx *xfrm_ctx){\n\tint len = 0;\n\n\tif (xfrm_ctx) {\n\t\tlen += sizeof(struct xfrm_user_sec_ctx);\n\t\tlen += xfrm_ctx->ctx_len;\n\t}\n\treturn len;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_143": {
        "vulnerable_code": {
            "Code": "snd_timer_global_register(struct snd_timer *timer){\n\tstruct snd_device dev;\n\n\tmemset(&dev, 0, sizeof(dev));\n\tdev.device_data = timer;\n\treturn snd_timer_dev_register(&dev);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_global_register(struct snd_timer *timer){\n\tstruct snd_device dev;\n\n\tmemset(&dev, 0, sizeof(dev));\n\tdev.device_data = timer;\n\treturn snd_timer_dev_register(&dev);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_144": {
        "vulnerable_code": {
            "Code": "snd_timer_instance_new(char *owner,\n\t\t\t\t\t\t\t struct snd_timer *timer){\n\tstruct snd_timer_instance *timeri;\n\ttimeri = kzalloc(sizeof(*timeri), GFP_KERNEL);\n\tif (timeri == NULL)\n\t\treturn NULL;\n\ttimeri->owner = kstrdup(owner, GFP_KERNEL);\n\tif (! timeri->owner) {\n\t\tkfree(timeri);\n\t\treturn NULL;\n\t}\n\tINIT_LIST_HEAD(&timeri->open_list);\n\tINIT_LIST_HEAD(&timeri->active_list);\n\tINIT_LIST_HEAD(&timeri->ack_list);\n\tINIT_LIST_HEAD(&timeri->slave_list_head);\n\tINIT_LIST_HEAD(&timeri->slave_active_head);\n\n\ttimeri->timer = timer;\n\tif (timer && !try_module_get(timer->module)) {\n\t\tkfree(timeri->owner);\n\t\tkfree(timeri);\n\t\treturn NULL;\n\t}\n\n\treturn timeri;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_instance_new(char *owner,\n\t\t\t\t\t\t\t struct snd_timer *timer){\n\tstruct snd_timer_instance *timeri;\n\ttimeri = kzalloc(sizeof(*timeri), GFP_KERNEL);\n\tif (timeri == NULL)\n\t\treturn NULL;\n\ttimeri->owner = kstrdup(owner, GFP_KERNEL);\n\tif (! timeri->owner) {\n\t\tkfree(timeri);\n\t\treturn NULL;\n\t}\n\tINIT_LIST_HEAD(&timeri->open_list);\n\tINIT_LIST_HEAD(&timeri->active_list);\n\tINIT_LIST_HEAD(&timeri->ack_list);\n\tINIT_LIST_HEAD(&timeri->slave_list_head);\n\tINIT_LIST_HEAD(&timeri->slave_active_head);\n\n\ttimeri->timer = timer;\n\tif (timer && !try_module_get(timer->module)) {\n\t\tkfree(timeri->owner);\n\t\tkfree(timeri);\n\t\treturn NULL;\n\t}\n\n\treturn timeri;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_145": {
        "vulnerable_code": {
            "Code": "snd_timer_interrupt(struct snd_timer * timer, unsigned long ticks_left){\n\tstruct snd_timer_instance *ti, *ts, *tmp;\n\tunsigned long resolution, ticks;\n\tstruct list_head *p, *ack_list_head;\n\tunsigned long flags;\n\tint use_tasklet = 0;\n\n\tif (timer == NULL)\n\t\treturn;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\n\t/* remember the current resolution */\n\tif (timer->hw.c_resolution)\n\t\tresolution = timer->hw.c_resolution(timer);\n\telse\n\t\tresolution = timer->hw.resolution;\n\n\t/* loop for all active instances\n\t * Here we cannot use list_for_each_entry because the active_list of a\n\t * processed instance is relinked to done_list_head before the callback\n\t * is called.\n\t */\n\tlist_for_each_entry_safe(ti, tmp, &timer->active_list_head,\n\t\t\t\t active_list) {\n\t\tif (!(ti->flags & SNDRV_TIMER_IFLG_RUNNING))\n\t\t\tcontinue;\n\t\tti->pticks += ticks_left;\n\t\tti->resolution = resolution;\n\t\tif (ti->cticks < ticks_left)\n\t\t\tti->cticks = 0;\n\t\telse\n\t\t\tti->cticks -= ticks_left;\n\t\tif (ti->cticks) /* not expired */\n\t\t\tcontinue;\n\t\tif (ti->flags & SNDRV_TIMER_IFLG_AUTO) {\n\t\t\tti->cticks = ti->ticks;\n\t\t} else {\n\t\t\tti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\t--timer->running;\n\t\t\tlist_del_init(&ti->active_list);\n\t\t}\n\t\tif ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||\n\t\t    (ti->flags & SNDRV_TIMER_IFLG_FAST))\n\t\t\tack_list_head = &timer->ack_list_head;\n\t\telse\n\t\t\tack_list_head = &timer->sack_list_head;\n\t\tif (list_empty(&ti->ack_list))\n\t\t\tlist_add_tail(&ti->ack_list, ack_list_head);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list) {\n\t\t\tts->pticks = ti->pticks;\n\t\t\tts->resolution = resolution;\n\t\t\tif (list_empty(&ts->ack_list))\n\t\t\t\tlist_add_tail(&ts->ack_list, ack_list_head);\n\t\t}\n\t}\n\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED)\n\t\tsnd_timer_reschedule(timer, timer->sticks);\n\tif (timer->running) {\n\t\tif (timer->hw.flags & SNDRV_TIMER_HW_STOP) {\n\t\t\ttimer->hw.stop(timer);\n\t\t\ttimer->flags |= SNDRV_TIMER_FLG_CHANGE;\n\t\t}\n\t\tif (!(timer->hw.flags & SNDRV_TIMER_HW_AUTO) ||\n\t\t    (timer->flags & SNDRV_TIMER_FLG_CHANGE)) {\n\t\t\t/* restart timer */\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\ttimer->hw.start(timer);\n\t\t}\n\t} else {\n\t\ttimer->hw.stop(timer);\n\t}\n\n\t/* now process all fast callbacks */\n\twhile (!list_empty(&timer->ack_list_head)) {\n\t\tp = timer->ack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\n\t/* do we have any slow callbacks? */\n\tuse_tasklet = !list_empty(&timer->sack_list_head);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n\n\tif (use_tasklet)\n\t\ttasklet_schedule(&timer->task_queue);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_interrupt(struct snd_timer * timer, unsigned long ticks_left){\n\tstruct snd_timer_instance *ti, *ts, *tmp;\n\tunsigned long resolution, ticks;\n\tstruct list_head *p, *ack_list_head;\n\tunsigned long flags;\n\tint use_tasklet = 0;\n\n\tif (timer == NULL)\n\t\treturn;\n\n\tif (timer->card && timer->card->shutdown)\n\t\treturn;\n\n\tspin_lock_irqsave(&timer->lock, flags);\n\n\t/* remember the current resolution */\n\tif (timer->hw.c_resolution)\n\t\tresolution = timer->hw.c_resolution(timer);\n\telse\n\t\tresolution = timer->hw.resolution;\n\n\t/* loop for all active instances\n\t * Here we cannot use list_for_each_entry because the active_list of a\n\t * processed instance is relinked to done_list_head before the callback\n\t * is called.\n\t */\n\tlist_for_each_entry_safe(ti, tmp, &timer->active_list_head,\n\t\t\t\t active_list) {\n\t\tif (!(ti->flags & SNDRV_TIMER_IFLG_RUNNING))\n\t\t\tcontinue;\n\t\tti->pticks += ticks_left;\n\t\tti->resolution = resolution;\n\t\tif (ti->cticks < ticks_left)\n\t\t\tti->cticks = 0;\n\t\telse\n\t\t\tti->cticks -= ticks_left;\n\t\tif (ti->cticks) /* not expired */\n\t\t\tcontinue;\n\t\tif (ti->flags & SNDRV_TIMER_IFLG_AUTO) {\n\t\t\tti->cticks = ti->ticks;\n\t\t} else {\n\t\t\tti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;\n\t\t\t--timer->running;\n\t\t\tlist_del_init(&ti->active_list);\n\t\t}\n\t\tif ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||\n\t\t    (ti->flags & SNDRV_TIMER_IFLG_FAST))\n\t\t\tack_list_head = &timer->ack_list_head;\n\t\telse\n\t\t\tack_list_head = &timer->sack_list_head;\n\t\tif (list_empty(&ti->ack_list))\n\t\t\tlist_add_tail(&ti->ack_list, ack_list_head);\n\t\tlist_for_each_entry(ts, &ti->slave_active_head, active_list) {\n\t\t\tts->pticks = ti->pticks;\n\t\t\tts->resolution = resolution;\n\t\t\tif (list_empty(&ts->ack_list))\n\t\t\t\tlist_add_tail(&ts->ack_list, ack_list_head);\n\t\t}\n\t}\n\tif (timer->flags & SNDRV_TIMER_FLG_RESCHED)\n\t\tsnd_timer_reschedule(timer, timer->sticks);\n\tif (timer->running) {\n\t\tif (timer->hw.flags & SNDRV_TIMER_HW_STOP) {\n\t\t\ttimer->hw.stop(timer);\n\t\t\ttimer->flags |= SNDRV_TIMER_FLG_CHANGE;\n\t\t}\n\t\tif (!(timer->hw.flags & SNDRV_TIMER_HW_AUTO) ||\n\t\t    (timer->flags & SNDRV_TIMER_FLG_CHANGE)) {\n\t\t\t/* restart timer */\n\t\t\ttimer->flags &= ~SNDRV_TIMER_FLG_CHANGE;\n\t\t\ttimer->hw.start(timer);\n\t\t}\n\t} else {\n\t\ttimer->hw.stop(timer);\n\t}\n\n\t/* now process all fast callbacks */\n\twhile (!list_empty(&timer->ack_list_head)) {\n\t\tp = timer->ack_list_head.next;\t\t/* get first item */\n\t\tti = list_entry(p, struct snd_timer_instance, ack_list);\n\n\t\t/* remove from ack_list and make empty */\n\t\tlist_del_init(p);\n\n\t\tticks = ti->pticks;\n\t\tti->pticks = 0;\n\n\t\tti->flags |= SNDRV_TIMER_IFLG_CALLBACK;\n\t\tspin_unlock(&timer->lock);\n\t\tif (ti->callback)\n\t\t\tti->callback(ti, resolution, ticks);\n\t\tspin_lock(&timer->lock);\n\t\tti->flags &= ~SNDRV_TIMER_IFLG_CALLBACK;\n\t}\n\n\t/* do we have any slow callbacks? */\n\tuse_tasklet = !list_empty(&timer->sack_list_head);\n\tspin_unlock_irqrestore(&timer->lock, flags);\n\n\tif (use_tasklet)\n\t\ttasklet_schedule(&timer->task_queue);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_146": {
        "vulnerable_code": {
            "Code": "snd_timer_resolution(struct snd_timer_instance *timeri){\n\tstruct snd_timer * timer;\n\n\tif (timeri == NULL)\n\t\treturn 0;\n\tif ((timer = timeri->timer) != NULL) {\n\t\tif (timer->hw.c_resolution)\n\t\t\treturn timer->hw.c_resolution(timer);\n\t\treturn timer->hw.resolution;\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_resolution(struct snd_timer_instance *timeri){\n\tstruct snd_timer * timer;\n\n\tif (timeri == NULL)\n\t\treturn 0;\n\tif ((timer = timeri->timer) != NULL) {\n\t\tif (timer->hw.c_resolution)\n\t\t\treturn timer->hw.c_resolution(timer);\n\t\treturn timer->hw.resolution;\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_147": {
        "vulnerable_code": {
            "Code": "snd_timer_s_close(struct snd_timer *timer){\n\tstruct snd_timer_system_private *priv;\n\n\tpriv = (struct snd_timer_system_private *)timer->private_data;\n\tdel_timer_sync(&priv->tlist);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_s_close(struct snd_timer *timer){\n\tstruct snd_timer_system_private *priv;\n\n\tpriv = (struct snd_timer_system_private *)timer->private_data;\n\tdel_timer_sync(&priv->tlist);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_148": {
        "vulnerable_code": {
            "Code": "snd_timer_start_slave(struct snd_timer_instance *timeri,\n\t\t\t\t bool start){\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\tif (timeri->flags & SNDRV_TIMER_IFLG_RUNNING) {\n\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\t\treturn -EBUSY;\n\t}\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master && timeri->timer) {\n\t\tspin_lock(&timeri->timer->lock);\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\t\tsnd_timer_notify1(timeri, start ? SNDRV_TIMER_EVENT_START :\n\t\t\t\t  SNDRV_TIMER_EVENT_CONTINUE);\n\t\tspin_unlock(&timeri->timer->lock);\n\t}\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_start_slave(struct snd_timer_instance *timeri,\n\t\t\t\t bool start){\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&slave_active_lock, flags);\n\tif (timeri->flags & SNDRV_TIMER_IFLG_RUNNING) {\n\t\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\t\treturn -EBUSY;\n\t}\n\ttimeri->flags |= SNDRV_TIMER_IFLG_RUNNING;\n\tif (timeri->master && timeri->timer) {\n\t\tspin_lock(&timeri->timer->lock);\n\t\tlist_add_tail(&timeri->active_list,\n\t\t\t      &timeri->master->slave_active_head);\n\t\tsnd_timer_notify1(timeri, start ? SNDRV_TIMER_EVENT_START :\n\t\t\t\t  SNDRV_TIMER_EVENT_CONTINUE);\n\t\tspin_unlock(&timeri->timer->lock);\n\t}\n\tspin_unlock_irqrestore(&slave_active_lock, flags);\n\treturn 1; /* delayed start */\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_149": {
        "vulnerable_code": {
            "Code": "snd_timer_user_copy_id(struct snd_timer_id *id, struct snd_timer *timer){\n\tid->dev_class = timer->tmr_class;\n\tid->dev_sclass = SNDRV_TIMER_SCLASS_NONE;\n\tid->card = timer->card ? timer->card->number : -1;\n\tid->device = timer->tmr_device;\n\tid->subdevice = timer->tmr_subdevice;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_copy_id(struct snd_timer_id *id, struct snd_timer *timer){\n\tid->dev_class = timer->tmr_class;\n\tid->dev_sclass = SNDRV_TIMER_SCLASS_NONE;\n\tid->card = timer->card ? timer->card->number : -1;\n\tid->device = timer->tmr_device;\n\tid->subdevice = timer->tmr_subdevice;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_150": {
        "vulnerable_code": {
            "Code": "snd_timer_user_interrupt(struct snd_timer_instance *timeri,\n\t\t\t\t     unsigned long resolution,\n\t\t\t\t     unsigned long ticks){\n\tstruct snd_timer_user *tu = timeri->callback_data;\n\tstruct snd_timer_read *r;\n\tint prev;\n\n\tspin_lock(&tu->qlock);\n\tif (tu->qused > 0) {\n\t\tprev = tu->qtail == 0 ? tu->queue_size - 1 : tu->qtail - 1;\n\t\tr = &tu->queue[prev];\n\t\tif (r->resolution == resolution) {\n\t\t\tr->ticks += ticks;\n\t\t\tgoto __wake;\n\t\t}\n\t}\n\tif (tu->qused >= tu->queue_size) {\n\t\ttu->overrun++;\n\t} else {\n\t\tr = &tu->queue[tu->qtail++];\n\t\ttu->qtail %= tu->queue_size;\n\t\tr->resolution = resolution;\n\t\tr->ticks = ticks;\n\t\ttu->qused++;\n\t}\n      __wake:\n\tspin_unlock(&tu->qlock);\n\tkill_fasync(&tu->fasync, SIGIO, POLL_IN);\n\twake_up(&tu->qchange_sleep);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_interrupt(struct snd_timer_instance *timeri,\n\t\t\t\t     unsigned long resolution,\n\t\t\t\t     unsigned long ticks){\n\tstruct snd_timer_user *tu = timeri->callback_data;\n\tstruct snd_timer_read *r;\n\tint prev;\n\n\tspin_lock(&tu->qlock);\n\tif (tu->qused > 0) {\n\t\tprev = tu->qtail == 0 ? tu->queue_size - 1 : tu->qtail - 1;\n\t\tr = &tu->queue[prev];\n\t\tif (r->resolution == resolution) {\n\t\t\tr->ticks += ticks;\n\t\t\tgoto __wake;\n\t\t}\n\t}\n\tif (tu->qused >= tu->queue_size) {\n\t\ttu->overrun++;\n\t} else {\n\t\tr = &tu->queue[tu->qtail++];\n\t\ttu->qtail %= tu->queue_size;\n\t\tr->resolution = resolution;\n\t\tr->ticks = ticks;\n\t\ttu->qused++;\n\t}\n      __wake:\n\tspin_unlock(&tu->qlock);\n\tkill_fasync(&tu->fasync, SIGIO, POLL_IN);\n\twake_up(&tu->qchange_sleep);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_151": {
        "vulnerable_code": {
            "Code": "snd_timer_user_poll(struct file *file, poll_table * wait){\n        unsigned int mask;\n        struct snd_timer_user *tu;\n\n        tu = file->private_data;\n\n        poll_wait(file, &tu->qchange_sleep, wait);\n\n\tmask = 0;\n\tif (tu->qused)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (tu->disconnected)\n\t\tmask |= POLLERR;\n\n\treturn mask;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_timer_user_poll(struct file *file, poll_table * wait){\n        unsigned int mask;\n        struct snd_timer_user *tu;\n\n        tu = file->private_data;\n\n        poll_wait(file, &tu->qchange_sleep, wait);\n\n\tmask = 0;\n\tif (tu->qused)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (tu->disconnected)\n\t\tmask |= POLLERR;\n\n\treturn mask;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_152": {
        "vulnerable_code": {
            "Code": "__grow_ple_window(int val){\n\tif (ple_window_grow < 1)\n\t\treturn ple_window;\n\n\tval = min(val, ple_window_actual_max);\n\n\tif (ple_window_grow < ple_window)\n\t\tval *= ple_window_grow;\n\telse\n\t\tval += ple_window_grow;\n\n\treturn val;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__grow_ple_window(int val){\n\tif (ple_window_grow < 1)\n\t\treturn ple_window;\n\n\tval = min(val, ple_window_actual_max);\n\n\tif (ple_window_grow < ple_window)\n\t\tval *= ple_window_grow;\n\telse\n\t\tval += ple_window_grow;\n\n\treturn val;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_153": {
        "vulnerable_code": {
            "Code": "__shrink_ple_window(int val, int modifier, int minimum){\n\tif (modifier < 1)\n\t\treturn ple_window;\n\n\tif (modifier < ple_window)\n\t\tval /= modifier;\n\telse\n\t\tval -= modifier;\n\n\treturn max(val, minimum);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__shrink_ple_window(int val, int modifier, int minimum){\n\tif (modifier < 1)\n\t\treturn ple_window;\n\n\tif (modifier < ple_window)\n\t\tval /= modifier;\n\telse\n\t\tval -= modifier;\n\n\treturn max(val, minimum);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_154": {
        "vulnerable_code": {
            "Code": "alloc_identity_pagetable(struct kvm *kvm){\n\t/* Called with kvm->slots_lock held. */\n\n\tint r = 0;\n\n\tBUG_ON(kvm->arch.ept_identity_pagetable_done);\n\n\tr = __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,\n\t\t\t\t    kvm->arch.ept_identity_map_addr, PAGE_SIZE);\n\n\treturn r;\n}",
            "Size": 2,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "alloc_identity_pagetable(struct kvm *kvm){\n\t/* Called with kvm->slots_lock held. */\n\n\tint r = 0;\n\n\tBUG_ON(kvm->arch.ept_identity_pagetable_done);\n\n\tr = __x86_set_memory_region(kvm, IDENTITY_PAGETABLE_PRIVATE_MEMSLOT,\n\t\t\t\t    kvm->arch.ept_identity_map_addr, PAGE_SIZE);\n\n\treturn r;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_155": {
        "vulnerable_code": {
            "Code": "check_vmentry_postreqs(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t\t  u32 *exit_qual){\n\tbool ia32e;\n\n\t*exit_qual = ENTRY_FAIL_DEFAULT;\n\n\tif (!nested_guest_cr0_valid(vcpu, vmcs12->guest_cr0) ||\n\t    !nested_guest_cr4_valid(vcpu, vmcs12->guest_cr4))\n\t\treturn 1;\n\n\tif (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_SHADOW_VMCS) &&\n\t    vmcs12->vmcs_link_pointer != -1ull) {\n\t\t*exit_qual = ENTRY_FAIL_VMCS_LINK_PTR;\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If the load IA32_EFER VM-entry control is 1, the following checks\n\t * are performed on the field for the IA32_EFER MSR:\n\t * - Bits reserved in the IA32_EFER MSR must be 0.\n\t * - Bit 10 (corresponding to IA32_EFER.LMA) must equal the value of\n\t *   the IA-32e mode guest VM-exit control. It must also be identical\n\t *   to bit 8 (LME) if bit 31 in the CR0 field (corresponding to\n\t *   CR0.PG) is 1.\n\t */\n\tif (to_vmx(vcpu)->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER)) {\n\t\tia32e = (vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE) != 0;\n\t\tif (!kvm_valid_efer(vcpu, vmcs12->guest_ia32_efer) ||\n\t\t    ia32e != !!(vmcs12->guest_ia32_efer & EFER_LMA) ||\n\t\t    ((vmcs12->guest_cr0 & X86_CR0_PG) &&\n\t\t     ia32e != !!(vmcs12->guest_ia32_efer & EFER_LME)))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * If the load IA32_EFER VM-exit control is 1, bits reserved in the\n\t * IA32_EFER MSR must be 0 in the field for that register. In addition,\n\t * the values of the LMA and LME bits in the field must each be that of\n\t * the host address-space size VM-exit control.\n\t */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_EFER) {\n\t\tia32e = (vmcs12->vm_exit_controls &\n\t\t\t VM_EXIT_HOST_ADDR_SPACE_SIZE) != 0;\n\t\tif (!kvm_valid_efer(vcpu, vmcs12->host_ia32_efer) ||\n\t\t    ia32e != !!(vmcs12->host_ia32_efer & EFER_LMA) ||\n\t\t    ia32e != !!(vmcs12->host_ia32_efer & EFER_LME))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "check_vmentry_postreqs(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t\t  u32 *exit_qual){\n\tbool ia32e;\n\n\t*exit_qual = ENTRY_FAIL_DEFAULT;\n\n\tif (!nested_guest_cr0_valid(vcpu, vmcs12->guest_cr0) ||\n\t    !nested_guest_cr4_valid(vcpu, vmcs12->guest_cr4))\n\t\treturn 1;\n\n\tif (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_SHADOW_VMCS) &&\n\t    vmcs12->vmcs_link_pointer != -1ull) {\n\t\t*exit_qual = ENTRY_FAIL_VMCS_LINK_PTR;\n\t\treturn 1;\n\t}\n\n\t/*\n\t * If the load IA32_EFER VM-entry control is 1, the following checks\n\t * are performed on the field for the IA32_EFER MSR:\n\t * - Bits reserved in the IA32_EFER MSR must be 0.\n\t * - Bit 10 (corresponding to IA32_EFER.LMA) must equal the value of\n\t *   the IA-32e mode guest VM-exit control. It must also be identical\n\t *   to bit 8 (LME) if bit 31 in the CR0 field (corresponding to\n\t *   CR0.PG) is 1.\n\t */\n\tif (to_vmx(vcpu)->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER)) {\n\t\tia32e = (vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE) != 0;\n\t\tif (!kvm_valid_efer(vcpu, vmcs12->guest_ia32_efer) ||\n\t\t    ia32e != !!(vmcs12->guest_ia32_efer & EFER_LMA) ||\n\t\t    ((vmcs12->guest_cr0 & X86_CR0_PG) &&\n\t\t     ia32e != !!(vmcs12->guest_ia32_efer & EFER_LME)))\n\t\t\treturn 1;\n\t}\n\n\t/*\n\t * If the load IA32_EFER VM-exit control is 1, bits reserved in the\n\t * IA32_EFER MSR must be 0 in the field for that register. In addition,\n\t * the values of the LMA and LME bits in the field must each be that of\n\t * the host address-space size VM-exit control.\n\t */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_EFER) {\n\t\tia32e = (vmcs12->vm_exit_controls &\n\t\t\t VM_EXIT_HOST_ADDR_SPACE_SIZE) != 0;\n\t\tif (!kvm_valid_efer(vcpu, vmcs12->host_ia32_efer) ||\n\t\t    ia32e != !!(vmcs12->host_ia32_efer & EFER_LMA) ||\n\t\t    ia32e != !!(vmcs12->host_ia32_efer & EFER_LME))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_156": {
        "vulnerable_code": {
            "Code": "cpu_has_broken_vmx_preemption_timer(void){\n\tu32 eax = cpuid_eax(0x00000001), i;\n\n\t/* Clear the reserved bits */\n\teax &= ~(0x3U << 14 | 0xfU << 28);\n\tfor (i = 0; i < ARRAY_SIZE(vmx_preemption_cpu_tfms); i++)\n\t\tif (eax == vmx_preemption_cpu_tfms[i])\n\t\t\treturn true;\n\n\treturn false;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_broken_vmx_preemption_timer(void){\n\tu32 eax = cpuid_eax(0x00000001), i;\n\n\t/* Clear the reserved bits */\n\teax &= ~(0x3U << 14 | 0xfU << 28);\n\tfor (i = 0; i < ARRAY_SIZE(vmx_preemption_cpu_tfms); i++)\n\t\tif (eax == vmx_preemption_cpu_tfms[i])\n\t\t\treturn true;\n\n\treturn false;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_157": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_apic_register_virt(void){\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_apic_register_virt(void){\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_158": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_ept_4levels(void){\n\treturn vmx_capability.ept & VMX_EPT_PAGE_WALK_4_BIT;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 5,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_ept_4levels(void){\n\treturn vmx_capability.ept & VMX_EPT_PAGE_WALK_4_BIT;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_159": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_ept_ad_bits(void){\n\treturn vmx_capability.ept & VMX_EPT_AD_BIT;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_ept_ad_bits(void){\n\treturn vmx_capability.ept & VMX_EPT_AD_BIT;\n}",
            "Size": 4,
            "Code Complexity": 0,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_160": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_ept_execute_only(void){\n\treturn vmx_capability.ept & VMX_EPT_EXECUTE_ONLY_BIT;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_ept_execute_only(void){\n\treturn vmx_capability.ept & VMX_EPT_EXECUTE_ONLY_BIT;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_161": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_ept_mt_wb(void){\n\treturn vmx_capability.ept & VMX_EPTP_WB_BIT;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_ept_mt_wb(void){\n\treturn vmx_capability.ept & VMX_EPTP_WB_BIT;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_162": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_invept_global(void){\n\treturn vmx_capability.ept & VMX_EPT_EXTENT_GLOBAL_BIT;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_invept_global(void){\n\treturn vmx_capability.ept & VMX_EPT_EXTENT_GLOBAL_BIT;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_163": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_invvpid_global(void){\n\treturn vmx_capability.vpid & VMX_VPID_EXTENT_GLOBAL_CONTEXT_BIT;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_invvpid_global(void){\n\treturn vmx_capability.vpid & VMX_VPID_EXTENT_GLOBAL_CONTEXT_BIT;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_164": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_pml(void){\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_ENABLE_PML;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_pml(void){\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl & SECONDARY_EXEC_ENABLE_PML;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_165": {
        "vulnerable_code": {
            "Code": "cpu_has_vmx_vmfunc(void){\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_VMFUNC;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "cpu_has_vmx_vmfunc(void){\n\treturn vmcs_config.cpu_based_2nd_exec_ctrl &\n\t\tSECONDARY_EXEC_ENABLE_VMFUNC;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_166": {
        "vulnerable_code": {
            "Code": "enable_nmi_window(struct kvm_vcpu *vcpu){\n\tif (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {\n\t\tenable_irq_window(vcpu);\n\t\treturn;\n\t}\n\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t      CPU_BASED_VIRTUAL_NMI_PENDING);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "enable_nmi_window(struct kvm_vcpu *vcpu){\n\tif (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {\n\t\tenable_irq_window(vcpu);\n\t\treturn;\n\t}\n\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t      CPU_BASED_VIRTUAL_NMI_PENDING);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_167": {
        "vulnerable_code": {
            "Code": "guest_read_tsc(struct kvm_vcpu *vcpu){\n\tu64 host_tsc, tsc_offset;\n\n\thost_tsc = rdtsc();\n\ttsc_offset = vmcs_read64(TSC_OFFSET);\n\treturn kvm_scale_tsc(vcpu, host_tsc) + tsc_offset;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "guest_read_tsc(struct kvm_vcpu *vcpu){\n\tu64 host_tsc, tsc_offset;\n\n\thost_tsc = rdtsc();\n\ttsc_offset = vmcs_read64(TSC_OFFSET);\n\treturn kvm_scale_tsc(vcpu, host_tsc) + tsc_offset;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_168": {
        "vulnerable_code": {
            "Code": "handle_apic_write(struct kvm_vcpu *vcpu){\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 offset = exit_qualification & 0xfff;\n\n\t/* APIC-write VM exit is trap-like and thus no need to adjust IP */\n\tkvm_apic_write_nodecode(vcpu, offset);\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_apic_write(struct kvm_vcpu *vcpu){\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 offset = exit_qualification & 0xfff;\n\n\t/* APIC-write VM exit is trap-like and thus no need to adjust IP */\n\tkvm_apic_write_nodecode(vcpu, offset);\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_169": {
        "vulnerable_code": {
            "Code": "handle_cr(struct kvm_vcpu *vcpu){\n\tunsigned long exit_qualification, val;\n\tint cr;\n\tint reg;\n\tint err;\n\tint ret;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tcr = exit_qualification & 15;\n\treg = (exit_qualification >> 8) & 15;\n\tswitch ((exit_qualification >> 4) & 3) {\n\tcase 0: /* mov to cr */\n\t\tval = kvm_register_readl(vcpu, reg);\n\t\ttrace_kvm_cr_write(cr, val);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\terr = handle_set_cr0(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 3:\n\t\t\terr = kvm_set_cr3(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 4:\n\t\t\terr = handle_set_cr4(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 8: {\n\t\t\t\tu8 cr8_prev = kvm_get_cr8(vcpu);\n\t\t\t\tu8 cr8 = (u8)val;\n\t\t\t\terr = kvm_set_cr8(vcpu, cr8);\n\t\t\t\tret = kvm_complete_insn_gp(vcpu, err);\n\t\t\t\tif (lapic_in_kernel(vcpu))\n\t\t\t\t\treturn ret;\n\t\t\t\tif (cr8_prev <= cr8)\n\t\t\t\t\treturn ret;\n\t\t\t\t/*\n\t\t\t\t * TODO: we might be squashing a\n\t\t\t\t * KVM_GUESTDBG_SINGLESTEP-triggered\n\t\t\t\t * KVM_EXIT_DEBUG here.\n\t\t\t\t */\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_SET_TPR;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 2: /* clts */\n\t\tWARN_ONCE(1, \"Guest should always own CR0.TS\");\n\t\tvmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));\n\t\ttrace_kvm_cr_write(0, kvm_read_cr0(vcpu));\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tcase 1: /*mov from cr*/\n\t\tswitch (cr) {\n\t\tcase 3:\n\t\t\tval = kvm_read_cr3(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\tcase 8:\n\t\t\tval = kvm_get_cr8(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tbreak;\n\tcase 3: /* lmsw */\n\t\tval = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;\n\t\ttrace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);\n\t\tkvm_lmsw(vcpu, val);\n\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tdefault:\n\t\tbreak;\n\t}\n\tvcpu->run->exit_reason = 0;\n\tvcpu_unimpl(vcpu, \"unhandled control register: op %d cr %d\\n\",\n\t       (int)(exit_qualification >> 4) & 3, cr);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_cr(struct kvm_vcpu *vcpu){\n\tunsigned long exit_qualification, val;\n\tint cr;\n\tint reg;\n\tint err;\n\tint ret;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tcr = exit_qualification & 15;\n\treg = (exit_qualification >> 8) & 15;\n\tswitch ((exit_qualification >> 4) & 3) {\n\tcase 0: /* mov to cr */\n\t\tval = kvm_register_readl(vcpu, reg);\n\t\ttrace_kvm_cr_write(cr, val);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\terr = handle_set_cr0(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 3:\n\t\t\terr = kvm_set_cr3(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 4:\n\t\t\terr = handle_set_cr4(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 8: {\n\t\t\t\tu8 cr8_prev = kvm_get_cr8(vcpu);\n\t\t\t\tu8 cr8 = (u8)val;\n\t\t\t\terr = kvm_set_cr8(vcpu, cr8);\n\t\t\t\tret = kvm_complete_insn_gp(vcpu, err);\n\t\t\t\tif (lapic_in_kernel(vcpu))\n\t\t\t\t\treturn ret;\n\t\t\t\tif (cr8_prev <= cr8)\n\t\t\t\t\treturn ret;\n\t\t\t\t/*\n\t\t\t\t * TODO: we might be squashing a\n\t\t\t\t * KVM_GUESTDBG_SINGLESTEP-triggered\n\t\t\t\t * KVM_EXIT_DEBUG here.\n\t\t\t\t */\n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_SET_TPR;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 2: /* clts */\n\t\tWARN_ONCE(1, \"Guest should always own CR0.TS\");\n\t\tvmx_set_cr0(vcpu, kvm_read_cr0_bits(vcpu, ~X86_CR0_TS));\n\t\ttrace_kvm_cr_write(0, kvm_read_cr0(vcpu));\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tcase 1: /*mov from cr*/\n\t\tswitch (cr) {\n\t\tcase 3:\n\t\t\tval = kvm_read_cr3(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\tcase 8:\n\t\t\tval = kvm_get_cr8(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tbreak;\n\tcase 3: /* lmsw */\n\t\tval = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;\n\t\ttrace_kvm_cr_write(0, (kvm_read_cr0(vcpu) & ~0xful) | val);\n\t\tkvm_lmsw(vcpu, val);\n\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tdefault:\n\t\tbreak;\n\t}\n\tvcpu->run->exit_reason = 0;\n\tvcpu_unimpl(vcpu, \"unhandled control register: op %d cr %d\\n\",\n\t       (int)(exit_qualification >> 4) & 3, cr);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_170": {
        "vulnerable_code": {
            "Code": "handle_dr(struct kvm_vcpu *vcpu){\n\tunsigned long exit_qualification;\n\tint dr, dr7, reg;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tdr = exit_qualification & DEBUG_REG_ACCESS_NUM;\n\n\t/* First, if DR does not exist, trigger UD */\n\tif (!kvm_require_dr(vcpu, dr))\n\t\treturn 1;\n\n\t/* Do not handle if the CPL > 0, will trigger GP on re-entry */\n\tif (!kvm_require_cpl(vcpu, 0))\n\t\treturn 1;\n\tdr7 = vmcs_readl(GUEST_DR7);\n\tif (dr7 & DR7_GD) {\n\t\t/*\n\t\t * As the vm-exit takes precedence over the debug trap, we\n\t\t * need to emulate the latter, either for the host or the\n\t\t * guest debugging itself.\n\t\t */\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {\n\t\t\tvcpu->run->debug.arch.dr6 = vcpu->arch.dr6;\n\t\t\tvcpu->run->debug.arch.dr7 = dr7;\n\t\t\tvcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\t\tvcpu->run->debug.arch.exception = DB_VECTOR;\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_DEBUG;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= DR6_BD | DR6_RTM;\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (vcpu->guest_debug == 0) {\n\t\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tCPU_BASED_MOV_DR_EXITING);\n\n\t\t/*\n\t\t * No more DR vmexits; force a reload of the debug registers\n\t\t * and reenter on this instruction.  The next vmexit will\n\t\t * retrieve the full state of the debug registers.\n\t\t */\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;\n\t\treturn 1;\n\t}\n\n\treg = DEBUG_REG_ACCESS_REG(exit_qualification);\n\tif (exit_qualification & TYPE_MOV_FROM_DR) {\n\t\tunsigned long val;\n\n\t\tif (kvm_get_dr(vcpu, dr, &val))\n\t\t\treturn 1;\n\t\tkvm_register_write(vcpu, reg, val);\n\t} else\n\t\tif (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))\n\t\t\treturn 1;\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_dr(struct kvm_vcpu *vcpu){\n\tunsigned long exit_qualification;\n\tint dr, dr7, reg;\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tdr = exit_qualification & DEBUG_REG_ACCESS_NUM;\n\n\t/* First, if DR does not exist, trigger UD */\n\tif (!kvm_require_dr(vcpu, dr))\n\t\treturn 1;\n\n\t/* Do not handle if the CPL > 0, will trigger GP on re-entry */\n\tif (!kvm_require_cpl(vcpu, 0))\n\t\treturn 1;\n\tdr7 = vmcs_readl(GUEST_DR7);\n\tif (dr7 & DR7_GD) {\n\t\t/*\n\t\t * As the vm-exit takes precedence over the debug trap, we\n\t\t * need to emulate the latter, either for the host or the\n\t\t * guest debugging itself.\n\t\t */\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {\n\t\t\tvcpu->run->debug.arch.dr6 = vcpu->arch.dr6;\n\t\t\tvcpu->run->debug.arch.dr7 = dr7;\n\t\t\tvcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\t\tvcpu->run->debug.arch.exception = DB_VECTOR;\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_DEBUG;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tvcpu->arch.dr6 &= ~15;\n\t\t\tvcpu->arch.dr6 |= DR6_BD | DR6_RTM;\n\t\t\tkvm_queue_exception(vcpu, DB_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (vcpu->guest_debug == 0) {\n\t\tvmcs_clear_bits(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tCPU_BASED_MOV_DR_EXITING);\n\n\t\t/*\n\t\t * No more DR vmexits; force a reload of the debug registers\n\t\t * and reenter on this instruction.  The next vmexit will\n\t\t * retrieve the full state of the debug registers.\n\t\t */\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;\n\t\treturn 1;\n\t}\n\n\treg = DEBUG_REG_ACCESS_REG(exit_qualification);\n\tif (exit_qualification & TYPE_MOV_FROM_DR) {\n\t\tunsigned long val;\n\n\t\tif (kvm_get_dr(vcpu, dr, &val))\n\t\t\treturn 1;\n\t\tkvm_register_write(vcpu, reg, val);\n\t} else\n\t\tif (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))\n\t\t\treturn 1;\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_171": {
        "vulnerable_code": {
            "Code": "handle_halt(struct kvm_vcpu *vcpu){\n\treturn kvm_emulate_halt(vcpu);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_halt(struct kvm_vcpu *vcpu){\n\treturn kvm_emulate_halt(vcpu);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_172": {
        "vulnerable_code": {
            "Code": "handle_invalid_op(struct kvm_vcpu *vcpu){\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_invalid_op(struct kvm_vcpu *vcpu){\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_173": {
        "vulnerable_code": {
            "Code": "handle_monitor_trap(struct kvm_vcpu *vcpu){\n\treturn 1;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_monitor_trap(struct kvm_vcpu *vcpu){\n\treturn 1;\n}",
            "Size": 2,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_174": {
        "vulnerable_code": {
            "Code": "handle_preemption_timer(struct kvm_vcpu *vcpu){\n\tkvm_lapic_expired_hv_timer(vcpu);\n\treturn 1;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_preemption_timer(struct kvm_vcpu *vcpu){\n\tkvm_lapic_expired_hv_timer(vcpu);\n\treturn 1;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_175": {
        "vulnerable_code": {
            "Code": "handle_task_switch(struct kvm_vcpu *vcpu){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qualification;\n\tbool has_error_code = false;\n\tu32 error_code = 0;\n\tu16 tss_selector;\n\tint reason, type, idt_v, idt_index;\n\n\tidt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);\n\tidt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);\n\ttype = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\treason = (u32)exit_qualification >> 30;\n\tif (reason == TASK_SWITCH_GATE && idt_v) {\n\t\tswitch (type) {\n\t\tcase INTR_TYPE_NMI_INTR:\n\t\t\tvcpu->arch.nmi_injected = false;\n\t\t\tvmx_set_nmi_mask(vcpu, true);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_EXT_INTR:\n\t\tcase INTR_TYPE_SOFT_INTR:\n\t\t\tkvm_clear_interrupt_queue(vcpu);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_HARD_EXCEPTION:\n\t\t\tif (vmx->idt_vectoring_info &\n\t\t\t    VECTORING_INFO_DELIVER_CODE_MASK) {\n\t\t\t\thas_error_code = true;\n\t\t\t\terror_code =\n\t\t\t\t\tvmcs_read32(IDT_VECTORING_ERROR_CODE);\n\t\t\t}\n\t\t\t/* fall through */\n\t\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\t\tkvm_clear_exception_queue(vcpu);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\ttss_selector = exit_qualification;\n\n\tif (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&\n\t\t       type != INTR_TYPE_EXT_INTR &&\n\t\t       type != INTR_TYPE_NMI_INTR))\n\t\tskip_emulated_instruction(vcpu);\n\n\tif (kvm_task_switch(vcpu, tss_selector,\n\t\t\t    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,\n\t\t\t    has_error_code, error_code) == EMULATE_FAIL) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: What about debug traps on tss switch?\n\t *       Are we supposed to inject them and update dr6?\n\t */\n\n\treturn 1;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_task_switch(struct kvm_vcpu *vcpu){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qualification;\n\tbool has_error_code = false;\n\tu32 error_code = 0;\n\tu16 tss_selector;\n\tint reason, type, idt_v, idt_index;\n\n\tidt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);\n\tidt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);\n\ttype = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);\n\n\texit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\n\treason = (u32)exit_qualification >> 30;\n\tif (reason == TASK_SWITCH_GATE && idt_v) {\n\t\tswitch (type) {\n\t\tcase INTR_TYPE_NMI_INTR:\n\t\t\tvcpu->arch.nmi_injected = false;\n\t\t\tvmx_set_nmi_mask(vcpu, true);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_EXT_INTR:\n\t\tcase INTR_TYPE_SOFT_INTR:\n\t\t\tkvm_clear_interrupt_queue(vcpu);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_HARD_EXCEPTION:\n\t\t\tif (vmx->idt_vectoring_info &\n\t\t\t    VECTORING_INFO_DELIVER_CODE_MASK) {\n\t\t\t\thas_error_code = true;\n\t\t\t\terror_code =\n\t\t\t\t\tvmcs_read32(IDT_VECTORING_ERROR_CODE);\n\t\t\t}\n\t\t\t/* fall through */\n\t\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\t\tkvm_clear_exception_queue(vcpu);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\ttss_selector = exit_qualification;\n\n\tif (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&\n\t\t       type != INTR_TYPE_EXT_INTR &&\n\t\t       type != INTR_TYPE_NMI_INTR))\n\t\tskip_emulated_instruction(vcpu);\n\n\tif (kvm_task_switch(vcpu, tss_selector,\n\t\t\t    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,\n\t\t\t    has_error_code, error_code) == EMULATE_FAIL) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * TODO: What about debug traps on tss switch?\n\t *       Are we supposed to inject them and update dr6?\n\t */\n\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_176": {
        "vulnerable_code": {
            "Code": "handle_vmclear(struct kvm_vcpu *vcpu){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 zero = 0;\n\tgpa_t vmptr;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr))\n\t\treturn 1;\n\n\tif (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.vmxon_ptr) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.current_vmptr)\n\t\tnested_release_vmcs12(vmx);\n\n\tkvm_vcpu_write_guest(vcpu,\n\t\t\tvmptr + offsetof(struct vmcs12, launch_state),\n\t\t\t&zero, sizeof(zero));\n\n\tnested_free_vmcs02(vmx, vmptr);\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_vmclear(struct kvm_vcpu *vcpu){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 zero = 0;\n\tgpa_t vmptr;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr))\n\t\treturn 1;\n\n\tif (!PAGE_ALIGNED(vmptr) || (vmptr >> cpuid_maxphyaddr(vcpu))) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.vmxon_ptr) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\tif (vmptr == vmx->nested.current_vmptr)\n\t\tnested_release_vmcs12(vmx);\n\n\tkvm_vcpu_write_guest(vcpu,\n\t\t\tvmptr + offsetof(struct vmcs12, launch_state),\n\t\t\t&zero, sizeof(zero));\n\n\tnested_free_vmcs02(vmx, vmptr);\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_177": {
        "vulnerable_code": {
            "Code": "handle_vmread(struct kvm_vcpu *vcpu){\n\tunsigned long field;\n\tu64 field_value;\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgva_t gva = 0;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!nested_vmx_check_vmcs12(vcpu))\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\t/* Decode instruction info and find the field to read */\n\tfield = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));\n\t/* Read the field, zero-extended to a u64 field_value */\n\tif (vmcs12_read_any(vcpu, field, &field_value) < 0) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\t/*\n\t * Now copy part of this value to register or memory, as requested.\n\t * Note that the number of bits actually copied is 32 or 64 depending\n\t * on the guest's mode (32 or 64 bit), not on the given field's length.\n\t */\n\tif (vmx_instruction_info & (1u << 10)) {\n\t\tkvm_register_writel(vcpu, (((vmx_instruction_info) >> 3) & 0xf),\n\t\t\tfield_value);\n\t} else {\n\t\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\t\tvmx_instruction_info, true, &gva))\n\t\t\treturn 1;\n\t\t/* _system ok, as hardware has verified cpl=0 */\n\t\tkvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, gva,\n\t\t\t     &field_value, (is_long_mode(vcpu) ? 8 : 4), NULL);\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "handle_vmread(struct kvm_vcpu *vcpu){\n\tunsigned long field;\n\tu64 field_value;\n\tunsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);\n\tu32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgva_t gva = 0;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (!nested_vmx_check_vmcs12(vcpu))\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\n\t/* Decode instruction info and find the field to read */\n\tfield = kvm_register_readl(vcpu, (((vmx_instruction_info) >> 28) & 0xf));\n\t/* Read the field, zero-extended to a u64 field_value */\n\tif (vmcs12_read_any(vcpu, field, &field_value) < 0) {\n\t\tnested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\t/*\n\t * Now copy part of this value to register or memory, as requested.\n\t * Note that the number of bits actually copied is 32 or 64 depending\n\t * on the guest's mode (32 or 64 bit), not on the given field's length.\n\t */\n\tif (vmx_instruction_info & (1u << 10)) {\n\t\tkvm_register_writel(vcpu, (((vmx_instruction_info) >> 3) & 0xf),\n\t\t\tfield_value);\n\t} else {\n\t\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\t\tvmx_instruction_info, true, &gva))\n\t\t\treturn 1;\n\t\t/* _system ok, as hardware has verified cpl=0 */\n\t\tkvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, gva,\n\t\t\t     &field_value, (is_long_mode(vcpu) ? 8 : 4), NULL);\n\t}\n\n\tnested_vmx_succeed(vcpu);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_178": {
        "vulnerable_code": {
            "Code": "init_vmcs_shadow_fields(void){\n\tint i, j;\n\n\t/* No checks for read only fields yet */\n\n\tfor (i = j = 0; i < max_shadow_read_write_fields; i++) {\n\t\tswitch (shadow_read_write_fields[i]) {\n\t\tcase GUEST_BNDCFGS:\n\t\t\tif (!kvm_mpx_supported())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (j < i)\n\t\t\tshadow_read_write_fields[j] =\n\t\t\t\tshadow_read_write_fields[i];\n\t\tj++;\n\t}\n\tmax_shadow_read_write_fields = j;\n\n\t/* shadowed fields guest access without vmexit */\n\tfor (i = 0; i < max_shadow_read_write_fields; i++) {\n\t\tunsigned long field = shadow_read_write_fields[i];\n\n\t\tclear_bit(field, vmx_vmwrite_bitmap);\n\t\tclear_bit(field, vmx_vmread_bitmap);\n\t\tif (vmcs_field_type(field) == VMCS_FIELD_TYPE_U64) {\n\t\t\tclear_bit(field + 1, vmx_vmwrite_bitmap);\n\t\t\tclear_bit(field + 1, vmx_vmread_bitmap);\n\t\t}\n\t}\n\tfor (i = 0; i < max_shadow_read_only_fields; i++) {\n\t\tunsigned long field = shadow_read_only_fields[i];\n\n\t\tclear_bit(field, vmx_vmread_bitmap);\n\t\tif (vmcs_field_type(field) == VMCS_FIELD_TYPE_U64)\n\t\t\tclear_bit(field + 1, vmx_vmread_bitmap);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "init_vmcs_shadow_fields(void){\n\tint i, j;\n\n\t/* No checks for read only fields yet */\n\n\tfor (i = j = 0; i < max_shadow_read_write_fields; i++) {\n\t\tswitch (shadow_read_write_fields[i]) {\n\t\tcase GUEST_BNDCFGS:\n\t\t\tif (!kvm_mpx_supported())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (j < i)\n\t\t\tshadow_read_write_fields[j] =\n\t\t\t\tshadow_read_write_fields[i];\n\t\tj++;\n\t}\n\tmax_shadow_read_write_fields = j;\n\n\t/* shadowed fields guest access without vmexit */\n\tfor (i = 0; i < max_shadow_read_write_fields; i++) {\n\t\tunsigned long field = shadow_read_write_fields[i];\n\n\t\tclear_bit(field, vmx_vmwrite_bitmap);\n\t\tclear_bit(field, vmx_vmread_bitmap);\n\t\tif (vmcs_field_type(field) == VMCS_FIELD_TYPE_U64) {\n\t\t\tclear_bit(field + 1, vmx_vmwrite_bitmap);\n\t\t\tclear_bit(field + 1, vmx_vmread_bitmap);\n\t\t}\n\t}\n\tfor (i = 0; i < max_shadow_read_only_fields; i++) {\n\t\tunsigned long field = shadow_read_only_fields[i];\n\n\t\tclear_bit(field, vmx_vmread_bitmap);\n\t\tif (vmcs_field_type(field) == VMCS_FIELD_TYPE_U64)\n\t\t\tclear_bit(field + 1, vmx_vmread_bitmap);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_179": {
        "vulnerable_code": {
            "Code": "is_bitwise_subset(u64 superset, u64 subset, u64 mask){\n\tsuperset &= mask;\n\tsubset &= mask;\n\n\treturn (superset | subset) == superset;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "is_bitwise_subset(u64 superset, u64 subset, u64 mask){\n\tsuperset &= mask;\n\tsubset &= mask;\n\n\treturn (superset | subset) == superset;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_180": {
        "vulnerable_code": {
            "Code": "is_no_device(u32 intr_info){\n\treturn is_exception_n(intr_info, NM_VECTOR);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "is_no_device(u32 intr_info){\n\treturn is_exception_n(intr_info, NM_VECTOR);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_181": {
        "vulnerable_code": {
            "Code": "is_page_fault(u32 intr_info){\n\treturn is_exception_n(intr_info, PF_VECTOR);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "is_page_fault(u32 intr_info){\n\treturn is_exception_n(intr_info, PF_VECTOR);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_182": {
        "vulnerable_code": {
            "Code": "kvm_flush_pml_buffers(struct kvm *kvm){\n\tint i;\n\tstruct kvm_vcpu *vcpu;\n\t/*\n\t * We only need to kick vcpu out of guest mode here, as PML buffer\n\t * is flushed at beginning of all VMEXITs, and it's obvious that only\n\t * vcpus running in guest are possible to have unflushed GPAs in PML\n\t * buffer.\n\t */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "kvm_flush_pml_buffers(struct kvm *kvm){\n\tint i;\n\tstruct kvm_vcpu *vcpu;\n\t/*\n\t * We only need to kick vcpu out of guest mode here, as PML buffer\n\t * is flushed at beginning of all VMEXITs, and it's obvious that only\n\t * vcpus running in guest are possible to have unflushed GPAs in PML\n\t * buffer.\n\t */\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tkvm_vcpu_kick(vcpu);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_183": {
        "vulnerable_code": {
            "Code": "kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t     bool nested){\n#ifdef CONFIG_SMP\n\tint pi_vec = nested ? POSTED_INTR_NESTED_VECTOR : POSTED_INTR_VECTOR;\n\n\tif (vcpu->mode == IN_GUEST_MODE) {\n\t\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t\t/*\n\t\t * Currently, we don't support urgent interrupt,\n\t\t * all interrupts are recognized as non-urgent\n\t\t * interrupt, so we cannot post interrupts when\n\t\t * 'SN' is set.\n\t\t *\n\t\t * If the vcpu is in guest mode, it means it is\n\t\t * running instead of being scheduled out and\n\t\t * waiting in the run queue, and that's the only\n\t\t * case when 'SN' is set currently, warning if\n\t\t * 'SN' is set.\n\t\t */\n\t\tWARN_ON_ONCE(pi_test_sn(&vmx->pi_desc));\n\n\t\tapic->send_IPI_mask(get_cpu_mask(vcpu->cpu), pi_vec);\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t     bool nested){\n#ifdef CONFIG_SMP\n\tint pi_vec = nested ? POSTED_INTR_NESTED_VECTOR : POSTED_INTR_VECTOR;\n\n\tif (vcpu->mode == IN_GUEST_MODE) {\n\t\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t\t/*\n\t\t * Currently, we don't support urgent interrupt,\n\t\t * all interrupts are recognized as non-urgent\n\t\t * interrupt, so we cannot post interrupts when\n\t\t * 'SN' is set.\n\t\t *\n\t\t * If the vcpu is in guest mode, it means it is\n\t\t * running instead of being scheduled out and\n\t\t * waiting in the run queue, and that's the only\n\t\t * case when 'SN' is set currently, warning if\n\t\t * 'SN' is set.\n\t\t */\n\t\tWARN_ON_ONCE(pi_test_sn(&vmx->pi_desc));\n\n\t\tapic->send_IPI_mask(get_cpu_mask(vcpu->cpu), pi_vec);\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_184": {
        "vulnerable_code": {
            "Code": "nested_cpu_has_ept(struct vmcs12 *vmcs12){\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_EPT);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "nested_cpu_has_ept(struct vmcs12 *vmcs12){\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_EPT);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_185": {
        "vulnerable_code": {
            "Code": "nested_cpu_has_vmfunc(struct vmcs12 *vmcs12){\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_VMFUNC);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "nested_cpu_has_vmfunc(struct vmcs12 *vmcs12){\n\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_VMFUNC);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_186": {
        "vulnerable_code": {
            "Code": "nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,\n\t\tstruct x86_exception *fault){\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 exit_reason;\n\tunsigned long exit_qualification = vcpu->arch.exit_qualification;\n\n\tif (vmx->nested.pml_full) {\n\t\texit_reason = EXIT_REASON_PML_FULL;\n\t\tvmx->nested.pml_full = false;\n\t\texit_qualification &= INTR_INFO_UNBLOCK_NMI;\n\t} else if (fault->error_code & PFERR_RSVD_MASK)\n\t\texit_reason = EXIT_REASON_EPT_MISCONFIG;\n\telse\n\t\texit_reason = EXIT_REASON_EPT_VIOLATION;\n\n\tnested_vmx_vmexit(vcpu, exit_reason, 0, exit_qualification);\n\tvmcs12->guest_physical_address = fault->address;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,\n\t\tstruct x86_exception *fault){\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 exit_reason;\n\tunsigned long exit_qualification = vcpu->arch.exit_qualification;\n\n\tif (vmx->nested.pml_full) {\n\t\texit_reason = EXIT_REASON_PML_FULL;\n\t\tvmx->nested.pml_full = false;\n\t\texit_qualification &= INTR_INFO_UNBLOCK_NMI;\n\t} else if (fault->error_code & PFERR_RSVD_MASK)\n\t\texit_reason = EXIT_REASON_EPT_MISCONFIG;\n\telse\n\t\texit_reason = EXIT_REASON_EPT_VIOLATION;\n\n\tnested_vmx_vmexit(vcpu, exit_reason, 0, exit_qualification);\n\tvmcs12->guest_physical_address = fault->address;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_187": {
        "vulnerable_code": {
            "Code": "nested_exit_intr_ack_set(struct kvm_vcpu *vcpu){\n\treturn get_vmcs12(vcpu)->vm_exit_controls &\n\t\tVM_EXIT_ACK_INTR_ON_EXIT;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "nested_exit_intr_ack_set(struct kvm_vcpu *vcpu){\n\treturn get_vmcs12(vcpu)->vm_exit_controls &\n\t\tVM_EXIT_ACK_INTR_ON_EXIT;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_188": {
        "vulnerable_code": {
            "Code": "nested_vmx_check_tpr_shadow_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12){\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn 0;\n\n\tif (!page_address_valid(vcpu, vmcs12->virtual_apic_page_addr))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "nested_vmx_check_tpr_shadow_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12){\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn 0;\n\n\tif (!page_address_valid(vcpu, vmcs12->virtual_apic_page_addr))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_189": {
        "vulnerable_code": {
            "Code": "nested_vmx_entry_failure(struct kvm_vcpu *vcpu,\n\t\t\tstruct vmcs12 *vmcs12,\n\t\t\tu32 reason, unsigned long qualification){\n\tload_vmcs12_host_state(vcpu, vmcs12);\n\tvmcs12->vm_exit_reason = reason | VMX_EXIT_REASONS_FAILED_VMENTRY;\n\tvmcs12->exit_qualification = qualification;\n\tnested_vmx_succeed(vcpu);\n\tif (enable_shadow_vmcs)\n\t\tto_vmx(vcpu)->nested.sync_shadow_vmcs = true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "nested_vmx_entry_failure(struct kvm_vcpu *vcpu,\n\t\t\tstruct vmcs12 *vmcs12,\n\t\t\tu32 reason, unsigned long qualification){\n\tload_vmcs12_host_state(vcpu, vmcs12);\n\tvmcs12->vm_exit_reason = reason | VMX_EXIT_REASONS_FAILED_VMENTRY;\n\tvmcs12->exit_qualification = qualification;\n\tnested_vmx_succeed(vcpu);\n\tif (enable_shadow_vmcs)\n\t\tto_vmx(vcpu)->nested.sync_shadow_vmcs = true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_190": {
        "vulnerable_code": {
            "Code": "nested_vmx_merge_msr_bitmap(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       struct vmcs12 *vmcs12){\n\tint msr;\n\tstruct page *page;\n\tunsigned long *msr_bitmap_l1;\n\tunsigned long *msr_bitmap_l0 = to_vmx(vcpu)->nested.msr_bitmap;\n\n\t/* This shortcut is ok because we support only x2APIC MSRs so far. */\n\tif (!nested_cpu_has_virt_x2apic_mode(vmcs12))\n\t\treturn false;\n\n\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->msr_bitmap);\n\tif (is_error_page(page))\n\t\treturn false;\n\tmsr_bitmap_l1 = (unsigned long *)kmap(page);\n\n\tmemset(msr_bitmap_l0, 0xff, PAGE_SIZE);\n\n\tif (nested_cpu_has_virt_x2apic_mode(vmcs12)) {\n\t\tif (nested_cpu_has_apic_reg_virt(vmcs12))\n\t\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\t\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\tmsr, MSR_TYPE_R);\n\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\tAPIC_BASE_MSR + (APIC_TASKPRI >> 4),\n\t\t\t\tMSR_TYPE_R | MSR_TYPE_W);\n\n\t\tif (nested_cpu_has_vid(vmcs12)) {\n\t\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\tAPIC_BASE_MSR + (APIC_EOI >> 4),\n\t\t\t\tMSR_TYPE_W);\n\t\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\tAPIC_BASE_MSR + (APIC_SELF_IPI >> 4),\n\t\t\t\tMSR_TYPE_W);\n\t\t}\n\t}\n\tkunmap(page);\n\tkvm_release_page_clean(page);\n\n\treturn true;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "nested_vmx_merge_msr_bitmap(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       struct vmcs12 *vmcs12){\n\tint msr;\n\tstruct page *page;\n\tunsigned long *msr_bitmap_l1;\n\tunsigned long *msr_bitmap_l0 = to_vmx(vcpu)->nested.msr_bitmap;\n\n\t/* This shortcut is ok because we support only x2APIC MSRs so far. */\n\tif (!nested_cpu_has_virt_x2apic_mode(vmcs12))\n\t\treturn false;\n\n\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->msr_bitmap);\n\tif (is_error_page(page))\n\t\treturn false;\n\tmsr_bitmap_l1 = (unsigned long *)kmap(page);\n\n\tmemset(msr_bitmap_l0, 0xff, PAGE_SIZE);\n\n\tif (nested_cpu_has_virt_x2apic_mode(vmcs12)) {\n\t\tif (nested_cpu_has_apic_reg_virt(vmcs12))\n\t\t\tfor (msr = 0x800; msr <= 0x8ff; msr++)\n\t\t\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\tmsr, MSR_TYPE_R);\n\n\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\tAPIC_BASE_MSR + (APIC_TASKPRI >> 4),\n\t\t\t\tMSR_TYPE_R | MSR_TYPE_W);\n\n\t\tif (nested_cpu_has_vid(vmcs12)) {\n\t\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\tAPIC_BASE_MSR + (APIC_EOI >> 4),\n\t\t\t\tMSR_TYPE_W);\n\t\t\tnested_vmx_disable_intercept_for_msr(\n\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\tAPIC_BASE_MSR + (APIC_SELF_IPI >> 4),\n\t\t\t\tMSR_TYPE_W);\n\t\t}\n\t}\n\tkunmap(page);\n\tkvm_release_page_clean(page);\n\n\treturn true;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_191": {
        "vulnerable_code": {
            "Code": "nested_vmx_msr_check_common(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmx_msr_entry *e){\n\t/* x2APIC MSR accesses are not allowed */\n\tif (vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8)\n\t\treturn -EINVAL;\n\tif (e->index == MSR_IA32_UCODE_WRITE || /* SDM Table 35-2 */\n\t    e->index == MSR_IA32_UCODE_REV)\n\t\treturn -EINVAL;\n\tif (e->reserved != 0)\n\t\treturn -EINVAL;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "nested_vmx_msr_check_common(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmx_msr_entry *e){\n\t/* x2APIC MSR accesses are not allowed */\n\tif (vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8)\n\t\treturn -EINVAL;\n\tif (e->index == MSR_IA32_UCODE_WRITE || /* SDM Table 35-2 */\n\t    e->index == MSR_IA32_UCODE_REV)\n\t\treturn -EINVAL;\n\tif (e->reserved != 0)\n\t\treturn -EINVAL;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_192": {
        "vulnerable_code": {
            "Code": "pi_clear_sn(struct pi_desc *pi_desc){\n\treturn clear_bit(POSTED_INTR_SN,\n\t\t\t(unsigned long *)&pi_desc->control);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "pi_clear_sn(struct pi_desc *pi_desc){\n\treturn clear_bit(POSTED_INTR_SN,\n\t\t\t(unsigned long *)&pi_desc->control);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_193": {
        "vulnerable_code": {
            "Code": "pi_pre_block(struct kvm_vcpu *vcpu){\n\tunsigned long flags;\n\tunsigned int dest;\n\tstruct pi_desc old, new;\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\n\tif (!kvm_arch_has_assigned_device(vcpu->kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP)  ||\n\t\t!kvm_vcpu_apicv_active(vcpu))\n\t\treturn 0;\n\n\tvcpu->pre_pcpu = vcpu->cpu;\n\tspin_lock_irqsave(&per_cpu(blocked_vcpu_on_cpu_lock,\n\t\t\t  vcpu->pre_pcpu), flags);\n\tlist_add_tail(&vcpu->blocked_vcpu_list,\n\t\t      &per_cpu(blocked_vcpu_on_cpu,\n\t\t      vcpu->pre_pcpu));\n\tspin_unlock_irqrestore(&per_cpu(blocked_vcpu_on_cpu_lock,\n\t\t\t       vcpu->pre_pcpu), flags);\n\n\tdo {\n\t\told.control = new.control = pi_desc->control;\n\n\t\t/*\n\t\t * We should not block the vCPU if\n\t\t * an interrupt is posted for it.\n\t\t */\n\t\tif (pi_test_on(pi_desc) == 1) {\n\t\t\tspin_lock_irqsave(&per_cpu(blocked_vcpu_on_cpu_lock,\n\t\t\t\t\t  vcpu->pre_pcpu), flags);\n\t\t\tlist_del(&vcpu->blocked_vcpu_list);\n\t\t\tspin_unlock_irqrestore(\n\t\t\t\t\t&per_cpu(blocked_vcpu_on_cpu_lock,\n\t\t\t\t\tvcpu->pre_pcpu), flags);\n\t\t\tvcpu->pre_pcpu = -1;\n\n\t\t\treturn 1;\n\t\t}\n\n\t\tWARN((pi_desc->sn == 1),\n\t\t     \"Warning: SN field of posted-interrupts \"\n\t\t     \"is set before blocking\\n\");\n\n\t\t/*\n\t\t * Since vCPU can be preempted during this process,\n\t\t * vcpu->cpu could be different with pre_pcpu, we\n\t\t * need to set pre_pcpu as the destination of wakeup\n\t\t * notification event, then we can find the right vCPU\n\t\t * to wakeup in wakeup handler if interrupts happen\n\t\t * when the vCPU is in blocked state.\n\t\t */\n\t\tdest = cpu_physical_id(vcpu->pre_pcpu);\n\n\t\tif (x2apic_enabled())\n\t\t\tnew.ndst = dest;\n\t\telse\n\t\t\tnew.ndst = (dest << 8) & 0xFF00;\n\n\t\t/* set 'NV' to 'wakeup vector' */\n\t\tnew.nv = POSTED_INTR_WAKEUP_VECTOR;\n\t} while (cmpxchg(&pi_desc->control, old.control,\n\t\t\tnew.control) != old.control);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "pi_pre_block(struct kvm_vcpu *vcpu){\n\tunsigned long flags;\n\tunsigned int dest;\n\tstruct pi_desc old, new;\n\tstruct pi_desc *pi_desc = vcpu_to_pi_desc(vcpu);\n\n\tif (!kvm_arch_has_assigned_device(vcpu->kvm) ||\n\t\t!irq_remapping_cap(IRQ_POSTING_CAP)  ||\n\t\t!kvm_vcpu_apicv_active(vcpu))\n\t\treturn 0;\n\n\tvcpu->pre_pcpu = vcpu->cpu;\n\tspin_lock_irqsave(&per_cpu(blocked_vcpu_on_cpu_lock,\n\t\t\t  vcpu->pre_pcpu), flags);\n\tlist_add_tail(&vcpu->blocked_vcpu_list,\n\t\t      &per_cpu(blocked_vcpu_on_cpu,\n\t\t      vcpu->pre_pcpu));\n\tspin_unlock_irqrestore(&per_cpu(blocked_vcpu_on_cpu_lock,\n\t\t\t       vcpu->pre_pcpu), flags);\n\n\tdo {\n\t\told.control = new.control = pi_desc->control;\n\n\t\t/*\n\t\t * We should not block the vCPU if\n\t\t * an interrupt is posted for it.\n\t\t */\n\t\tif (pi_test_on(pi_desc) == 1) {\n\t\t\tspin_lock_irqsave(&per_cpu(blocked_vcpu_on_cpu_lock,\n\t\t\t\t\t  vcpu->pre_pcpu), flags);\n\t\t\tlist_del(&vcpu->blocked_vcpu_list);\n\t\t\tspin_unlock_irqrestore(\n\t\t\t\t\t&per_cpu(blocked_vcpu_on_cpu_lock,\n\t\t\t\t\tvcpu->pre_pcpu), flags);\n\t\t\tvcpu->pre_pcpu = -1;\n\n\t\t\treturn 1;\n\t\t}\n\n\t\tWARN((pi_desc->sn == 1),\n\t\t     \"Warning: SN field of posted-interrupts \"\n\t\t     \"is set before blocking\\n\");\n\n\t\t/*\n\t\t * Since vCPU can be preempted during this process,\n\t\t * vcpu->cpu could be different with pre_pcpu, we\n\t\t * need to set pre_pcpu as the destination of wakeup\n\t\t * notification event, then we can find the right vCPU\n\t\t * to wakeup in wakeup handler if interrupts happen\n\t\t * when the vCPU is in blocked state.\n\t\t */\n\t\tdest = cpu_physical_id(vcpu->pre_pcpu);\n\n\t\tif (x2apic_enabled())\n\t\t\tnew.ndst = dest;\n\t\telse\n\t\t\tnew.ndst = (dest << 8) & 0xFF00;\n\n\t\t/* set 'NV' to 'wakeup vector' */\n\t\tnew.nv = POSTED_INTR_WAKEUP_VECTOR;\n\t} while (cmpxchg(&pi_desc->control, old.control,\n\t\t\tnew.control) != old.control);\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_194": {
        "vulnerable_code": {
            "Code": "setup_msrs(struct vcpu_vmx *vmx){\n\tint save_nmsrs, index;\n\n\tsave_nmsrs = 0;\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu)) {\n\t\tindex = __find_msr_index(vmx, MSR_SYSCALL_MASK);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_LSTAR);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_CSTAR);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_TSC_AUX);\n\t\tif (index >= 0 && guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP))\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\t/*\n\t\t * MSR_STAR is only needed on long mode guests, and only\n\t\t * if efer.sce is enabled.\n\t\t */\n\t\tindex = __find_msr_index(vmx, MSR_STAR);\n\t\tif ((index >= 0) && (vmx->vcpu.arch.efer & EFER_SCE))\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t}\n#endif\n\tindex = __find_msr_index(vmx, MSR_EFER);\n\tif (index >= 0 && update_transition_efer(vmx, index))\n\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\n\tvmx->save_nmsrs = save_nmsrs;\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_set_msr_bitmap(&vmx->vcpu);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "setup_msrs(struct vcpu_vmx *vmx){\n\tint save_nmsrs, index;\n\n\tsave_nmsrs = 0;\n#ifdef CONFIG_X86_64\n\tif (is_long_mode(&vmx->vcpu)) {\n\t\tindex = __find_msr_index(vmx, MSR_SYSCALL_MASK);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_LSTAR);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_CSTAR);\n\t\tif (index >= 0)\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\tindex = __find_msr_index(vmx, MSR_TSC_AUX);\n\t\tif (index >= 0 && guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP))\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t\t/*\n\t\t * MSR_STAR is only needed on long mode guests, and only\n\t\t * if efer.sce is enabled.\n\t\t */\n\t\tindex = __find_msr_index(vmx, MSR_STAR);\n\t\tif ((index >= 0) && (vmx->vcpu.arch.efer & EFER_SCE))\n\t\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\t}\n#endif\n\tindex = __find_msr_index(vmx, MSR_EFER);\n\tif (index >= 0 && update_transition_efer(vmx, index))\n\t\tmove_msr_up(vmx, index, save_nmsrs++);\n\n\tvmx->save_nmsrs = save_nmsrs;\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmx_set_msr_bitmap(&vmx->vcpu);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_195": {
        "vulnerable_code": {
            "Code": "skip_emulated_instruction(struct kvm_vcpu *vcpu){\n\tunsigned long rip;\n\n\trip = kvm_rip_read(vcpu);\n\trip += vmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\tkvm_rip_write(vcpu, rip);\n\n\t/* skipping an emulated instruction also counts */\n\tvmx_set_interrupt_shadow(vcpu, 0);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "skip_emulated_instruction(struct kvm_vcpu *vcpu){\n\tunsigned long rip;\n\n\trip = kvm_rip_read(vcpu);\n\trip += vmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\tkvm_rip_write(vcpu, rip);\n\n\t/* skipping an emulated instruction also counts */\n\tvmx_set_interrupt_shadow(vcpu, 0);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_196": {
        "vulnerable_code": {
            "Code": "sync_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12){\n\tvmcs12->guest_cr0 = vmcs12_guest_cr0(vcpu, vmcs12);\n\tvmcs12->guest_cr4 = vmcs12_guest_cr4(vcpu, vmcs12);\n\n\tvmcs12->guest_rsp = kvm_register_read(vcpu, VCPU_REGS_RSP);\n\tvmcs12->guest_rip = kvm_register_read(vcpu, VCPU_REGS_RIP);\n\tvmcs12->guest_rflags = vmcs_readl(GUEST_RFLAGS);\n\n\tvmcs12->guest_es_selector = vmcs_read16(GUEST_ES_SELECTOR);\n\tvmcs12->guest_cs_selector = vmcs_read16(GUEST_CS_SELECTOR);\n\tvmcs12->guest_ss_selector = vmcs_read16(GUEST_SS_SELECTOR);\n\tvmcs12->guest_ds_selector = vmcs_read16(GUEST_DS_SELECTOR);\n\tvmcs12->guest_fs_selector = vmcs_read16(GUEST_FS_SELECTOR);\n\tvmcs12->guest_gs_selector = vmcs_read16(GUEST_GS_SELECTOR);\n\tvmcs12->guest_ldtr_selector = vmcs_read16(GUEST_LDTR_SELECTOR);\n\tvmcs12->guest_tr_selector = vmcs_read16(GUEST_TR_SELECTOR);\n\tvmcs12->guest_es_limit = vmcs_read32(GUEST_ES_LIMIT);\n\tvmcs12->guest_cs_limit = vmcs_read32(GUEST_CS_LIMIT);\n\tvmcs12->guest_ss_limit = vmcs_read32(GUEST_SS_LIMIT);\n\tvmcs12->guest_ds_limit = vmcs_read32(GUEST_DS_LIMIT);\n\tvmcs12->guest_fs_limit = vmcs_read32(GUEST_FS_LIMIT);\n\tvmcs12->guest_gs_limit = vmcs_read32(GUEST_GS_LIMIT);\n\tvmcs12->guest_ldtr_limit = vmcs_read32(GUEST_LDTR_LIMIT);\n\tvmcs12->guest_tr_limit = vmcs_read32(GUEST_TR_LIMIT);\n\tvmcs12->guest_gdtr_limit = vmcs_read32(GUEST_GDTR_LIMIT);\n\tvmcs12->guest_idtr_limit = vmcs_read32(GUEST_IDTR_LIMIT);\n\tvmcs12->guest_es_ar_bytes = vmcs_read32(GUEST_ES_AR_BYTES);\n\tvmcs12->guest_cs_ar_bytes = vmcs_read32(GUEST_CS_AR_BYTES);\n\tvmcs12->guest_ss_ar_bytes = vmcs_read32(GUEST_SS_AR_BYTES);\n\tvmcs12->guest_ds_ar_bytes = vmcs_read32(GUEST_DS_AR_BYTES);\n\tvmcs12->guest_fs_ar_bytes = vmcs_read32(GUEST_FS_AR_BYTES);\n\tvmcs12->guest_gs_ar_bytes = vmcs_read32(GUEST_GS_AR_BYTES);\n\tvmcs12->guest_ldtr_ar_bytes = vmcs_read32(GUEST_LDTR_AR_BYTES);\n\tvmcs12->guest_tr_ar_bytes = vmcs_read32(GUEST_TR_AR_BYTES);\n\tvmcs12->guest_es_base = vmcs_readl(GUEST_ES_BASE);\n\tvmcs12->guest_cs_base = vmcs_readl(GUEST_CS_BASE);\n\tvmcs12->guest_ss_base = vmcs_readl(GUEST_SS_BASE);\n\tvmcs12->guest_ds_base = vmcs_readl(GUEST_DS_BASE);\n\tvmcs12->guest_fs_base = vmcs_readl(GUEST_FS_BASE);\n\tvmcs12->guest_gs_base = vmcs_readl(GUEST_GS_BASE);\n\tvmcs12->guest_ldtr_base = vmcs_readl(GUEST_LDTR_BASE);\n\tvmcs12->guest_tr_base = vmcs_readl(GUEST_TR_BASE);\n\tvmcs12->guest_gdtr_base = vmcs_readl(GUEST_GDTR_BASE);\n\tvmcs12->guest_idtr_base = vmcs_readl(GUEST_IDTR_BASE);\n\n\tvmcs12->guest_interruptibility_info =\n\t\tvmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tvmcs12->guest_pending_dbg_exceptions =\n\t\tvmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);\n\tif (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_HLT;\n\telse\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_ACTIVE;\n\n\tif (nested_cpu_has_preemption_timer(vmcs12)) {\n\t\tif (vmcs12->vm_exit_controls &\n\t\t    VM_EXIT_SAVE_VMX_PREEMPTION_TIMER)\n\t\t\tvmcs12->vmx_preemption_timer_value =\n\t\t\t\tvmx_get_preemption_timer_value(vcpu);\n\t\thrtimer_cancel(&to_vmx(vcpu)->nested.preemption_timer);\n\t}\n\n\t/*\n\t * In some cases (usually, nested EPT), L2 is allowed to change its\n\t * own CR3 without exiting. If it has changed it, we must keep it.\n\t * Of course, if L0 is using shadow page tables, GUEST_CR3 was defined\n\t * by L0, not L1 or L2, so we mustn't unconditionally copy it to vmcs12.\n\t *\n\t * Additionally, restore L2's PDPTR to vmcs12.\n\t */\n\tif (enable_ept) {\n\t\tvmcs12->guest_cr3 = vmcs_readl(GUEST_CR3);\n\t\tvmcs12->guest_pdptr0 = vmcs_read64(GUEST_PDPTR0);\n\t\tvmcs12->guest_pdptr1 = vmcs_read64(GUEST_PDPTR1);\n\t\tvmcs12->guest_pdptr2 = vmcs_read64(GUEST_PDPTR2);\n\t\tvmcs12->guest_pdptr3 = vmcs_read64(GUEST_PDPTR3);\n\t}\n\n\tvmcs12->guest_linear_address = vmcs_readl(GUEST_LINEAR_ADDRESS);\n\n\tif (nested_cpu_has_vid(vmcs12))\n\t\tvmcs12->guest_intr_status = vmcs_read16(GUEST_INTR_STATUS);\n\n\tvmcs12->vm_entry_controls =\n\t\t(vmcs12->vm_entry_controls & ~VM_ENTRY_IA32E_MODE) |\n\t\t(vm_entry_controls_get(to_vmx(vcpu)) & VM_ENTRY_IA32E_MODE);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_DEBUG_CONTROLS) {\n\t\tkvm_get_dr(vcpu, 7, (unsigned long *)&vmcs12->guest_dr7);\n\t\tvmcs12->guest_ia32_debugctl = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\t}\n\n\t/* TODO: These cannot have changed unless we have MSR bitmaps and\n\t * the relevant bit asks not to trap the change */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_PAT)\n\t\tvmcs12->guest_ia32_pat = vmcs_read64(GUEST_IA32_PAT);\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_EFER)\n\t\tvmcs12->guest_ia32_efer = vcpu->arch.efer;\n\tvmcs12->guest_sysenter_cs = vmcs_read32(GUEST_SYSENTER_CS);\n\tvmcs12->guest_sysenter_esp = vmcs_readl(GUEST_SYSENTER_ESP);\n\tvmcs12->guest_sysenter_eip = vmcs_readl(GUEST_SYSENTER_EIP);\n\tif (kvm_mpx_supported())\n\t\tvmcs12->guest_bndcfgs = vmcs_read64(GUEST_BNDCFGS);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sync_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12){\n\tvmcs12->guest_cr0 = vmcs12_guest_cr0(vcpu, vmcs12);\n\tvmcs12->guest_cr4 = vmcs12_guest_cr4(vcpu, vmcs12);\n\n\tvmcs12->guest_rsp = kvm_register_read(vcpu, VCPU_REGS_RSP);\n\tvmcs12->guest_rip = kvm_register_read(vcpu, VCPU_REGS_RIP);\n\tvmcs12->guest_rflags = vmcs_readl(GUEST_RFLAGS);\n\n\tvmcs12->guest_es_selector = vmcs_read16(GUEST_ES_SELECTOR);\n\tvmcs12->guest_cs_selector = vmcs_read16(GUEST_CS_SELECTOR);\n\tvmcs12->guest_ss_selector = vmcs_read16(GUEST_SS_SELECTOR);\n\tvmcs12->guest_ds_selector = vmcs_read16(GUEST_DS_SELECTOR);\n\tvmcs12->guest_fs_selector = vmcs_read16(GUEST_FS_SELECTOR);\n\tvmcs12->guest_gs_selector = vmcs_read16(GUEST_GS_SELECTOR);\n\tvmcs12->guest_ldtr_selector = vmcs_read16(GUEST_LDTR_SELECTOR);\n\tvmcs12->guest_tr_selector = vmcs_read16(GUEST_TR_SELECTOR);\n\tvmcs12->guest_es_limit = vmcs_read32(GUEST_ES_LIMIT);\n\tvmcs12->guest_cs_limit = vmcs_read32(GUEST_CS_LIMIT);\n\tvmcs12->guest_ss_limit = vmcs_read32(GUEST_SS_LIMIT);\n\tvmcs12->guest_ds_limit = vmcs_read32(GUEST_DS_LIMIT);\n\tvmcs12->guest_fs_limit = vmcs_read32(GUEST_FS_LIMIT);\n\tvmcs12->guest_gs_limit = vmcs_read32(GUEST_GS_LIMIT);\n\tvmcs12->guest_ldtr_limit = vmcs_read32(GUEST_LDTR_LIMIT);\n\tvmcs12->guest_tr_limit = vmcs_read32(GUEST_TR_LIMIT);\n\tvmcs12->guest_gdtr_limit = vmcs_read32(GUEST_GDTR_LIMIT);\n\tvmcs12->guest_idtr_limit = vmcs_read32(GUEST_IDTR_LIMIT);\n\tvmcs12->guest_es_ar_bytes = vmcs_read32(GUEST_ES_AR_BYTES);\n\tvmcs12->guest_cs_ar_bytes = vmcs_read32(GUEST_CS_AR_BYTES);\n\tvmcs12->guest_ss_ar_bytes = vmcs_read32(GUEST_SS_AR_BYTES);\n\tvmcs12->guest_ds_ar_bytes = vmcs_read32(GUEST_DS_AR_BYTES);\n\tvmcs12->guest_fs_ar_bytes = vmcs_read32(GUEST_FS_AR_BYTES);\n\tvmcs12->guest_gs_ar_bytes = vmcs_read32(GUEST_GS_AR_BYTES);\n\tvmcs12->guest_ldtr_ar_bytes = vmcs_read32(GUEST_LDTR_AR_BYTES);\n\tvmcs12->guest_tr_ar_bytes = vmcs_read32(GUEST_TR_AR_BYTES);\n\tvmcs12->guest_es_base = vmcs_readl(GUEST_ES_BASE);\n\tvmcs12->guest_cs_base = vmcs_readl(GUEST_CS_BASE);\n\tvmcs12->guest_ss_base = vmcs_readl(GUEST_SS_BASE);\n\tvmcs12->guest_ds_base = vmcs_readl(GUEST_DS_BASE);\n\tvmcs12->guest_fs_base = vmcs_readl(GUEST_FS_BASE);\n\tvmcs12->guest_gs_base = vmcs_readl(GUEST_GS_BASE);\n\tvmcs12->guest_ldtr_base = vmcs_readl(GUEST_LDTR_BASE);\n\tvmcs12->guest_tr_base = vmcs_readl(GUEST_TR_BASE);\n\tvmcs12->guest_gdtr_base = vmcs_readl(GUEST_GDTR_BASE);\n\tvmcs12->guest_idtr_base = vmcs_readl(GUEST_IDTR_BASE);\n\n\tvmcs12->guest_interruptibility_info =\n\t\tvmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tvmcs12->guest_pending_dbg_exceptions =\n\t\tvmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);\n\tif (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_HLT;\n\telse\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_ACTIVE;\n\n\tif (nested_cpu_has_preemption_timer(vmcs12)) {\n\t\tif (vmcs12->vm_exit_controls &\n\t\t    VM_EXIT_SAVE_VMX_PREEMPTION_TIMER)\n\t\t\tvmcs12->vmx_preemption_timer_value =\n\t\t\t\tvmx_get_preemption_timer_value(vcpu);\n\t\thrtimer_cancel(&to_vmx(vcpu)->nested.preemption_timer);\n\t}\n\n\t/*\n\t * In some cases (usually, nested EPT), L2 is allowed to change its\n\t * own CR3 without exiting. If it has changed it, we must keep it.\n\t * Of course, if L0 is using shadow page tables, GUEST_CR3 was defined\n\t * by L0, not L1 or L2, so we mustn't unconditionally copy it to vmcs12.\n\t *\n\t * Additionally, restore L2's PDPTR to vmcs12.\n\t */\n\tif (enable_ept) {\n\t\tvmcs12->guest_cr3 = vmcs_readl(GUEST_CR3);\n\t\tvmcs12->guest_pdptr0 = vmcs_read64(GUEST_PDPTR0);\n\t\tvmcs12->guest_pdptr1 = vmcs_read64(GUEST_PDPTR1);\n\t\tvmcs12->guest_pdptr2 = vmcs_read64(GUEST_PDPTR2);\n\t\tvmcs12->guest_pdptr3 = vmcs_read64(GUEST_PDPTR3);\n\t}\n\n\tvmcs12->guest_linear_address = vmcs_readl(GUEST_LINEAR_ADDRESS);\n\n\tif (nested_cpu_has_vid(vmcs12))\n\t\tvmcs12->guest_intr_status = vmcs_read16(GUEST_INTR_STATUS);\n\n\tvmcs12->vm_entry_controls =\n\t\t(vmcs12->vm_entry_controls & ~VM_ENTRY_IA32E_MODE) |\n\t\t(vm_entry_controls_get(to_vmx(vcpu)) & VM_ENTRY_IA32E_MODE);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_DEBUG_CONTROLS) {\n\t\tkvm_get_dr(vcpu, 7, (unsigned long *)&vmcs12->guest_dr7);\n\t\tvmcs12->guest_ia32_debugctl = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\t}\n\n\t/* TODO: These cannot have changed unless we have MSR bitmaps and\n\t * the relevant bit asks not to trap the change */\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_PAT)\n\t\tvmcs12->guest_ia32_pat = vmcs_read64(GUEST_IA32_PAT);\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_EFER)\n\t\tvmcs12->guest_ia32_efer = vcpu->arch.efer;\n\tvmcs12->guest_sysenter_cs = vmcs_read32(GUEST_SYSENTER_CS);\n\tvmcs12->guest_sysenter_esp = vmcs_readl(GUEST_SYSENTER_ESP);\n\tvmcs12->guest_sysenter_eip = vmcs_readl(GUEST_SYSENTER_EIP);\n\tif (kvm_mpx_supported())\n\t\tvmcs12->guest_bndcfgs = vmcs_read64(GUEST_BNDCFGS);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_197": {
        "vulnerable_code": {
            "Code": "update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr){\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t\tnested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn;\n\n\tif (irr == -1 || tpr < irr) {\n\t\tvmcs_write32(TPR_THRESHOLD, 0);\n\t\treturn;\n\t}\n\n\tvmcs_write32(TPR_THRESHOLD, irr);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr){\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t\tnested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn;\n\n\tif (irr == -1 || tpr < irr) {\n\t\tvmcs_write32(TPR_THRESHOLD, 0);\n\t\treturn;\n\t}\n\n\tvmcs_write32(TPR_THRESHOLD, irr);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_198": {
        "vulnerable_code": {
            "Code": "vm_exit_controls_set(struct vcpu_vmx *vmx, u32 val){\n\tif (vmx->vm_exit_controls_shadow != val)\n\t\tvm_exit_controls_init(vmx, val);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vm_exit_controls_set(struct vcpu_vmx *vmx, u32 val){\n\tif (vmx->vm_exit_controls_shadow != val)\n\t\tvm_exit_controls_init(vmx, val);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_199": {
        "vulnerable_code": {
            "Code": "vmclear_local_loaded_vmcss(void){\n\tint cpu = raw_smp_processor_id();\n\tstruct loaded_vmcs *v, *n;\n\n\tlist_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),\n\t\t\t\t loaded_vmcss_on_cpu_link)\n\t\t__loaded_vmcs_clear(v);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmclear_local_loaded_vmcss(void){\n\tint cpu = raw_smp_processor_id();\n\tstruct loaded_vmcs *v, *n;\n\n\tlist_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),\n\t\t\t\t loaded_vmcss_on_cpu_link)\n\t\t__loaded_vmcs_clear(v);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_200": {
        "vulnerable_code": {
            "Code": "vmcs_field_to_offset(unsigned long field){\n\tBUILD_BUG_ON(ARRAY_SIZE(vmcs_field_to_offset_table) > SHRT_MAX);\n\n\tif (field >= ARRAY_SIZE(vmcs_field_to_offset_table) ||\n\t    vmcs_field_to_offset_table[field] == 0)\n\t\treturn -ENOENT;\n\n\treturn vmcs_field_to_offset_table[field];\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmcs_field_to_offset(unsigned long field){\n\tBUILD_BUG_ON(ARRAY_SIZE(vmcs_field_to_offset_table) > SHRT_MAX);\n\n\tif (field >= ARRAY_SIZE(vmcs_field_to_offset_table) ||\n\t    vmcs_field_to_offset_table[field] == 0)\n\t\treturn -ENOENT;\n\n\treturn vmcs_field_to_offset_table[field];\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_201": {
        "vulnerable_code": {
            "Code": "vmcs_load(struct vmcs *vmcs){\n\tu64 phys_addr = __pa(vmcs);\n\tu8 error;\n\n\tasm volatile (__ex(ASM_VMX_VMPTRLD_RAX) \"; setna %0\"\n\t\t\t: \"=qm\"(error) : \"a\"(&phys_addr), \"m\"(phys_addr)\n\t\t\t: \"cc\", \"memory\");\n\tif (error)\n\t\tprintk(KERN_ERR \"kvm: vmptrld %p/%llx failed\\n\",\n\t\t       vmcs, phys_addr);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmcs_load(struct vmcs *vmcs){\n\tu64 phys_addr = __pa(vmcs);\n\tu8 error;\n\n\tasm volatile (__ex(ASM_VMX_VMPTRLD_RAX) \"; setna %0\"\n\t\t\t: \"=qm\"(error) : \"a\"(&phys_addr), \"m\"(phys_addr)\n\t\t\t: \"cc\", \"memory\");\n\tif (error)\n\t\tprintk(KERN_ERR \"kvm: vmptrld %p/%llx failed\\n\",\n\t\t       vmcs, phys_addr);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_202": {
        "vulnerable_code": {
            "Code": "vmx_check_nested_events(struct kvm_vcpu *vcpu, bool external_intr){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qual;\n\n\tif (kvm_event_needs_reinjection(vcpu))\n\t\treturn -EBUSY;\n\n\tif (vcpu->arch.exception.pending &&\n\t\tnested_vmx_check_exception(vcpu, &exit_qual)) {\n\t\tif (vmx->nested.nested_run_pending)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_inject_exception_vmexit(vcpu, exit_qual);\n\t\tvcpu->arch.exception.pending = false;\n\t\treturn 0;\n\t}\n\n\tif (nested_cpu_has_preemption_timer(get_vmcs12(vcpu)) &&\n\t    vmx->nested.preemption_timer_expired) {\n\t\tif (vmx->nested.nested_run_pending)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.nmi_pending && nested_exit_on_nmi(vcpu)) {\n\t\tif (vmx->nested.nested_run_pending)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,\n\t\t\t\t  NMI_VECTOR | INTR_TYPE_NMI_INTR |\n\t\t\t\t  INTR_INFO_VALID_MASK, 0);\n\t\t/*\n\t\t * The NMI-triggered VM exit counts as injection:\n\t\t * clear this one and block further NMIs.\n\t\t */\n\t\tvcpu->arch.nmi_pending = 0;\n\t\tvmx_set_nmi_mask(vcpu, true);\n\t\treturn 0;\n\t}\n\n\tif ((kvm_cpu_has_interrupt(vcpu) || external_intr) &&\n\t    nested_exit_on_intr(vcpu)) {\n\t\tif (vmx->nested.nested_run_pending)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);\n\t\treturn 0;\n\t}\n\n\tvmx_complete_nested_posted_interrupt(vcpu);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_check_nested_events(struct kvm_vcpu *vcpu, bool external_intr){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qual;\n\n\tif (kvm_event_needs_reinjection(vcpu))\n\t\treturn -EBUSY;\n\n\tif (vcpu->arch.exception.pending &&\n\t\tnested_vmx_check_exception(vcpu, &exit_qual)) {\n\t\tif (vmx->nested.nested_run_pending)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_inject_exception_vmexit(vcpu, exit_qual);\n\t\tvcpu->arch.exception.pending = false;\n\t\treturn 0;\n\t}\n\n\tif (nested_cpu_has_preemption_timer(get_vmcs12(vcpu)) &&\n\t    vmx->nested.preemption_timer_expired) {\n\t\tif (vmx->nested.nested_run_pending)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.nmi_pending && nested_exit_on_nmi(vcpu)) {\n\t\tif (vmx->nested.nested_run_pending)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,\n\t\t\t\t  NMI_VECTOR | INTR_TYPE_NMI_INTR |\n\t\t\t\t  INTR_INFO_VALID_MASK, 0);\n\t\t/*\n\t\t * The NMI-triggered VM exit counts as injection:\n\t\t * clear this one and block further NMIs.\n\t\t */\n\t\tvcpu->arch.nmi_pending = 0;\n\t\tvmx_set_nmi_mask(vcpu, true);\n\t\treturn 0;\n\t}\n\n\tif ((kvm_cpu_has_interrupt(vcpu) || external_intr) &&\n\t    nested_exit_on_intr(vcpu)) {\n\t\tif (vmx->nested.nested_run_pending)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);\n\t\treturn 0;\n\t}\n\n\tvmx_complete_nested_posted_interrupt(vcpu);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_203": {
        "vulnerable_code": {
            "Code": "vmx_complete_atomic_exit(struct vcpu_vmx *vmx){\n\tu32 exit_intr_info = 0;\n\tu16 basic_exit_reason = (u16)vmx->exit_reason;\n\n\tif (!(basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY\n\t      || basic_exit_reason == EXIT_REASON_EXCEPTION_NMI))\n\t\treturn;\n\n\tif (!(vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))\n\t\texit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tvmx->exit_intr_info = exit_intr_info;\n\n\t/* if exit due to PF check for async PF */\n\tif (is_page_fault(exit_intr_info))\n\t\tvmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();\n\n\t/* Handle machine checks before interrupts are enabled */\n\tif (basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY ||\n\t    is_machine_check(exit_intr_info))\n\t\tkvm_machine_check();\n\n\t/* We need to handle NMIs before interrupts are enabled */\n\tif (is_nmi(exit_intr_info)) {\n\t\tkvm_before_handle_nmi(&vmx->vcpu);\n\t\tasm(\"int $2\");\n\t\tkvm_after_handle_nmi(&vmx->vcpu);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_complete_atomic_exit(struct vcpu_vmx *vmx){\n\tu32 exit_intr_info = 0;\n\tu16 basic_exit_reason = (u16)vmx->exit_reason;\n\n\tif (!(basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY\n\t      || basic_exit_reason == EXIT_REASON_EXCEPTION_NMI))\n\t\treturn;\n\n\tif (!(vmx->exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY))\n\t\texit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);\n\tvmx->exit_intr_info = exit_intr_info;\n\n\t/* if exit due to PF check for async PF */\n\tif (is_page_fault(exit_intr_info))\n\t\tvmx->vcpu.arch.apf.host_apf_reason = kvm_read_and_reset_pf_reason();\n\n\t/* Handle machine checks before interrupts are enabled */\n\tif (basic_exit_reason == EXIT_REASON_MCE_DURING_VMENTRY ||\n\t    is_machine_check(exit_intr_info))\n\t\tkvm_machine_check();\n\n\t/* We need to handle NMIs before interrupts are enabled */\n\tif (is_nmi(exit_intr_info)) {\n\t\tkvm_before_handle_nmi(&vmx->vcpu);\n\t\tasm(\"int $2\");\n\t\tkvm_after_handle_nmi(&vmx->vcpu);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_204": {
        "vulnerable_code": {
            "Code": "vmx_create_vcpu(struct kvm *kvm, unsigned int id){\n\tint err;\n\tstruct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);\n\tint cpu;\n\n\tif (!vmx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvmx->vpid = allocate_vpid();\n\n\terr = kvm_vcpu_init(&vmx->vcpu, kvm, id);\n\tif (err)\n\t\tgoto free_vcpu;\n\n\terr = -ENOMEM;\n\n\t/*\n\t * If PML is turned on, failure on enabling PML just results in failure\n\t * of creating the vcpu, therefore we can simplify PML logic (by\n\t * avoiding dealing with cases, such as enabling PML partially on vcpus\n\t * for the guest, etc.\n\t */\n\tif (enable_pml) {\n\t\tvmx->pml_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\t\tif (!vmx->pml_pg)\n\t\t\tgoto uninit_vcpu;\n\t}\n\n\tvmx->guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tBUILD_BUG_ON(ARRAY_SIZE(vmx_msr_index) * sizeof(vmx->guest_msrs[0])\n\t\t     > PAGE_SIZE);\n\n\tif (!vmx->guest_msrs)\n\t\tgoto free_pml;\n\n\tvmx->loaded_vmcs = &vmx->vmcs01;\n\tvmx->loaded_vmcs->vmcs = alloc_vmcs();\n\tvmx->loaded_vmcs->shadow_vmcs = NULL;\n\tif (!vmx->loaded_vmcs->vmcs)\n\t\tgoto free_msrs;\n\tloaded_vmcs_init(vmx->loaded_vmcs);\n\n\tcpu = get_cpu();\n\tvmx_vcpu_load(&vmx->vcpu, cpu);\n\tvmx->vcpu.cpu = cpu;\n\terr = vmx_vcpu_setup(vmx);\n\tvmx_vcpu_put(&vmx->vcpu);\n\tput_cpu();\n\tif (err)\n\t\tgoto free_vmcs;\n\tif (cpu_need_virtualize_apic_accesses(&vmx->vcpu)) {\n\t\terr = alloc_apic_access_page(kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (enable_ept) {\n\t\tif (!kvm->arch.ept_identity_map_addr)\n\t\t\tkvm->arch.ept_identity_map_addr =\n\t\t\t\tVMX_EPT_IDENTITY_PAGETABLE_ADDR;\n\t\terr = init_rmode_identity_map(kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (nested) {\n\t\tnested_vmx_setup_ctls_msrs(vmx);\n\t\tvmx->nested.vpid02 = allocate_vpid();\n\t}\n\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = -1ull;\n\n\tvmx->msr_ia32_feature_control_valid_bits = FEATURE_CONTROL_LOCKED;\n\n\treturn &vmx->vcpu;\n\nfree_vmcs:\n\tfree_vpid(vmx->nested.vpid02);\n\tfree_loaded_vmcs(vmx->loaded_vmcs);\nfree_msrs:\n\tkfree(vmx->guest_msrs);\nfree_pml:\n\tvmx_destroy_pml_buffer(vmx);\nuninit_vcpu:\n\tkvm_vcpu_uninit(&vmx->vcpu);\nfree_vcpu:\n\tfree_vpid(vmx->vpid);\n\tkmem_cache_free(kvm_vcpu_cache, vmx);\n\treturn ERR_PTR(err);\n}",
            "Size": 3,
            "Code Complexity": 0,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_create_vcpu(struct kvm *kvm, unsigned int id){\n\tint err;\n\tstruct vcpu_vmx *vmx = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);\n\tint cpu;\n\n\tif (!vmx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvmx->vpid = allocate_vpid();\n\n\terr = kvm_vcpu_init(&vmx->vcpu, kvm, id);\n\tif (err)\n\t\tgoto free_vcpu;\n\n\terr = -ENOMEM;\n\n\t/*\n\t * If PML is turned on, failure on enabling PML just results in failure\n\t * of creating the vcpu, therefore we can simplify PML logic (by\n\t * avoiding dealing with cases, such as enabling PML partially on vcpus\n\t * for the guest, etc.\n\t */\n\tif (enable_pml) {\n\t\tvmx->pml_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\t\tif (!vmx->pml_pg)\n\t\t\tgoto uninit_vcpu;\n\t}\n\n\tvmx->guest_msrs = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tBUILD_BUG_ON(ARRAY_SIZE(vmx_msr_index) * sizeof(vmx->guest_msrs[0])\n\t\t     > PAGE_SIZE);\n\n\tif (!vmx->guest_msrs)\n\t\tgoto free_pml;\n\n\tvmx->loaded_vmcs = &vmx->vmcs01;\n\tvmx->loaded_vmcs->vmcs = alloc_vmcs();\n\tvmx->loaded_vmcs->shadow_vmcs = NULL;\n\tif (!vmx->loaded_vmcs->vmcs)\n\t\tgoto free_msrs;\n\tloaded_vmcs_init(vmx->loaded_vmcs);\n\n\tcpu = get_cpu();\n\tvmx_vcpu_load(&vmx->vcpu, cpu);\n\tvmx->vcpu.cpu = cpu;\n\terr = vmx_vcpu_setup(vmx);\n\tvmx_vcpu_put(&vmx->vcpu);\n\tput_cpu();\n\tif (err)\n\t\tgoto free_vmcs;\n\tif (cpu_need_virtualize_apic_accesses(&vmx->vcpu)) {\n\t\terr = alloc_apic_access_page(kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (enable_ept) {\n\t\tif (!kvm->arch.ept_identity_map_addr)\n\t\t\tkvm->arch.ept_identity_map_addr =\n\t\t\t\tVMX_EPT_IDENTITY_PAGETABLE_ADDR;\n\t\terr = init_rmode_identity_map(kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (nested) {\n\t\tnested_vmx_setup_ctls_msrs(vmx);\n\t\tvmx->nested.vpid02 = allocate_vpid();\n\t}\n\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = -1ull;\n\n\tvmx->msr_ia32_feature_control_valid_bits = FEATURE_CONTROL_LOCKED;\n\n\treturn &vmx->vcpu;\n\nfree_vmcs:\n\tfree_vpid(vmx->nested.vpid02);\n\tfree_loaded_vmcs(vmx->loaded_vmcs);\nfree_msrs:\n\tkfree(vmx->guest_msrs);\nfree_pml:\n\tvmx_destroy_pml_buffer(vmx);\nuninit_vcpu:\n\tkvm_vcpu_uninit(&vmx->vcpu);\nfree_vcpu:\n\tfree_vpid(vmx->vpid);\n\tkmem_cache_free(kvm_vcpu_cache, vmx);\n\treturn ERR_PTR(err);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_205": {
        "vulnerable_code": {
            "Code": "vmx_disable_intercept_for_msr(u32 msr, bool longmode_only){\n\tif (!longmode_only)\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy,\n\t\t\t\t\t\tmsr, MSR_TYPE_R | MSR_TYPE_W);\n\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode,\n\t\t\t\t\t\tmsr, MSR_TYPE_R | MSR_TYPE_W);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_disable_intercept_for_msr(u32 msr, bool longmode_only){\n\tif (!longmode_only)\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy,\n\t\t\t\t\t\tmsr, MSR_TYPE_R | MSR_TYPE_W);\n\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode,\n\t\t\t\t\t\tmsr, MSR_TYPE_R | MSR_TYPE_W);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_206": {
        "vulnerable_code": {
            "Code": "vmx_disable_intercept_msr_x2apic(u32 msr, int type, bool apicv_active){\n\tif (apicv_active) {\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy_x2apic_apicv,\n\t\t\t\tmsr, type);\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode_x2apic_apicv,\n\t\t\t\tmsr, type);\n\t} else {\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy_x2apic,\n\t\t\t\tmsr, type);\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode_x2apic,\n\t\t\t\tmsr, type);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_disable_intercept_msr_x2apic(u32 msr, int type, bool apicv_active){\n\tif (apicv_active) {\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy_x2apic_apicv,\n\t\t\t\tmsr, type);\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode_x2apic_apicv,\n\t\t\t\tmsr, type);\n\t} else {\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_legacy_x2apic,\n\t\t\t\tmsr, type);\n\t\t__vmx_disable_intercept_for_msr(vmx_msr_bitmap_longmode_x2apic,\n\t\t\t\tmsr, type);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_207": {
        "vulnerable_code": {
            "Code": "vmx_get_dr6(struct kvm_vcpu *vcpu){\n\treturn vcpu->arch.dr6;\n}",
            "Size": 5,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_get_dr6(struct kvm_vcpu *vcpu){\n\treturn vcpu->arch.dr6;\n}",
            "Size": 3,
            "Code Complexity": 1,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_208": {
        "vulnerable_code": {
            "Code": "vmx_get_enable_apicv(struct kvm_vcpu *vcpu){\n\treturn enable_apicv;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_get_enable_apicv(struct kvm_vcpu *vcpu){\n\treturn enable_apicv;\n}",
            "Size": 4,
            "Code Complexity": 1,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_209": {
        "vulnerable_code": {
            "Code": "vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt){\n\tdt->size = vmcs_read32(GUEST_IDTR_LIMIT);\n\tdt->address = vmcs_readl(GUEST_IDTR_BASE);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt){\n\tdt->size = vmcs_read32(GUEST_IDTR_LIMIT);\n\tdt->address = vmcs_readl(GUEST_IDTR_BASE);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_210": {
        "vulnerable_code": {
            "Code": "vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu){\n\tu32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tint ret = 0;\n\n\tif (interruptibility & GUEST_INTR_STATE_STI)\n\t\tret |= KVM_X86_SHADOW_INT_STI;\n\tif (interruptibility & GUEST_INTR_STATE_MOV_SS)\n\t\tret |= KVM_X86_SHADOW_INT_MOV_SS;\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu){\n\tu32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tint ret = 0;\n\n\tif (interruptibility & GUEST_INTR_STATE_STI)\n\t\tret |= KVM_X86_SHADOW_INT_STI;\n\tif (interruptibility & GUEST_INTR_STATE_MOV_SS)\n\t\tret |= KVM_X86_SHADOW_INT_MOV_SS;\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_211": {
        "vulnerable_code": {
            "Code": "vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio){\n\tu8 cache;\n\tu64 ipat = 0;\n\n\t/* For VT-d and EPT combination\n\t * 1. MMIO: always map as UC\n\t * 2. EPT with VT-d:\n\t *   a. VT-d without snooping control feature: can't guarantee the\n\t *\tresult, try to trust guest.\n\t *   b. VT-d with snooping control feature: snooping control feature of\n\t *\tVT-d engine can guarantee the cache correctness. Just set it\n\t *\tto WB to keep consistent with host. So the same as item 3.\n\t * 3. EPT without VT-d: always map as WB and set IPAT=1 to keep\n\t *    consistent with host MTRR\n\t */\n\tif (is_mmio) {\n\t\tcache = MTRR_TYPE_UNCACHABLE;\n\t\tgoto exit;\n\t}\n\n\tif (!kvm_arch_has_noncoherent_dma(vcpu->kvm)) {\n\t\tipat = VMX_EPT_IPAT_BIT;\n\t\tcache = MTRR_TYPE_WRBACK;\n\t\tgoto exit;\n\t}\n\n\tif (kvm_read_cr0(vcpu) & X86_CR0_CD) {\n\t\tipat = VMX_EPT_IPAT_BIT;\n\t\tif (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\t\tcache = MTRR_TYPE_WRBACK;\n\t\telse\n\t\t\tcache = MTRR_TYPE_UNCACHABLE;\n\t\tgoto exit;\n\t}\n\n\tcache = kvm_mtrr_get_guest_memory_type(vcpu, gfn);\n\nexit:\n\treturn (cache << VMX_EPT_MT_EPTE_SHIFT) | ipat;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio){\n\tu8 cache;\n\tu64 ipat = 0;\n\n\t/* For VT-d and EPT combination\n\t * 1. MMIO: always map as UC\n\t * 2. EPT with VT-d:\n\t *   a. VT-d without snooping control feature: can't guarantee the\n\t *\tresult, try to trust guest.\n\t *   b. VT-d with snooping control feature: snooping control feature of\n\t *\tVT-d engine can guarantee the cache correctness. Just set it\n\t *\tto WB to keep consistent with host. So the same as item 3.\n\t * 3. EPT without VT-d: always map as WB and set IPAT=1 to keep\n\t *    consistent with host MTRR\n\t */\n\tif (is_mmio) {\n\t\tcache = MTRR_TYPE_UNCACHABLE;\n\t\tgoto exit;\n\t}\n\n\tif (!kvm_arch_has_noncoherent_dma(vcpu->kvm)) {\n\t\tipat = VMX_EPT_IPAT_BIT;\n\t\tcache = MTRR_TYPE_WRBACK;\n\t\tgoto exit;\n\t}\n\n\tif (kvm_read_cr0(vcpu) & X86_CR0_CD) {\n\t\tipat = VMX_EPT_IPAT_BIT;\n\t\tif (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\t\tcache = MTRR_TYPE_WRBACK;\n\t\telse\n\t\t\tcache = MTRR_TYPE_UNCACHABLE;\n\t\tgoto exit;\n\t}\n\n\tcache = kvm_mtrr_get_guest_memory_type(vcpu, gfn);\n\nexit:\n\treturn (cache << VMX_EPT_MT_EPTE_SHIFT) | ipat;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_212": {
        "vulnerable_code": {
            "Code": "vmx_get_preemption_timer_value(struct kvm_vcpu *vcpu){\n\tktime_t remaining =\n\t\thrtimer_get_remaining(&to_vmx(vcpu)->nested.preemption_timer);\n\tu64 value;\n\n\tif (ktime_to_ns(remaining) <= 0)\n\t\treturn 0;\n\n\tvalue = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;\n\tdo_div(value, 1000000);\n\treturn value >> VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_get_preemption_timer_value(struct kvm_vcpu *vcpu){\n\tktime_t remaining =\n\t\thrtimer_get_remaining(&to_vmx(vcpu)->nested.preemption_timer);\n\tu64 value;\n\n\tif (ktime_to_ns(remaining) <= 0)\n\t\treturn 0;\n\n\tvalue = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;\n\tdo_div(value, 1000000);\n\treturn value >> VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_213": {
        "vulnerable_code": {
            "Code": "vmx_get_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *pdata){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\t*pdata = vmx->nested.nested_vmx_basic;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_pinbased_ctls_low,\n\t\t\tvmx->nested.nested_vmx_pinbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PINBASED_CTLS)\n\t\t\t*pdata |= PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_procbased_ctls_low,\n\t\t\tvmx->nested.nested_vmx_procbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PROCBASED_CTLS)\n\t\t\t*pdata |= CPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_exit_ctls_low,\n\t\t\tvmx->nested.nested_vmx_exit_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_EXIT_CTLS)\n\t\t\t*pdata |= VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_entry_ctls_low,\n\t\t\tvmx->nested.nested_vmx_entry_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_ENTRY_CTLS)\n\t\t\t*pdata |= VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_MISC:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_misc_low,\n\t\t\tvmx->nested.nested_vmx_misc_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\t\t*pdata = vmx->nested.nested_vmx_cr0_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\t\t*pdata = vmx->nested.nested_vmx_cr0_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\t*pdata = vmx->nested.nested_vmx_cr4_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t*pdata = vmx->nested.nested_vmx_cr4_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\t*pdata = vmx->nested.nested_vmx_vmcs_enum;\n\t\tbreak;\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_secondary_ctls_low,\n\t\t\tvmx->nested.nested_vmx_secondary_ctls_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\t*pdata = vmx->nested.nested_vmx_ept_caps |\n\t\t\t((u64)vmx->nested.nested_vmx_vpid_caps << 32);\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMFUNC:\n\t\t*pdata = vmx->nested.nested_vmx_vmfunc_controls;\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_get_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 *pdata){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\t*pdata = vmx->nested.nested_vmx_basic;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_pinbased_ctls_low,\n\t\t\tvmx->nested.nested_vmx_pinbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PINBASED_CTLS)\n\t\t\t*pdata |= PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_procbased_ctls_low,\n\t\t\tvmx->nested.nested_vmx_procbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PROCBASED_CTLS)\n\t\t\t*pdata |= CPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_exit_ctls_low,\n\t\t\tvmx->nested.nested_vmx_exit_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_EXIT_CTLS)\n\t\t\t*pdata |= VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_entry_ctls_low,\n\t\t\tvmx->nested.nested_vmx_entry_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_ENTRY_CTLS)\n\t\t\t*pdata |= VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_MISC:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_misc_low,\n\t\t\tvmx->nested.nested_vmx_misc_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\t\t*pdata = vmx->nested.nested_vmx_cr0_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\t\t*pdata = vmx->nested.nested_vmx_cr0_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\t*pdata = vmx->nested.nested_vmx_cr4_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t*pdata = vmx->nested.nested_vmx_cr4_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\t*pdata = vmx->nested.nested_vmx_vmcs_enum;\n\t\tbreak;\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\t*pdata = vmx_control_msr(\n\t\t\tvmx->nested.nested_vmx_secondary_ctls_low,\n\t\t\tvmx->nested.nested_vmx_secondary_ctls_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\t*pdata = vmx->nested.nested_vmx_ept_caps |\n\t\t\t((u64)vmx->nested.nested_vmx_vpid_caps << 32);\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMFUNC:\n\t\t*pdata = vmx->nested.nested_vmx_vmfunc_controls;\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_214": {
        "vulnerable_code": {
            "Code": "vmx_mpx_supported(void){\n\treturn (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&\n\t\t(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_mpx_supported(void){\n\treturn (vmcs_config.vmexit_ctrl & VM_EXIT_CLEAR_BNDCFGS) &&\n\t\t(vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_BNDCFGS);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_215": {
        "vulnerable_code": {
            "Code": "vmx_segment_cache_clear(struct vcpu_vmx *vmx){\n\tvmx->segment_cache.bitmask = 0;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_segment_cache_clear(struct vcpu_vmx *vmx){\n\tvmx->segment_cache.bitmask = 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_216": {
        "vulnerable_code": {
            "Code": "vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt){\n\tvmcs_write32(GUEST_GDTR_LIMIT, dt->size);\n\tvmcs_writel(GUEST_GDTR_BASE, dt->address);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt){\n\tvmcs_write32(GUEST_GDTR_LIMIT, dt->size);\n\tvmcs_writel(GUEST_GDTR_BASE, dt->address);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_217": {
        "vulnerable_code": {
            "Code": "vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask){\n\tu32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tu32 interruptibility = interruptibility_old;\n\n\tinterruptibility &= ~(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS);\n\n\tif (mask & KVM_X86_SHADOW_INT_MOV_SS)\n\t\tinterruptibility |= GUEST_INTR_STATE_MOV_SS;\n\telse if (mask & KVM_X86_SHADOW_INT_STI)\n\t\tinterruptibility |= GUEST_INTR_STATE_STI;\n\n\tif ((interruptibility != interruptibility_old))\n\t\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask){\n\tu32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tu32 interruptibility = interruptibility_old;\n\n\tinterruptibility &= ~(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS);\n\n\tif (mask & KVM_X86_SHADOW_INT_MOV_SS)\n\t\tinterruptibility |= GUEST_INTR_STATE_MOV_SS;\n\telse if (mask & KVM_X86_SHADOW_INT_STI)\n\t\tinterruptibility |= GUEST_INTR_STATE_STI;\n\n\tif ((interruptibility != interruptibility_old))\n\t\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_218": {
        "vulnerable_code": {
            "Code": "vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct shared_msr_entry *msr;\n\tint ret = 0;\n\tu32 msr_index = msr_info->index;\n\tu64 data = msr_info->data;\n\n\tswitch (msr_index) {\n\tcase MSR_EFER:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_FS_BASE, data);\n\t\tbreak;\n\tcase MSR_GS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_GS_BASE, data);\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tvmx_load_host_state(vmx);\n\t\tvmx->msr_guest_kernel_gs_base = data;\n\t\tbreak;\n#endif\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tvmcs_write32(GUEST_SYSENTER_CS, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tvmcs_writel(GUEST_SYSENTER_EIP, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tvmcs_writel(GUEST_SYSENTER_ESP, data);\n\t\tbreak;\n\tcase MSR_IA32_BNDCFGS:\n\t\tif (!kvm_mpx_supported() ||\n\t\t    (!msr_info->host_initiated &&\n\t\t     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))\n\t\t\treturn 1;\n\t\tif (is_noncanonical_address(data & PAGE_MASK, vcpu) ||\n\t\t    (data & MSR_IA32_BNDCFGS_RSVD))\n\t\t\treturn 1;\n\t\tvmcs_write64(GUEST_BNDCFGS, data);\n\t\tbreak;\n\tcase MSR_IA32_TSC:\n\t\tkvm_write_tsc(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_CR_PAT:\n\t\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\t\tif (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))\n\t\t\t\treturn 1;\n\t\t\tvmcs_write64(GUEST_IA32_PAT, data);\n\t\t\tvcpu->arch.pat = data;\n\t\t\tbreak;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_MCG_EXT_CTL:\n\t\tif ((!msr_info->host_initiated &&\n\t\t     !(to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t       FEATURE_CONTROL_LMCE)) ||\n\t\t    (data & ~MCG_EXT_CTL_LMCE_EN))\n\t\t\treturn 1;\n\t\tvcpu->arch.mcg_ext_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_FEATURE_CONTROL:\n\t\tif (!vmx_feature_control_msr_valid(vcpu, data) ||\n\t\t    (to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t     FEATURE_CONTROL_LOCKED && !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\tvmx->msr_ia32_feature_control = data;\n\t\tif (msr_info->host_initiated && data == 0)\n\t\t\tvmx_leave_nested(vcpu);\n\t\tbreak;\n\tcase MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1; /* they are read-only */\n\t\tif (!nested_vmx_allowed(vcpu))\n\t\t\treturn 1;\n\t\treturn vmx_set_vmx_msr(vcpu, msr_index, data);\n\tcase MSR_IA32_XSS:\n\t\tif (!vmx_xsaves_supported())\n\t\t\treturn 1;\n\t\t/*\n\t\t * The only supported bit as of Skylake is bit 8, but\n\t\t * it is not supported on KVM.\n\t\t */\n\t\tif (data != 0)\n\t\t\treturn 1;\n\t\tvcpu->arch.ia32_xss = data;\n\t\tif (vcpu->arch.ia32_xss != host_xss)\n\t\t\tadd_atomic_switch_msr(vmx, MSR_IA32_XSS,\n\t\t\t\tvcpu->arch.ia32_xss, host_xss);\n\t\telse\n\t\t\tclear_atomic_switch_msr(vmx, MSR_IA32_XSS);\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))\n\t\t\treturn 1;\n\t\t/* Check reserved bit, higher 32 bits should be zero */\n\t\tif ((data >> 32) != 0)\n\t\t\treturn 1;\n\t\t/* Otherwise falls through */\n\tdefault:\n\t\tmsr = find_msr_entry(vmx, msr_index);\n\t\tif (msr) {\n\t\t\tu64 old_msr_data = msr->data;\n\t\t\tmsr->data = data;\n\t\t\tif (msr - vmx->guest_msrs < vmx->save_nmsrs) {\n\t\t\t\tpreempt_disable();\n\t\t\t\tret = kvm_set_shared_msr(msr->index, msr->data,\n\t\t\t\t\t\t\t msr->mask);\n\t\t\t\tpreempt_enable();\n\t\t\t\tif (ret)\n\t\t\t\t\tmsr->data = old_msr_data;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t}\n\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct shared_msr_entry *msr;\n\tint ret = 0;\n\tu32 msr_index = msr_info->index;\n\tu64 data = msr_info->data;\n\n\tswitch (msr_index) {\n\tcase MSR_EFER:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_FS_BASE, data);\n\t\tbreak;\n\tcase MSR_GS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_GS_BASE, data);\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tvmx_load_host_state(vmx);\n\t\tvmx->msr_guest_kernel_gs_base = data;\n\t\tbreak;\n#endif\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tvmcs_write32(GUEST_SYSENTER_CS, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tvmcs_writel(GUEST_SYSENTER_EIP, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tvmcs_writel(GUEST_SYSENTER_ESP, data);\n\t\tbreak;\n\tcase MSR_IA32_BNDCFGS:\n\t\tif (!kvm_mpx_supported() ||\n\t\t    (!msr_info->host_initiated &&\n\t\t     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))\n\t\t\treturn 1;\n\t\tif (is_noncanonical_address(data & PAGE_MASK, vcpu) ||\n\t\t    (data & MSR_IA32_BNDCFGS_RSVD))\n\t\t\treturn 1;\n\t\tvmcs_write64(GUEST_BNDCFGS, data);\n\t\tbreak;\n\tcase MSR_IA32_TSC:\n\t\tkvm_write_tsc(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_CR_PAT:\n\t\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\t\tif (!kvm_mtrr_valid(vcpu, MSR_IA32_CR_PAT, data))\n\t\t\t\treturn 1;\n\t\t\tvmcs_write64(GUEST_IA32_PAT, data);\n\t\t\tvcpu->arch.pat = data;\n\t\t\tbreak;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_TSC_ADJUST:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\tcase MSR_IA32_MCG_EXT_CTL:\n\t\tif ((!msr_info->host_initiated &&\n\t\t     !(to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t       FEATURE_CONTROL_LMCE)) ||\n\t\t    (data & ~MCG_EXT_CTL_LMCE_EN))\n\t\t\treturn 1;\n\t\tvcpu->arch.mcg_ext_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_FEATURE_CONTROL:\n\t\tif (!vmx_feature_control_msr_valid(vcpu, data) ||\n\t\t    (to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t     FEATURE_CONTROL_LOCKED && !msr_info->host_initiated))\n\t\t\treturn 1;\n\t\tvmx->msr_ia32_feature_control = data;\n\t\tif (msr_info->host_initiated && data == 0)\n\t\t\tvmx_leave_nested(vcpu);\n\t\tbreak;\n\tcase MSR_IA32_VMX_BASIC ... MSR_IA32_VMX_VMFUNC:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1; /* they are read-only */\n\t\tif (!nested_vmx_allowed(vcpu))\n\t\t\treturn 1;\n\t\treturn vmx_set_vmx_msr(vcpu, msr_index, data);\n\tcase MSR_IA32_XSS:\n\t\tif (!vmx_xsaves_supported())\n\t\t\treturn 1;\n\t\t/*\n\t\t * The only supported bit as of Skylake is bit 8, but\n\t\t * it is not supported on KVM.\n\t\t */\n\t\tif (data != 0)\n\t\t\treturn 1;\n\t\tvcpu->arch.ia32_xss = data;\n\t\tif (vcpu->arch.ia32_xss != host_xss)\n\t\t\tadd_atomic_switch_msr(vmx, MSR_IA32_XSS,\n\t\t\t\tvcpu->arch.ia32_xss, host_xss);\n\t\telse\n\t\t\tclear_atomic_switch_msr(vmx, MSR_IA32_XSS);\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))\n\t\t\treturn 1;\n\t\t/* Check reserved bit, higher 32 bits should be zero */\n\t\tif ((data >> 32) != 0)\n\t\t\treturn 1;\n\t\t/* Otherwise falls through */\n\tdefault:\n\t\tmsr = find_msr_entry(vmx, msr_index);\n\t\tif (msr) {\n\t\t\tu64 old_msr_data = msr->data;\n\t\t\tmsr->data = data;\n\t\t\tif (msr - vmx->guest_msrs < vmx->save_nmsrs) {\n\t\t\t\tpreempt_disable();\n\t\t\t\tret = kvm_set_shared_msr(msr->index, msr->data,\n\t\t\t\t\t\t\t msr->mask);\n\t\t\t\tpreempt_enable();\n\t\t\t\tif (ret)\n\t\t\t\t\tmsr->data = old_msr_data;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t}\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_219": {
        "vulnerable_code": {
            "Code": "vmx_set_tss_addr(struct kvm *kvm, unsigned int addr){\n\tint ret;\n\n\tret = x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,\n\t\t\t\t    PAGE_SIZE * 3);\n\tif (ret)\n\t\treturn ret;\n\tkvm->arch.tss_addr = addr;\n\treturn init_rmode_tss(kvm);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_set_tss_addr(struct kvm *kvm, unsigned int addr){\n\tint ret;\n\n\tret = x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,\n\t\t\t\t    PAGE_SIZE * 3);\n\tif (ret)\n\t\treturn ret;\n\tkvm->arch.tss_addr = addr;\n\treturn init_rmode_tss(kvm);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_220": {
        "vulnerable_code": {
            "Code": "vmx_set_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\treturn vmx_restore_vmx_basic(vmx, data);\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t/*\n\t\t * The \"non-true\" VMX capability MSRs are generated from the\n\t\t * \"true\" MSRs, so we do not support restoring them directly.\n\t\t *\n\t\t * If userspace wants to emulate VMX_BASIC[55]=0, userspace\n\t\t * should restore the \"true\" MSRs with the must-be-1 bits\n\t\t * set according to the SDM Vol 3. A.2 \"RESERVED CONTROLS AND\n\t\t * DEFAULT SETTINGS\".\n\t\t */\n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\treturn vmx_restore_control_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_MISC:\n\t\treturn vmx_restore_vmx_misc(vmx, data);\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\treturn vmx_restore_fixed0_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t/*\n\t\t * These MSRs are generated based on the vCPU's CPUID, so we\n\t\t * do not support restoring them directly.\n\t\t */\n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\treturn vmx_restore_vmx_ept_vpid_cap(vmx, data);\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\tvmx->nested.nested_vmx_vmcs_enum = data;\n\t\treturn 0;\n\tdefault:\n\t\t/*\n\t\t * The rest of the VMX capability MSRs do not support restore.\n\t\t */\n\t\treturn -EINVAL;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_set_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\treturn vmx_restore_vmx_basic(vmx, data);\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t/*\n\t\t * The \"non-true\" VMX capability MSRs are generated from the\n\t\t * \"true\" MSRs, so we do not support restoring them directly.\n\t\t *\n\t\t * If userspace wants to emulate VMX_BASIC[55]=0, userspace\n\t\t * should restore the \"true\" MSRs with the must-be-1 bits\n\t\t * set according to the SDM Vol 3. A.2 \"RESERVED CONTROLS AND\n\t\t * DEFAULT SETTINGS\".\n\t\t */\n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\treturn vmx_restore_control_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_MISC:\n\t\treturn vmx_restore_vmx_misc(vmx, data);\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\treturn vmx_restore_fixed0_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t/*\n\t\t * These MSRs are generated based on the vCPU's CPUID, so we\n\t\t * do not support restoring them directly.\n\t\t */\n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\treturn vmx_restore_vmx_ept_vpid_cap(vmx, data);\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\tvmx->nested.nested_vmx_vmcs_enum = data;\n\t\treturn 0;\n\tdefault:\n\t\t/*\n\t\t * The rest of the VMX capability MSRs do not support restore.\n\t\t */\n\t\treturn -EINVAL;\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_221": {
        "vulnerable_code": {
            "Code": "vmx_start_preemption_timer(struct kvm_vcpu *vcpu){\n\tu64 preemption_timeout = get_vmcs12(vcpu)->vmx_preemption_timer_value;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (vcpu->arch.virtual_tsc_khz == 0)\n\t\treturn;\n\n\t/* Make sure short timeouts reliably trigger an immediate vmexit.\n\t * hrtimer_start does not guarantee this. */\n\tif (preemption_timeout <= 1) {\n\t\tvmx_preemption_timer_fn(&vmx->nested.preemption_timer);\n\t\treturn;\n\t}\n\n\tpreemption_timeout <<= VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n\tpreemption_timeout *= 1000000;\n\tdo_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);\n\thrtimer_start(&vmx->nested.preemption_timer,\n\t\t      ns_to_ktime(preemption_timeout), HRTIMER_MODE_REL);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_start_preemption_timer(struct kvm_vcpu *vcpu){\n\tu64 preemption_timeout = get_vmcs12(vcpu)->vmx_preemption_timer_value;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (vcpu->arch.virtual_tsc_khz == 0)\n\t\treturn;\n\n\t/* Make sure short timeouts reliably trigger an immediate vmexit.\n\t * hrtimer_start does not guarantee this. */\n\tif (preemption_timeout <= 1) {\n\t\tvmx_preemption_timer_fn(&vmx->nested.preemption_timer);\n\t\treturn;\n\t}\n\n\tpreemption_timeout <<= VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n\tpreemption_timeout *= 1000000;\n\tdo_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);\n\thrtimer_start(&vmx->nested.preemption_timer,\n\t\t      ns_to_ktime(preemption_timeout), HRTIMER_MODE_REL);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_222": {
        "vulnerable_code": {
            "Code": "vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu){\n\tget_debugreg(vcpu->arch.db[0], 0);\n\tget_debugreg(vcpu->arch.db[1], 1);\n\tget_debugreg(vcpu->arch.db[2], 2);\n\tget_debugreg(vcpu->arch.db[3], 3);\n\tget_debugreg(vcpu->arch.dr6, 6);\n\tvcpu->arch.dr7 = vmcs_readl(GUEST_DR7);\n\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu){\n\tget_debugreg(vcpu->arch.db[0], 0);\n\tget_debugreg(vcpu->arch.db[1], 1);\n\tget_debugreg(vcpu->arch.db[2], 2);\n\tget_debugreg(vcpu->arch.db[3], 3);\n\tget_debugreg(vcpu->arch.dr6, 6);\n\tvcpu->arch.dr7 = vmcs_readl(GUEST_DR7);\n\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;\n\tvmcs_set_bits(CPU_BASED_VM_EXEC_CONTROL, CPU_BASED_MOV_DR_EXITING);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_223": {
        "vulnerable_code": {
            "Code": "vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint max_irr;\n\n\tWARN_ON(!vcpu->arch.apicv_active);\n\tif (pi_test_on(&vmx->pi_desc)) {\n\t\tpi_clear_on(&vmx->pi_desc);\n\t\t/*\n\t\t * IOMMU can write to PIR.ON, so the barrier matters even on UP.\n\t\t * But on x86 this is just a compiler barrier anyway.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tmax_irr = kvm_apic_update_irr(vcpu, vmx->pi_desc.pir);\n\t} else {\n\t\tmax_irr = kvm_lapic_find_highest_irr(vcpu);\n\t}\n\tvmx_hwapic_irr_update(vcpu, max_irr);\n\treturn max_irr;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu){\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint max_irr;\n\n\tWARN_ON(!vcpu->arch.apicv_active);\n\tif (pi_test_on(&vmx->pi_desc)) {\n\t\tpi_clear_on(&vmx->pi_desc);\n\t\t/*\n\t\t * IOMMU can write to PIR.ON, so the barrier matters even on UP.\n\t\t * But on x86 this is just a compiler barrier anyway.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tmax_irr = kvm_apic_update_irr(vcpu, vmx->pi_desc.pir);\n\t} else {\n\t\tmax_irr = kvm_lapic_find_highest_irr(vcpu);\n\t}\n\tvmx_hwapic_irr_update(vcpu, max_irr);\n\treturn max_irr;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_224": {
        "vulnerable_code": {
            "Code": "vmx_write_pml_buffer(struct kvm_vcpu *vcpu){\n\tstruct vmcs12 *vmcs12;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tgpa_t gpa;\n\tstruct page *page = NULL;\n\tu64 *pml_address;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tWARN_ON_ONCE(vmx->nested.pml_full);\n\n\t\t/*\n\t\t * Check if PML is enabled for the nested guest.\n\t\t * Whether eptp bit 6 is set is already checked\n\t\t * as part of A/D emulation.\n\t\t */\n\t\tvmcs12 = get_vmcs12(vcpu);\n\t\tif (!nested_cpu_has_pml(vmcs12))\n\t\t\treturn 0;\n\n\t\tif (vmcs12->guest_pml_index >= PML_ENTITY_NUM) {\n\t\t\tvmx->nested.pml_full = true;\n\t\t\treturn 1;\n\t\t}\n\n\t\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS) & ~0xFFFull;\n\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->pml_address);\n\t\tif (is_error_page(page))\n\t\t\treturn 0;\n\n\t\tpml_address = kmap(page);\n\t\tpml_address[vmcs12->guest_pml_index--] = gpa;\n\t\tkunmap(page);\n\t\tkvm_release_page_clean(page);\n\t}\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "vmx_write_pml_buffer(struct kvm_vcpu *vcpu){\n\tstruct vmcs12 *vmcs12;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tgpa_t gpa;\n\tstruct page *page = NULL;\n\tu64 *pml_address;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tWARN_ON_ONCE(vmx->nested.pml_full);\n\n\t\t/*\n\t\t * Check if PML is enabled for the nested guest.\n\t\t * Whether eptp bit 6 is set is already checked\n\t\t * as part of A/D emulation.\n\t\t */\n\t\tvmcs12 = get_vmcs12(vcpu);\n\t\tif (!nested_cpu_has_pml(vmcs12))\n\t\t\treturn 0;\n\n\t\tif (vmcs12->guest_pml_index >= PML_ENTITY_NUM) {\n\t\t\tvmx->nested.pml_full = true;\n\t\t\treturn 1;\n\t\t}\n\n\t\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS) & ~0xFFFull;\n\n\t\tpage = kvm_vcpu_gpa_to_page(vcpu, vmcs12->pml_address);\n\t\tif (is_error_page(page))\n\t\t\treturn 0;\n\n\t\tpml_address = kmap(page);\n\t\tpml_address[vmcs12->guest_pml_index--] = gpa;\n\t\tkunmap(page);\n\t\tkvm_release_page_clean(page);\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_225": {
        "vulnerable_code": {
            "Code": "bio_add_page(struct bio *bio, struct page *page,\n\t\t unsigned int len, unsigned int offset){\n\tstruct bio_vec *bv;\n\n\t/*\n\t * cloned bio must not modify vec list\n\t */\n\tif (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))\n\t\treturn 0;\n\n\t/*\n\t * For filesystems with a blocksize smaller than the pagesize\n\t * we will often be called with the same page as last time and\n\t * a consecutive offset.  Optimize this special case.\n\t */\n\tif (bio->bi_vcnt > 0) {\n\t\tbv = &bio->bi_io_vec[bio->bi_vcnt - 1];\n\n\t\tif (page == bv->bv_page &&\n\t\t    offset == bv->bv_offset + bv->bv_len) {\n\t\t\tbv->bv_len += len;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (bio->bi_vcnt >= bio->bi_max_vecs)\n\t\treturn 0;\n\n\tbv\t\t= &bio->bi_io_vec[bio->bi_vcnt];\n\tbv->bv_page\t= page;\n\tbv->bv_len\t= len;\n\tbv->bv_offset\t= offset;\n\n\tbio->bi_vcnt++;\ndone:\n\tbio->bi_iter.bi_size += len;\n\treturn len;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "bio_add_page(struct bio *bio, struct page *page,\n\t\t unsigned int len, unsigned int offset){\n\tstruct bio_vec *bv;\n\n\t/*\n\t * cloned bio must not modify vec list\n\t */\n\tif (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))\n\t\treturn 0;\n\n\t/*\n\t * For filesystems with a blocksize smaller than the pagesize\n\t * we will often be called with the same page as last time and\n\t * a consecutive offset.  Optimize this special case.\n\t */\n\tif (bio->bi_vcnt > 0) {\n\t\tbv = &bio->bi_io_vec[bio->bi_vcnt - 1];\n\n\t\tif (page == bv->bv_page &&\n\t\t    offset == bv->bv_offset + bv->bv_len) {\n\t\t\tbv->bv_len += len;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (bio->bi_vcnt >= bio->bi_max_vecs)\n\t\treturn 0;\n\n\tbv\t\t= &bio->bi_io_vec[bio->bi_vcnt];\n\tbv->bv_page\t= page;\n\tbv->bv_len\t= len;\n\tbv->bv_offset\t= offset;\n\n\tbio->bi_vcnt++;\ndone:\n\tbio->bi_iter.bi_size += len;\n\treturn len;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_226": {
        "vulnerable_code": {
            "Code": "bio_clone_bioset(struct bio *bio_src, gfp_t gfp_mask,\n\t\t\t     struct bio_set *bs){\n\tstruct bvec_iter iter;\n\tstruct bio_vec bv;\n\tstruct bio *bio;\n\n\t/*\n\t * Pre immutable biovecs, __bio_clone() used to just do a memcpy from\n\t * bio_src->bi_io_vec to bio->bi_io_vec.\n\t *\n\t * We can't do that anymore, because:\n\t *\n\t *  - The point of cloning the biovec is to produce a bio with a biovec\n\t *    the caller can modify: bi_idx and bi_bvec_done should be 0.\n\t *\n\t *  - The original bio could've had more than BIO_MAX_PAGES biovecs; if\n\t *    we tried to clone the whole thing bio_alloc_bioset() would fail.\n\t *    But the clone should succeed as long as the number of biovecs we\n\t *    actually need to allocate is fewer than BIO_MAX_PAGES.\n\t *\n\t *  - Lastly, bi_vcnt should not be looked at or relied upon by code\n\t *    that does not own the bio - reason being drivers don't use it for\n\t *    iterating over the biovec anymore, so expecting it to be kept up\n\t *    to date (i.e. for clones that share the parent biovec) is just\n\t *    asking for trouble and would force extra work on\n\t *    __bio_clone_fast() anyways.\n\t */\n\n\tbio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);\n\tif (!bio)\n\t\treturn NULL;\n\tbio->bi_disk\t\t= bio_src->bi_disk;\n\tbio->bi_opf\t\t= bio_src->bi_opf;\n\tbio->bi_write_hint\t= bio_src->bi_write_hint;\n\tbio->bi_iter.bi_sector\t= bio_src->bi_iter.bi_sector;\n\tbio->bi_iter.bi_size\t= bio_src->bi_iter.bi_size;\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tbreak;\n\tcase REQ_OP_WRITE_SAME:\n\t\tbio->bi_io_vec[bio->bi_vcnt++] = bio_src->bi_io_vec[0];\n\t\tbreak;\n\tdefault:\n\t\tbio_for_each_segment(bv, bio_src, iter)\n\t\t\tbio->bi_io_vec[bio->bi_vcnt++] = bv;\n\t\tbreak;\n\t}\n\n\tif (bio_integrity(bio_src)) {\n\t\tint ret;\n\n\t\tret = bio_integrity_clone(bio, bio_src, gfp_mask);\n\t\tif (ret < 0) {\n\t\t\tbio_put(bio);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tbio_clone_blkcg_association(bio, bio_src);\n\n\treturn bio;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "bio_clone_bioset(struct bio *bio_src, gfp_t gfp_mask,\n\t\t\t     struct bio_set *bs){\n\tstruct bvec_iter iter;\n\tstruct bio_vec bv;\n\tstruct bio *bio;\n\n\t/*\n\t * Pre immutable biovecs, __bio_clone() used to just do a memcpy from\n\t * bio_src->bi_io_vec to bio->bi_io_vec.\n\t *\n\t * We can't do that anymore, because:\n\t *\n\t *  - The point of cloning the biovec is to produce a bio with a biovec\n\t *    the caller can modify: bi_idx and bi_bvec_done should be 0.\n\t *\n\t *  - The original bio could've had more than BIO_MAX_PAGES biovecs; if\n\t *    we tried to clone the whole thing bio_alloc_bioset() would fail.\n\t *    But the clone should succeed as long as the number of biovecs we\n\t *    actually need to allocate is fewer than BIO_MAX_PAGES.\n\t *\n\t *  - Lastly, bi_vcnt should not be looked at or relied upon by code\n\t *    that does not own the bio - reason being drivers don't use it for\n\t *    iterating over the biovec anymore, so expecting it to be kept up\n\t *    to date (i.e. for clones that share the parent biovec) is just\n\t *    asking for trouble and would force extra work on\n\t *    __bio_clone_fast() anyways.\n\t */\n\n\tbio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);\n\tif (!bio)\n\t\treturn NULL;\n\tbio->bi_disk\t\t= bio_src->bi_disk;\n\tbio->bi_opf\t\t= bio_src->bi_opf;\n\tbio->bi_write_hint\t= bio_src->bi_write_hint;\n\tbio->bi_iter.bi_sector\t= bio_src->bi_iter.bi_sector;\n\tbio->bi_iter.bi_size\t= bio_src->bi_iter.bi_size;\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tbreak;\n\tcase REQ_OP_WRITE_SAME:\n\t\tbio->bi_io_vec[bio->bi_vcnt++] = bio_src->bi_io_vec[0];\n\t\tbreak;\n\tdefault:\n\t\tbio_for_each_segment(bv, bio_src, iter)\n\t\t\tbio->bi_io_vec[bio->bi_vcnt++] = bv;\n\t\tbreak;\n\t}\n\n\tif (bio_integrity(bio_src)) {\n\t\tint ret;\n\n\t\tret = bio_integrity_clone(bio, bio_src, gfp_mask);\n\t\tif (ret < 0) {\n\t\t\tbio_put(bio);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tbio_clone_blkcg_association(bio, bio_src);\n\n\treturn bio;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_227": {
        "vulnerable_code": {
            "Code": "bio_copy_data(struct bio *dst, struct bio *src){\n\tstruct bvec_iter src_iter, dst_iter;\n\tstruct bio_vec src_bv, dst_bv;\n\tvoid *src_p, *dst_p;\n\tunsigned bytes;\n\n\tsrc_iter = src->bi_iter;\n\tdst_iter = dst->bi_iter;\n\n\twhile (1) {\n\t\tif (!src_iter.bi_size) {\n\t\t\tsrc = src->bi_next;\n\t\t\tif (!src)\n\t\t\t\tbreak;\n\n\t\t\tsrc_iter = src->bi_iter;\n\t\t}\n\n\t\tif (!dst_iter.bi_size) {\n\t\t\tdst = dst->bi_next;\n\t\t\tif (!dst)\n\t\t\t\tbreak;\n\n\t\t\tdst_iter = dst->bi_iter;\n\t\t}\n\n\t\tsrc_bv = bio_iter_iovec(src, src_iter);\n\t\tdst_bv = bio_iter_iovec(dst, dst_iter);\n\n\t\tbytes = min(src_bv.bv_len, dst_bv.bv_len);\n\n\t\tsrc_p = kmap_atomic(src_bv.bv_page);\n\t\tdst_p = kmap_atomic(dst_bv.bv_page);\n\n\t\tmemcpy(dst_p + dst_bv.bv_offset,\n\t\t       src_p + src_bv.bv_offset,\n\t\t       bytes);\n\n\t\tkunmap_atomic(dst_p);\n\t\tkunmap_atomic(src_p);\n\n\t\tbio_advance_iter(src, &src_iter, bytes);\n\t\tbio_advance_iter(dst, &dst_iter, bytes);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "bio_copy_data(struct bio *dst, struct bio *src){\n\tstruct bvec_iter src_iter, dst_iter;\n\tstruct bio_vec src_bv, dst_bv;\n\tvoid *src_p, *dst_p;\n\tunsigned bytes;\n\n\tsrc_iter = src->bi_iter;\n\tdst_iter = dst->bi_iter;\n\n\twhile (1) {\n\t\tif (!src_iter.bi_size) {\n\t\t\tsrc = src->bi_next;\n\t\t\tif (!src)\n\t\t\t\tbreak;\n\n\t\t\tsrc_iter = src->bi_iter;\n\t\t}\n\n\t\tif (!dst_iter.bi_size) {\n\t\t\tdst = dst->bi_next;\n\t\t\tif (!dst)\n\t\t\t\tbreak;\n\n\t\t\tdst_iter = dst->bi_iter;\n\t\t}\n\n\t\tsrc_bv = bio_iter_iovec(src, src_iter);\n\t\tdst_bv = bio_iter_iovec(dst, dst_iter);\n\n\t\tbytes = min(src_bv.bv_len, dst_bv.bv_len);\n\n\t\tsrc_p = kmap_atomic(src_bv.bv_page);\n\t\tdst_p = kmap_atomic(dst_bv.bv_page);\n\n\t\tmemcpy(dst_p + dst_bv.bv_offset,\n\t\t       src_p + src_bv.bv_offset,\n\t\t       bytes);\n\n\t\tkunmap_atomic(dst_p);\n\t\tkunmap_atomic(src_p);\n\n\t\tbio_advance_iter(src, &src_iter, bytes);\n\t\tbio_advance_iter(dst, &dst_iter, bytes);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_228": {
        "vulnerable_code": {
            "Code": "bio_find_or_create_slab(unsigned int extra_size){\n\tunsigned int sz = sizeof(struct bio) + extra_size;\n\tstruct kmem_cache *slab = NULL;\n\tstruct bio_slab *bslab, *new_bio_slabs;\n\tunsigned int new_bio_slab_max;\n\tunsigned int i, entry = -1;\n\n\tmutex_lock(&bio_slab_lock);\n\n\ti = 0;\n\twhile (i < bio_slab_nr) {\n\t\tbslab = &bio_slabs[i];\n\n\t\tif (!bslab->slab && entry == -1)\n\t\t\tentry = i;\n\t\telse if (bslab->slab_size == sz) {\n\t\t\tslab = bslab->slab;\n\t\t\tbslab->slab_ref++;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\n\tif (slab)\n\t\tgoto out_unlock;\n\n\tif (bio_slab_nr == bio_slab_max && entry == -1) {\n\t\tnew_bio_slab_max = bio_slab_max << 1;\n\t\tnew_bio_slabs = krealloc(bio_slabs,\n\t\t\t\t\t new_bio_slab_max * sizeof(struct bio_slab),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!new_bio_slabs)\n\t\t\tgoto out_unlock;\n\t\tbio_slab_max = new_bio_slab_max;\n\t\tbio_slabs = new_bio_slabs;\n\t}\n\tif (entry == -1)\n\t\tentry = bio_slab_nr++;\n\n\tbslab = &bio_slabs[entry];\n\n\tsnprintf(bslab->name, sizeof(bslab->name), \"bio-%d\", entry);\n\tslab = kmem_cache_create(bslab->name, sz, ARCH_KMALLOC_MINALIGN,\n\t\t\t\t SLAB_HWCACHE_ALIGN, NULL);\n\tif (!slab)\n\t\tgoto out_unlock;\n\n\tbslab->slab = slab;\n\tbslab->slab_ref = 1;\n\tbslab->slab_size = sz;\nout_unlock:\n\tmutex_unlock(&bio_slab_lock);\n\treturn slab;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "bio_find_or_create_slab(unsigned int extra_size){\n\tunsigned int sz = sizeof(struct bio) + extra_size;\n\tstruct kmem_cache *slab = NULL;\n\tstruct bio_slab *bslab, *new_bio_slabs;\n\tunsigned int new_bio_slab_max;\n\tunsigned int i, entry = -1;\n\n\tmutex_lock(&bio_slab_lock);\n\n\ti = 0;\n\twhile (i < bio_slab_nr) {\n\t\tbslab = &bio_slabs[i];\n\n\t\tif (!bslab->slab && entry == -1)\n\t\t\tentry = i;\n\t\telse if (bslab->slab_size == sz) {\n\t\t\tslab = bslab->slab;\n\t\t\tbslab->slab_ref++;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\n\tif (slab)\n\t\tgoto out_unlock;\n\n\tif (bio_slab_nr == bio_slab_max && entry == -1) {\n\t\tnew_bio_slab_max = bio_slab_max << 1;\n\t\tnew_bio_slabs = krealloc(bio_slabs,\n\t\t\t\t\t new_bio_slab_max * sizeof(struct bio_slab),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!new_bio_slabs)\n\t\t\tgoto out_unlock;\n\t\tbio_slab_max = new_bio_slab_max;\n\t\tbio_slabs = new_bio_slabs;\n\t}\n\tif (entry == -1)\n\t\tentry = bio_slab_nr++;\n\n\tbslab = &bio_slabs[entry];\n\n\tsnprintf(bslab->name, sizeof(bslab->name), \"bio-%d\", entry);\n\tslab = kmem_cache_create(bslab->name, sz, ARCH_KMALLOC_MINALIGN,\n\t\t\t\t SLAB_HWCACHE_ALIGN, NULL);\n\tif (!slab)\n\t\tgoto out_unlock;\n\n\tbslab->slab = slab;\n\tbslab->slab_ref = 1;\n\tbslab->slab_size = sz;\nout_unlock:\n\tmutex_unlock(&bio_slab_lock);\n\treturn slab;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_229": {
        "vulnerable_code": {
            "Code": "bio_free_pages(struct bio *bio){\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i)\n\t\t__free_page(bvec->bv_page);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "bio_free_pages(struct bio *bio){\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_segment_all(bvec, bio, i)\n\t\t__free_page(bvec->bv_page);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_230": {
        "vulnerable_code": {
            "Code": "bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask){\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (ret < local_nr_pages) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tfor (j = 0; j < nr_pages; j++) {\n\t\tif (!pages[j])\n\t\t\tbreak;\n\t\tput_page(pages[j]);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "bio_map_user_iov(struct request_queue *q,\n\t\t\t     const struct iov_iter *iter,\n\t\t\t     gfp_t gfp_mask){\n\tint j;\n\tint nr_pages = 0;\n\tstruct page **pages;\n\tstruct bio *bio;\n\tint cur_page = 0;\n\tint ret, offset;\n\tstruct iov_iter i;\n\tstruct iovec iov;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\n\t\t/*\n\t\t * Overflow, abort\n\t\t */\n\t\tif (end < start)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tnr_pages += end - start;\n\t\t/*\n\t\t * buffer must be aligned to at least logical block size for now\n\t\t */\n\t\tif (uaddr & queue_dma_alignment(q))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_pages)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbio = bio_kmalloc(gfp_mask, nr_pages);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = -ENOMEM;\n\tpages = kcalloc(nr_pages, sizeof(struct page *), gfp_mask);\n\tif (!pages)\n\t\tgoto out;\n\n\tiov_for_each(iov, i, *iter) {\n\t\tunsigned long uaddr = (unsigned long) iov.iov_base;\n\t\tunsigned long len = iov.iov_len;\n\t\tunsigned long end = (uaddr + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\t\tunsigned long start = uaddr >> PAGE_SHIFT;\n\t\tconst int local_nr_pages = end - start;\n\t\tconst int page_limit = cur_page + local_nr_pages;\n\n\t\tret = get_user_pages_fast(uaddr, local_nr_pages,\n\t\t\t\t(iter->type & WRITE) != WRITE,\n\t\t\t\t&pages[cur_page]);\n\t\tif (ret < local_nr_pages) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\toffset = offset_in_page(uaddr);\n\t\tfor (j = cur_page; j < page_limit; j++) {\n\t\t\tunsigned int bytes = PAGE_SIZE - offset;\n\t\t\tunsigned short prev_bi_vcnt = bio->bi_vcnt;\n\n\t\t\tif (len <= 0)\n\t\t\t\tbreak;\n\t\t\t\n\t\t\tif (bytes > len)\n\t\t\t\tbytes = len;\n\n\t\t\t/*\n\t\t\t * sorry...\n\t\t\t */\n\t\t\tif (bio_add_pc_page(q, bio, pages[j], bytes, offset) <\n\t\t\t\t\t    bytes)\n\t\t\t\tbreak;\n\n\t\t\t/*\n\t\t\t * check if vector was merged with previous\n\t\t\t * drop page reference if needed\n\t\t\t */\n\t\t\tif (bio->bi_vcnt == prev_bi_vcnt)\n\t\t\t\tput_page(pages[j]);\n\n\t\t\tlen -= bytes;\n\t\t\toffset = 0;\n\t\t}\n\n\t\tcur_page = j;\n\t\t/*\n\t\t * release the pages we didn't map into the bio, if any\n\t\t */\n\t\twhile (j < page_limit)\n\t\t\tput_page(pages[j++]);\n\t}\n\n\tkfree(pages);\n\n\tbio_set_flag(bio, BIO_USER_MAPPED);\n\n\t/*\n\t * subtle -- if bio_map_user_iov() ended up bouncing a bio,\n\t * it would normally disappear when its bi_end_io is run.\n\t * however, we need it for the unmap, so grab an extra\n\t * reference to it\n\t */\n\tbio_get(bio);\n\treturn bio;\n\n out_unmap:\n\tfor (j = 0; j < nr_pages; j++) {\n\t\tif (!pages[j])\n\t\t\tbreak;\n\t\tput_page(pages[j]);\n\t}\n out:\n\tkfree(pages);\n\tbio_put(bio);\n\treturn ERR_PTR(ret);\n}",
            "Size": 3,
            "Code Complexity": 0,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_231": {
        "vulnerable_code": {
            "Code": "bio_reset(struct bio *bio){\n\tunsigned long flags = bio->bi_flags & (~0UL << BIO_RESET_BITS);\n\n\tbio_uninit(bio);\n\n\tmemset(bio, 0, BIO_RESET_BYTES);\n\tbio->bi_flags = flags;\n\tatomic_set(&bio->__bi_remaining, 1);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "bio_reset(struct bio *bio){\n\tunsigned long flags = bio->bi_flags & (~0UL << BIO_RESET_BITS);\n\n\tbio_uninit(bio);\n\n\tmemset(bio, 0, BIO_RESET_BYTES);\n\tbio->bi_flags = flags;\n\tatomic_set(&bio->__bi_remaining, 1);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_232": {
        "vulnerable_code": {
            "Code": "bioset_create(unsigned int pool_size,\n\t\t\t      unsigned int front_pad,\n\t\t\t      int flags){\n\tunsigned int back_pad = BIO_INLINE_VECS * sizeof(struct bio_vec);\n\tstruct bio_set *bs;\n\n\tbs = kzalloc(sizeof(*bs), GFP_KERNEL);\n\tif (!bs)\n\t\treturn NULL;\n\n\tbs->front_pad = front_pad;\n\n\tspin_lock_init(&bs->rescue_lock);\n\tbio_list_init(&bs->rescue_list);\n\tINIT_WORK(&bs->rescue_work, bio_alloc_rescue);\n\n\tbs->bio_slab = bio_find_or_create_slab(front_pad + back_pad);\n\tif (!bs->bio_slab) {\n\t\tkfree(bs);\n\t\treturn NULL;\n\t}\n\n\tbs->bio_pool = mempool_create_slab_pool(pool_size, bs->bio_slab);\n\tif (!bs->bio_pool)\n\t\tgoto bad;\n\n\tif (flags & BIOSET_NEED_BVECS) {\n\t\tbs->bvec_pool = biovec_create_pool(pool_size);\n\t\tif (!bs->bvec_pool)\n\t\t\tgoto bad;\n\t}\n\n\tif (!(flags & BIOSET_NEED_RESCUER))\n\t\treturn bs;\n\n\tbs->rescue_workqueue = alloc_workqueue(\"bioset\", WQ_MEM_RECLAIM, 0);\n\tif (!bs->rescue_workqueue)\n\t\tgoto bad;\n\n\treturn bs;\nbad:\n\tbioset_free(bs);\n\treturn NULL;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "bioset_create(unsigned int pool_size,\n\t\t\t      unsigned int front_pad,\n\t\t\t      int flags){\n\tunsigned int back_pad = BIO_INLINE_VECS * sizeof(struct bio_vec);\n\tstruct bio_set *bs;\n\n\tbs = kzalloc(sizeof(*bs), GFP_KERNEL);\n\tif (!bs)\n\t\treturn NULL;\n\n\tbs->front_pad = front_pad;\n\n\tspin_lock_init(&bs->rescue_lock);\n\tbio_list_init(&bs->rescue_list);\n\tINIT_WORK(&bs->rescue_work, bio_alloc_rescue);\n\n\tbs->bio_slab = bio_find_or_create_slab(front_pad + back_pad);\n\tif (!bs->bio_slab) {\n\t\tkfree(bs);\n\t\treturn NULL;\n\t}\n\n\tbs->bio_pool = mempool_create_slab_pool(pool_size, bs->bio_slab);\n\tif (!bs->bio_pool)\n\t\tgoto bad;\n\n\tif (flags & BIOSET_NEED_BVECS) {\n\t\tbs->bvec_pool = biovec_create_pool(pool_size);\n\t\tif (!bs->bvec_pool)\n\t\t\tgoto bad;\n\t}\n\n\tif (!(flags & BIOSET_NEED_RESCUER))\n\t\treturn bs;\n\n\tbs->rescue_workqueue = alloc_workqueue(\"bioset\", WQ_MEM_RECLAIM, 0);\n\tif (!bs->rescue_workqueue)\n\t\tgoto bad;\n\n\treturn bs;\nbad:\n\tbioset_free(bs);\n\treturn NULL;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_233": {
        "vulnerable_code": {
            "Code": "__mark_reg_unbounded(struct bpf_reg_state *reg){\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}",
            "Size": 4,
            "Code Complexity": 1,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__mark_reg_unbounded(struct bpf_reg_state *reg){\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_234": {
        "vulnerable_code": {
            "Code": "__reg_bound_offset(struct bpf_reg_state *reg){\n\treg->var_off = tnum_intersect(reg->var_off,\n\t\t\t\t      tnum_range(reg->umin_value,\n\t\t\t\t\t\t reg->umax_value));\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__reg_bound_offset(struct bpf_reg_state *reg){\n\treg->var_off = tnum_intersect(reg->var_off,\n\t\t\t\t      tnum_range(reg->umin_value,\n\t\t\t\t\t\t reg->umax_value));\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_235": {
        "vulnerable_code": {
            "Code": "adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn){\n\tstruct bpf_reg_state *regs = cur_regs(env), *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\tint rc;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar.\n\t\t\t\t */\n\t\t\t\tif (!env->allow_ptr_leaks) {\n\t\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\treturn 0;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t     src_reg, dst_reg);\n\t\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t\t/* scalar += unknown scalar */\n\t\t\t\t\t__mark_reg_unknown(&off_reg);\n\t\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\t\tenv, insn,\n\t\t\t\t\t\t\tdst_reg, off_reg);\n\t\t\t\t}\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t     dst_reg, src_reg);\n\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t/* unknown scalar += scalar */\n\t\t\t\t__mark_reg_unknown(dst_reg);\n\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\tenv, insn, dst_reg, *src_reg);\n\t\t\t}\n\t\t\treturn rc;\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) { /* pointer += K */\n\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t     ptr_reg, src_reg);\n\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t/* unknown scalar += K */\n\t\t\t\t__mark_reg_unknown(dst_reg);\n\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\tenv, insn, dst_reg, off_reg);\n\t\t\t}\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn){\n\tstruct bpf_reg_state *regs = cur_regs(env), *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\tint rc;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t/* Combining two pointers by any ALU op yields\n\t\t\t\t * an arbitrary scalar.\n\t\t\t\t */\n\t\t\t\tif (!env->allow_ptr_leaks) {\n\t\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\treturn 0;\n\t\t\t} else {\n\t\t\t\t/* scalar += pointer\n\t\t\t\t * This is legal, but we have to reverse our\n\t\t\t\t * src/dest handling in computing the range\n\t\t\t\t */\n\t\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t     src_reg, dst_reg);\n\t\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t\t/* scalar += unknown scalar */\n\t\t\t\t\t__mark_reg_unknown(&off_reg);\n\t\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\t\tenv, insn,\n\t\t\t\t\t\t\tdst_reg, off_reg);\n\t\t\t\t}\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t/* pointer += scalar */\n\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t     dst_reg, src_reg);\n\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t/* unknown scalar += scalar */\n\t\t\t\t__mark_reg_unknown(dst_reg);\n\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\tenv, insn, dst_reg, *src_reg);\n\t\t\t}\n\t\t\treturn rc;\n\t\t}\n\t} else {\n\t\t/* Pretend the src is a reg with a known value, since we only\n\t\t * need to be able to read from this state.\n\t\t */\n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg) { /* pointer += K */\n\t\t\trc = adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t     ptr_reg, src_reg);\n\t\t\tif (rc == -EACCES && env->allow_ptr_leaks) {\n\t\t\t\t/* unknown scalar += K */\n\t\t\t\t__mark_reg_unknown(dst_reg);\n\t\t\t\treturn adjust_scalar_min_max_vals(\n\t\t\t\t\t\tenv, insn, dst_reg, off_reg);\n\t\t\t}\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t/* Got here implies adding two SCALAR_VALUEs */\n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, env->cur_state);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}",
            "Size": 0,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_236": {
        "vulnerable_code": {
            "Code": "check_ids(u32 old_id, u32 cur_id, struct idpair *idmap){\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "check_ids(u32 old_id, u32 cur_id, struct idpair *idmap){\n\tunsigned int i;\n\n\tfor (i = 0; i < ID_MAP_SIZE; i++) {\n\t\tif (!idmap[i].old) {\n\t\t\t/* Reached an empty slot; haven't seen this id before */\n\t\t\tidmap[i].old = old_id;\n\t\t\tidmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (idmap[i].old == old_id)\n\t\t\treturn idmap[i].cur == cur_id;\n\t}\n\t/* We ran out of idmap slots, which should be impossible */\n\tWARN_ON_ONCE(1);\n\treturn false;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_237": {
        "vulnerable_code": {
            "Code": "check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog){\n\t/* Make sure that BPF_PROG_TYPE_PERF_EVENT programs only use\n\t * preallocated hash maps, since doing memory allocation\n\t * in overflow_handler can crash depending on where nmi got\n\t * triggered.\n\t */\n\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT) {\n\t\tif (!check_map_prealloc(map)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (map->inner_map_meta &&\n\t\t    !check_map_prealloc(map->inner_map_meta)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated inner hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog){\n\t/* Make sure that BPF_PROG_TYPE_PERF_EVENT programs only use\n\t * preallocated hash maps, since doing memory allocation\n\t * in overflow_handler can crash depending on where nmi got\n\t * triggered.\n\t */\n\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT) {\n\t\tif (!check_map_prealloc(map)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (map->inner_map_meta &&\n\t\t    !check_map_prealloc(map->inner_map_meta)) {\n\t\t\tverbose(env, \"perf_event programs can only use preallocated inner hash map\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_238": {
        "vulnerable_code": {
            "Code": "check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed){\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t/* We may have added a variable offset to the packet pointer; but any\n\t * reg->range we have comes after that.  We are only checking the fixed\n\t * offset.\n\t */\n\n\t/* We don't allow negative numbers, because we aren't tracking enough\n\t * detail to prove they're safe.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_packet_access(env, regno, off, size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed){\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t/* We may have added a variable offset to the packet pointer; but any\n\t * reg->range we have comes after that.  We are only checking the fixed\n\t * offset.\n\t */\n\n\t/* We don't allow negative numbers, because we aren't tracking enough\n\t * detail to prove they're safe.\n\t */\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_packet_access(env, regno, off, size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_239": {
        "vulnerable_code": {
            "Code": "clear_all_pkt_pointers(struct bpf_verifier_env *env){\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (reg_is_pkt_pointer_any(&regs[i]))\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\treg = &state->stack[i].spilled_ptr;\n\t\tif (reg_is_pkt_pointer_any(reg))\n\t\t\t__mark_reg_unknown(reg);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "clear_all_pkt_pointers(struct bpf_verifier_env *env){\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (reg_is_pkt_pointer_any(&regs[i]))\n\t\t\tmark_reg_unknown(env, regs, i);\n\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].slot_type[0] != STACK_SPILL)\n\t\t\tcontinue;\n\t\treg = &state->stack[i].spilled_ptr;\n\t\tif (reg_is_pkt_pointer_any(reg))\n\t\t\t__mark_reg_unknown(reg);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_240": {
        "vulnerable_code": {
            "Code": "convert_ctx_accesses(struct bpf_verifier_env *env){\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\tu32 target_size;\n\n\tif (ops->gen_prologue) {\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (!ops->convert_ctx_access)\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_READ;\n\t\telse if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_WRITE;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (env->insn_aux_data[i + delta].ptr_type != PTR_TO_CTX)\n\t\t\tcontinue;\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\n\t\t/* If the read access is a narrower load of the field,\n\t\t * convert to a 4/8-byte load, to minimum program type specific\n\t\t * convert_ctx_access changes. If conversion is successful,\n\t\t * we will apply proper mask to the result.\n\t\t */\n\t\tis_narrower_load = size < ctx_field_size;\n\t\tif (is_narrower_load) {\n\t\t\tu32 off = insn->off;\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(ctx_field_size - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = ops->convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t      &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tif (ctx_field_size <= 4)\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\telse\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t}\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t/* keep walking new program and skip insns we just inserted */\n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "convert_ctx_accesses(struct bpf_verifier_env *env){\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\tu32 target_size;\n\n\tif (ops->gen_prologue) {\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (!ops->convert_ctx_access)\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_READ;\n\t\telse if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t insn->code == (BPF_STX | BPF_MEM | BPF_DW))\n\t\t\ttype = BPF_WRITE;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (env->insn_aux_data[i + delta].ptr_type != PTR_TO_CTX)\n\t\t\tcontinue;\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\n\t\t/* If the read access is a narrower load of the field,\n\t\t * convert to a 4/8-byte load, to minimum program type specific\n\t\t * convert_ctx_access changes. If conversion is successful,\n\t\t * we will apply proper mask to the result.\n\t\t */\n\t\tis_narrower_load = size < ctx_field_size;\n\t\tif (is_narrower_load) {\n\t\t\tu32 off = insn->off;\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(ctx_field_size - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = ops->convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t      &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tif (ctx_field_size <= 4)\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\telse\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t}\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t/* keep walking new program and skip insns we just inserted */\n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_241": {
        "vulnerable_code": {
            "Code": "free_states(struct bpf_verifier_env *env){\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < env->prog->len; i++) {\n\t\tsl = env->explored_states[i];\n\n\t\tif (sl)\n\t\t\twhile (sl != STATE_LIST_MARK) {\n\t\t\t\tsln = sl->next;\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tsl = sln;\n\t\t\t}\n\t}\n\n\tkfree(env->explored_states);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "free_states(struct bpf_verifier_env *env){\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < env->prog->len; i++) {\n\t\tsl = env->explored_states[i];\n\n\t\tif (sl)\n\t\t\twhile (sl != STATE_LIST_MARK) {\n\t\t\t\tsln = sl->next;\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tsl = sln;\n\t\t\t}\n\t}\n\n\tkfree(env->explored_states);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_242": {
        "vulnerable_code": {
            "Code": "free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self){\n\tkfree(state->stack);\n\tif (free_self)\n\t\tkfree(state);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self){\n\tkfree(state->stack);\n\tif (free_self)\n\t\tkfree(state);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_243": {
        "vulnerable_code": {
            "Code": "is_pointer_value(struct bpf_verifier_env *env, int regno){\n\treturn __is_pointer_value(env->allow_ptr_leaks, cur_regs(env) + regno);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "is_pointer_value(struct bpf_verifier_env *env, int regno){\n\treturn __is_pointer_value(env->allow_ptr_leaks, cur_regs(env) + regno);\n}",
            "Size": 4,
            "Code Complexity": 0,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_244": {
        "vulnerable_code": {
            "Code": "mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno){\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno){\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t/* Something bad happened, let's kill all regs */\n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_245": {
        "vulnerable_code": {
            "Code": "signed_add_overflows(s64 a, s64 b){\n\t/* Do the add in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "signed_add_overflows(s64 a, s64 b){\n\t/* Do the add in u64, where overflow is well-defined */\n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_246": {
        "vulnerable_code": {
            "Code": "states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur){\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur){\n\tstruct idpair *idmap;\n\tbool ret = false;\n\tint i;\n\n\tidmap = kcalloc(ID_MAP_SIZE, sizeof(struct idpair), GFP_KERNEL);\n\t/* If we failed to allocate the idmap, just say it's not safe */\n\tif (!idmap)\n\t\treturn false;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tif (!regsafe(&old->regs[i], &cur->regs[i], idmap))\n\t\t\tgoto out_free;\n\t}\n\n\tif (!stacksafe(old, cur, idmap))\n\t\tgoto out_free;\n\tret = true;\nout_free:\n\tkfree(idmap);\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_247": {
        "vulnerable_code": {
            "Code": "add_match_busid(char *busid){\n\tint i;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\t/* already registered? */\n\tif (get_busid_idx(busid) >= 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (!busid_table[i].name[0]) {\n\t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n\t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n\t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n\t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "add_match_busid(char *busid){\n\tint i;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\t/* already registered? */\n\tif (get_busid_idx(busid) >= 0) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (!busid_table[i].name[0]) {\n\t\t\tstrlcpy(busid_table[i].name, busid, BUSID_SIZE);\n\t\t\tif ((busid_table[i].status != STUB_BUSID_ALLOC) &&\n\t\t\t    (busid_table[i].status != STUB_BUSID_REMOV))\n\t\t\t\tbusid_table[i].status = STUB_BUSID_ADDED;\n\t\t\tret = 0;\n\t\t\tspin_unlock(&busid_table[i].busid_lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_248": {
        "vulnerable_code": {
            "Code": "del_match_busid(char *busid){\n\tint idx;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx < 0)\n\t\tgoto out;\n\n\t/* found */\n\tret = 0;\n\n\tif (busid_table[idx].status == STUB_BUSID_OTHER)\n\t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n\n\tif ((busid_table[idx].status != STUB_BUSID_OTHER) &&\n\t    (busid_table[idx].status != STUB_BUSID_ADDED))\n\t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "del_match_busid(char *busid){\n\tint idx;\n\tint ret = -1;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx < 0)\n\t\tgoto out;\n\n\t/* found */\n\tret = 0;\n\n\tspin_lock(&busid_table[idx].busid_lock);\n\n\tif (busid_table[idx].status == STUB_BUSID_OTHER)\n\t\tmemset(busid_table[idx].name, 0, BUSID_SIZE);\n\n\tif ((busid_table[idx].status != STUB_BUSID_OTHER) &&\n\t    (busid_table[idx].status != STUB_BUSID_ADDED))\n\t\tbusid_table[idx].status = STUB_BUSID_REMOV;\n\n\tspin_unlock(&busid_table[idx].busid_lock);\nout:\n\tspin_unlock(&busid_table_lock);\n\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_249": {
        "vulnerable_code": {
            "Code": "get_busid_idx(const char *busid){\n\tint i;\n\tint idx = -1;\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (busid_table[i].name[0])\n\t\t\tif (!strncmp(busid_table[i].name, busid, BUSID_SIZE)) {\n\t\t\t\tidx = i;\n\t\t\t\tbreak;\n\t\t\t}\n\treturn idx;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_busid_idx(const char *busid){\n\tint i;\n\tint idx = -1;\n\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (busid_table[i].name[0])\n\t\t\tif (!strncmp(busid_table[i].name, busid, BUSID_SIZE)) {\n\t\t\t\tidx = i;\n\t\t\t\tspin_unlock(&busid_table[i].busid_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\treturn idx;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_250": {
        "vulnerable_code": {
            "Code": "get_busid_priv(const char *busid){\n\tint idx;\n\tstruct bus_id_priv *bid = NULL;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx >= 0)\n\t\tbid = &(busid_table[idx]);\n\tspin_unlock(&busid_table_lock);\n\n\treturn bid;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_busid_priv(const char *busid){\n\tint idx;\n\tstruct bus_id_priv *bid = NULL;\n\n\tspin_lock(&busid_table_lock);\n\tidx = get_busid_idx(busid);\n\tif (idx >= 0) {\n\t\tbid = &(busid_table[idx]);\n\t\t/* get busid_lock before returning */\n\t\tspin_lock(&bid->busid_lock);\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\treturn bid;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_251": {
        "vulnerable_code": {
            "Code": "init_busid_table(void){\n\t/*\n\t * This also sets the bus_table[i].status to\n\t * STUB_BUSID_OTHER, which is 0.\n\t */\n\tmemset(busid_table, 0, sizeof(busid_table));\n\n\tspin_lock_init(&busid_table_lock);\n}",
            "Size": 2,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "init_busid_table(void){\n\tint i;\n\n\t/*\n\t * This also sets the bus_table[i].status to\n\t * STUB_BUSID_OTHER, which is 0.\n\t */\n\tmemset(busid_table, 0, sizeof(busid_table));\n\n\tspin_lock_init(&busid_table_lock);\n\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tspin_lock_init(&busid_table[i].busid_lock);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_252": {
        "vulnerable_code": {
            "Code": "match_busid_show(struct device_driver *drv, char *buf){\n\tint i;\n\tchar *out = buf;\n\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++)\n\t\tif (busid_table[i].name[0])\n\t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n\tspin_unlock(&busid_table_lock);\n\tout += sprintf(out, \"\\n\");\n\n\treturn out - buf;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "match_busid_show(struct device_driver *drv, char *buf){\n\tint i;\n\tchar *out = buf;\n\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tspin_lock(&busid_table[i].busid_lock);\n\t\tif (busid_table[i].name[0])\n\t\t\tout += sprintf(out, \"%s \", busid_table[i].name);\n\t\tspin_unlock(&busid_table[i].busid_lock);\n\t}\n\tspin_unlock(&busid_table_lock);\n\tout += sprintf(out, \"\\n\");\n\n\treturn out - buf;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_253": {
        "vulnerable_code": {
            "Code": "match_busid_store(struct device_driver *dev, const char *buf,\n\t\t\t\t size_t count){\n\tint len;\n\tchar busid[BUSID_SIZE];\n\n\tif (count < 5)\n\t\treturn -EINVAL;\n\n\t/* busid needs to include \\0 termination */\n\tlen = strlcpy(busid, buf + 4, BUSID_SIZE);\n\tif (sizeof(busid) <= len)\n\t\treturn -EINVAL;\n\n\tif (!strncmp(buf, \"add \", 4)) {\n\t\tif (add_match_busid(busid) < 0)\n\t\t\treturn -ENOMEM;\n\n\t\tpr_debug(\"add busid %s\\n\", busid);\n\t\treturn count;\n\t}\n\n\tif (!strncmp(buf, \"del \", 4)) {\n\t\tif (del_match_busid(busid) < 0)\n\t\t\treturn -ENODEV;\n\n\t\tpr_debug(\"del busid %s\\n\", busid);\n\t\treturn count;\n\t}\n\n\treturn -EINVAL;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "match_busid_store(struct device_driver *dev, const char *buf,\n\t\t\t\t size_t count){\n\tint len;\n\tchar busid[BUSID_SIZE];\n\n\tif (count < 5)\n\t\treturn -EINVAL;\n\n\t/* busid needs to include \\0 termination */\n\tlen = strlcpy(busid, buf + 4, BUSID_SIZE);\n\tif (sizeof(busid) <= len)\n\t\treturn -EINVAL;\n\n\tif (!strncmp(buf, \"add \", 4)) {\n\t\tif (add_match_busid(busid) < 0)\n\t\t\treturn -ENOMEM;\n\n\t\tpr_debug(\"add busid %s\\n\", busid);\n\t\treturn count;\n\t}\n\n\tif (!strncmp(buf, \"del \", 4)) {\n\t\tif (del_match_busid(busid) < 0)\n\t\t\treturn -ENODEV;\n\n\t\tpr_debug(\"del busid %s\\n\", busid);\n\t\treturn count;\n\t}\n\n\treturn -EINVAL;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_254": {
        "vulnerable_code": {
            "Code": "rebind_store(struct device_driver *dev, const char *buf,\n\t\t\t\t size_t count){\n\tint ret;\n\tint len;\n\tstruct bus_id_priv *bid;\n\n\t/* buf length should be less that BUSID_SIZE */\n\tlen = strnlen(buf, BUSID_SIZE);\n\n\tif (!(len < BUSID_SIZE))\n\t\treturn -EINVAL;\n\n\tbid = get_busid_priv(buf);\n\tif (!bid)\n\t\treturn -ENODEV;\n\n\t/* mark the device for deletion so probe ignores it during rescan */\n\tbid->status = STUB_BUSID_OTHER;\n\n\tret = do_rebind((char *) buf, bid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* delete device from busid_table */\n\tdel_match_busid((char *) buf);\n\n\treturn count;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "rebind_store(struct device_driver *dev, const char *buf,\n\t\t\t\t size_t count){\n\tint ret;\n\tint len;\n\tstruct bus_id_priv *bid;\n\n\t/* buf length should be less that BUSID_SIZE */\n\tlen = strnlen(buf, BUSID_SIZE);\n\n\tif (!(len < BUSID_SIZE))\n\t\treturn -EINVAL;\n\n\tbid = get_busid_priv(buf);\n\tif (!bid)\n\t\treturn -ENODEV;\n\n\t/* mark the device for deletion so probe ignores it during rescan */\n\tbid->status = STUB_BUSID_OTHER;\n\t/* release the busid lock */\n\tput_busid_priv(bid);\n\n\tret = do_rebind((char *) buf, bid);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* delete device from busid_table */\n\tdel_match_busid((char *) buf);\n\n\treturn count;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_255": {
        "vulnerable_code": {
            "Code": "stub_device_rebind(void){\n#if IS_MODULE(CONFIG_USBIP_HOST)\n\tstruct bus_id_priv *busid_priv;\n\tint i;\n\n\t/* update status to STUB_BUSID_OTHER so probe ignores the device */\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tbusid_priv->status = STUB_BUSID_OTHER;\n\t\t}\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\t/* now run rebind */\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tdo_rebind(busid_table[i].name, busid_priv);\n\t\t}\n\t}\n#endif\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "stub_device_rebind(void){\n#if IS_MODULE(CONFIG_USBIP_HOST)\n\tstruct bus_id_priv *busid_priv;\n\tint i;\n\n\t/* update status to STUB_BUSID_OTHER so probe ignores the device */\n\tspin_lock(&busid_table_lock);\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tbusid_priv->status = STUB_BUSID_OTHER;\n\t\t}\n\t}\n\tspin_unlock(&busid_table_lock);\n\n\t/* now run rebind - no need to hold locks. driver files are removed */\n\tfor (i = 0; i < MAX_BUSID; i++) {\n\t\tif (busid_table[i].name[0] &&\n\t\t    busid_table[i].shutdown_busid) {\n\t\t\tbusid_priv = &(busid_table[i]);\n\t\t\tdo_rebind(busid_table[i].name, busid_priv);\n\t\t}\n\t}\n#endif\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_256": {
        "vulnerable_code": {
            "Code": "ext4_expand_extra_isize_ea(struct inode *inode, int new_extra_isize,\n\t\t\t       struct ext4_inode *raw_inode, handle_t *handle){\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct buffer_head *bh;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstatic unsigned int mnt_count;\n\tsize_t min_offs;\n\tsize_t ifree, bfree;\n\tint total_ino;\n\tvoid *base, *end;\n\tint error = 0, tried_min_extra_isize = 0;\n\tint s_min_extra_isize = le16_to_cpu(sbi->s_es->s_min_extra_isize);\n\tint isize_diff;\t/* How much do we need to grow i_extra_isize */\n\nretry:\n\tisize_diff = new_extra_isize - EXT4_I(inode)->i_extra_isize;\n\tif (EXT4_I(inode)->i_extra_isize >= new_extra_isize)\n\t\treturn 0;\n\n\theader = IHDR(inode, raw_inode);\n\n\t/*\n\t * Check if enough free space is available in the inode to shift the\n\t * entries ahead by new_extra_isize.\n\t */\n\n\tbase = IFIRST(header);\n\tend = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\tmin_offs = end - base;\n\ttotal_ino = sizeof(struct ext4_xattr_ibody_header);\n\n\terror = xattr_check_inode(inode, header, end);\n\tif (error)\n\t\tgoto cleanup;\n\n\tifree = ext4_xattr_free_space(base, &min_offs, base, &total_ino);\n\tif (ifree >= isize_diff)\n\t\tgoto shift;\n\n\t/*\n\t * Enough free space isn't available in the inode, check if\n\t * EA block can hold new_extra_isize bytes.\n\t */\n\tif (EXT4_I(inode)->i_file_acl) {\n\t\tbh = sb_bread(inode->i_sb, EXT4_I(inode)->i_file_acl);\n\t\terror = -EIO;\n\t\tif (!bh)\n\t\t\tgoto cleanup;\n\t\terror = ext4_xattr_check_block(inode, bh);\n\t\tif (error)\n\t\t\tgoto cleanup;\n\t\tbase = BHDR(bh);\n\t\tend = bh->b_data + bh->b_size;\n\t\tmin_offs = end - base;\n\t\tbfree = ext4_xattr_free_space(BFIRST(bh), &min_offs, base,\n\t\t\t\t\t      NULL);\n\t\tbrelse(bh);\n\t\tif (bfree + ifree < isize_diff) {\n\t\t\tif (!tried_min_extra_isize && s_min_extra_isize) {\n\t\t\t\ttried_min_extra_isize++;\n\t\t\t\tnew_extra_isize = s_min_extra_isize;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\terror = -ENOSPC;\n\t\t\tgoto cleanup;\n\t\t}\n\t} else {\n\t\tbfree = inode->i_sb->s_blocksize;\n\t}\n\n\terror = ext4_xattr_make_inode_space(handle, inode, raw_inode,\n\t\t\t\t\t    isize_diff, ifree, bfree,\n\t\t\t\t\t    &total_ino);\n\tif (error) {\n\t\tif (error == -ENOSPC && !tried_min_extra_isize &&\n\t\t    s_min_extra_isize) {\n\t\t\ttried_min_extra_isize++;\n\t\t\tnew_extra_isize = s_min_extra_isize;\n\t\t\tgoto retry;\n\t\t}\n\t\tgoto cleanup;\n\t}\nshift:\n\t/* Adjust the offsets and shift the remaining entries ahead */\n\text4_xattr_shift_entries(IFIRST(header), EXT4_I(inode)->i_extra_isize\n\t\t\t- new_extra_isize, (void *)raw_inode +\n\t\t\tEXT4_GOOD_OLD_INODE_SIZE + new_extra_isize,\n\t\t\t(void *)header, total_ino);\n\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\ncleanup:\n\tif (error && (mnt_count != le16_to_cpu(sbi->s_es->s_mnt_count))) {\n\t\text4_warning(inode->i_sb, \"Unable to expand inode %lu. Delete some EAs or run e2fsck.\",\n\t\t\t     inode->i_ino);\n\t\tmnt_count = le16_to_cpu(sbi->s_es->s_mnt_count);\n\t}\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_expand_extra_isize_ea(struct inode *inode, int new_extra_isize,\n\t\t\t       struct ext4_inode *raw_inode, handle_t *handle){\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct buffer_head *bh;\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\tstatic unsigned int mnt_count;\n\tsize_t min_offs;\n\tsize_t ifree, bfree;\n\tint total_ino;\n\tvoid *base, *end;\n\tint error = 0, tried_min_extra_isize = 0;\n\tint s_min_extra_isize = le16_to_cpu(sbi->s_es->s_min_extra_isize);\n\tint isize_diff;\t/* How much do we need to grow i_extra_isize */\n\nretry:\n\tisize_diff = new_extra_isize - EXT4_I(inode)->i_extra_isize;\n\tif (EXT4_I(inode)->i_extra_isize >= new_extra_isize)\n\t\treturn 0;\n\n\theader = IHDR(inode, raw_inode);\n\n\t/*\n\t * Check if enough free space is available in the inode to shift the\n\t * entries ahead by new_extra_isize.\n\t */\n\n\tbase = IFIRST(header);\n\tend = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\tmin_offs = end - base;\n\ttotal_ino = sizeof(struct ext4_xattr_ibody_header);\n\n\terror = xattr_check_inode(inode, header, end);\n\tif (error)\n\t\tgoto cleanup;\n\n\tifree = ext4_xattr_free_space(base, &min_offs, base, &total_ino);\n\tif (ifree >= isize_diff)\n\t\tgoto shift;\n\n\t/*\n\t * Enough free space isn't available in the inode, check if\n\t * EA block can hold new_extra_isize bytes.\n\t */\n\tif (EXT4_I(inode)->i_file_acl) {\n\t\tbh = sb_bread(inode->i_sb, EXT4_I(inode)->i_file_acl);\n\t\terror = -EIO;\n\t\tif (!bh)\n\t\t\tgoto cleanup;\n\t\terror = ext4_xattr_check_block(inode, bh);\n\t\tif (error)\n\t\t\tgoto cleanup;\n\t\tbase = BHDR(bh);\n\t\tend = bh->b_data + bh->b_size;\n\t\tmin_offs = end - base;\n\t\tbfree = ext4_xattr_free_space(BFIRST(bh), &min_offs, base,\n\t\t\t\t\t      NULL);\n\t\tbrelse(bh);\n\t\tif (bfree + ifree < isize_diff) {\n\t\t\tif (!tried_min_extra_isize && s_min_extra_isize) {\n\t\t\t\ttried_min_extra_isize++;\n\t\t\t\tnew_extra_isize = s_min_extra_isize;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\terror = -ENOSPC;\n\t\t\tgoto cleanup;\n\t\t}\n\t} else {\n\t\tbfree = inode->i_sb->s_blocksize;\n\t}\n\n\terror = ext4_xattr_make_inode_space(handle, inode, raw_inode,\n\t\t\t\t\t    isize_diff, ifree, bfree,\n\t\t\t\t\t    &total_ino);\n\tif (error) {\n\t\tif (error == -ENOSPC && !tried_min_extra_isize &&\n\t\t    s_min_extra_isize) {\n\t\t\ttried_min_extra_isize++;\n\t\t\tnew_extra_isize = s_min_extra_isize;\n\t\t\tgoto retry;\n\t\t}\n\t\tgoto cleanup;\n\t}\nshift:\n\t/* Adjust the offsets and shift the remaining entries ahead */\n\text4_xattr_shift_entries(IFIRST(header), EXT4_I(inode)->i_extra_isize\n\t\t\t- new_extra_isize, (void *)raw_inode +\n\t\t\tEXT4_GOOD_OLD_INODE_SIZE + new_extra_isize,\n\t\t\t(void *)header, total_ino);\n\tEXT4_I(inode)->i_extra_isize = new_extra_isize;\n\ncleanup:\n\tif (error && (mnt_count != le16_to_cpu(sbi->s_es->s_mnt_count))) {\n\t\text4_warning(inode->i_sb, \"Unable to expand inode %lu. Delete some EAs or run e2fsck.\",\n\t\t\t     inode->i_ino);\n\t\tmnt_count = le16_to_cpu(sbi->s_es->s_mnt_count);\n\t}\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_257": {
        "vulnerable_code": {
            "Code": "ext4_expand_inode_array(struct ext4_xattr_inode_array **ea_inode_array,\n\t\t\tstruct inode *inode){\n\tif (*ea_inode_array == NULL) {\n\t\t/*\n\t\t * Start with 15 inodes, so it fits into a power-of-two size.\n\t\t * If *ea_inode_array is NULL, this is essentially offsetof()\n\t\t */\n\t\t(*ea_inode_array) =\n\t\t\tkmalloc(offsetof(struct ext4_xattr_inode_array,\n\t\t\t\t\t inodes[EIA_MASK]),\n\t\t\t\tGFP_NOFS);\n\t\tif (*ea_inode_array == NULL)\n\t\t\treturn -ENOMEM;\n\t\t(*ea_inode_array)->count = 0;\n\t} else if (((*ea_inode_array)->count & EIA_MASK) == EIA_MASK) {\n\t\t/* expand the array once all 15 + n * 16 slots are full */\n\t\tstruct ext4_xattr_inode_array *new_array = NULL;\n\t\tint count = (*ea_inode_array)->count;\n\n\t\t/* if new_array is NULL, this is essentially offsetof() */\n\t\tnew_array = kmalloc(\n\t\t\t\toffsetof(struct ext4_xattr_inode_array,\n\t\t\t\t\t inodes[count + EIA_INCR]),\n\t\t\t\tGFP_NOFS);\n\t\tif (new_array == NULL)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(new_array, *ea_inode_array,\n\t\t       offsetof(struct ext4_xattr_inode_array, inodes[count]));\n\t\tkfree(*ea_inode_array);\n\t\t*ea_inode_array = new_array;\n\t}\n\t(*ea_inode_array)->inodes[(*ea_inode_array)->count++] = inode;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_expand_inode_array(struct ext4_xattr_inode_array **ea_inode_array,\n\t\t\tstruct inode *inode){\n\tif (*ea_inode_array == NULL) {\n\t\t/*\n\t\t * Start with 15 inodes, so it fits into a power-of-two size.\n\t\t * If *ea_inode_array is NULL, this is essentially offsetof()\n\t\t */\n\t\t(*ea_inode_array) =\n\t\t\tkmalloc(offsetof(struct ext4_xattr_inode_array,\n\t\t\t\t\t inodes[EIA_MASK]),\n\t\t\t\tGFP_NOFS);\n\t\tif (*ea_inode_array == NULL)\n\t\t\treturn -ENOMEM;\n\t\t(*ea_inode_array)->count = 0;\n\t} else if (((*ea_inode_array)->count & EIA_MASK) == EIA_MASK) {\n\t\t/* expand the array once all 15 + n * 16 slots are full */\n\t\tstruct ext4_xattr_inode_array *new_array = NULL;\n\t\tint count = (*ea_inode_array)->count;\n\n\t\t/* if new_array is NULL, this is essentially offsetof() */\n\t\tnew_array = kmalloc(\n\t\t\t\toffsetof(struct ext4_xattr_inode_array,\n\t\t\t\t\t inodes[count + EIA_INCR]),\n\t\t\t\tGFP_NOFS);\n\t\tif (new_array == NULL)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(new_array, *ea_inode_array,\n\t\t       offsetof(struct ext4_xattr_inode_array, inodes[count]));\n\t\tkfree(*ea_inode_array);\n\t\t*ea_inode_array = new_array;\n\t}\n\t(*ea_inode_array)->inodes[(*ea_inode_array)->count++] = inode;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_258": {
        "vulnerable_code": {
            "Code": "ext4_xattr_cmp(struct ext4_xattr_header *header1,\n\t       struct ext4_xattr_header *header2){\n\tstruct ext4_xattr_entry *entry1, *entry2;\n\n\tentry1 = ENTRY(header1+1);\n\tentry2 = ENTRY(header2+1);\n\twhile (!IS_LAST_ENTRY(entry1)) {\n\t\tif (IS_LAST_ENTRY(entry2))\n\t\t\treturn 1;\n\t\tif (entry1->e_hash != entry2->e_hash ||\n\t\t    entry1->e_name_index != entry2->e_name_index ||\n\t\t    entry1->e_name_len != entry2->e_name_len ||\n\t\t    entry1->e_value_size != entry2->e_value_size ||\n\t\t    entry1->e_value_inum != entry2->e_value_inum ||\n\t\t    memcmp(entry1->e_name, entry2->e_name, entry1->e_name_len))\n\t\t\treturn 1;\n\t\tif (!entry1->e_value_inum &&\n\t\t    memcmp((char *)header1 + le16_to_cpu(entry1->e_value_offs),\n\t\t\t   (char *)header2 + le16_to_cpu(entry2->e_value_offs),\n\t\t\t   le32_to_cpu(entry1->e_value_size)))\n\t\t\treturn 1;\n\n\t\tentry1 = EXT4_XATTR_NEXT(entry1);\n\t\tentry2 = EXT4_XATTR_NEXT(entry2);\n\t}\n\tif (!IS_LAST_ENTRY(entry2))\n\t\treturn 1;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_cmp(struct ext4_xattr_header *header1,\n\t       struct ext4_xattr_header *header2){\n\tstruct ext4_xattr_entry *entry1, *entry2;\n\n\tentry1 = ENTRY(header1+1);\n\tentry2 = ENTRY(header2+1);\n\twhile (!IS_LAST_ENTRY(entry1)) {\n\t\tif (IS_LAST_ENTRY(entry2))\n\t\t\treturn 1;\n\t\tif (entry1->e_hash != entry2->e_hash ||\n\t\t    entry1->e_name_index != entry2->e_name_index ||\n\t\t    entry1->e_name_len != entry2->e_name_len ||\n\t\t    entry1->e_value_size != entry2->e_value_size ||\n\t\t    entry1->e_value_inum != entry2->e_value_inum ||\n\t\t    memcmp(entry1->e_name, entry2->e_name, entry1->e_name_len))\n\t\t\treturn 1;\n\t\tif (!entry1->e_value_inum &&\n\t\t    memcmp((char *)header1 + le16_to_cpu(entry1->e_value_offs),\n\t\t\t   (char *)header2 + le16_to_cpu(entry2->e_value_offs),\n\t\t\t   le32_to_cpu(entry1->e_value_size)))\n\t\t\treturn 1;\n\n\t\tentry1 = EXT4_XATTR_NEXT(entry1);\n\t\tentry2 = EXT4_XATTR_NEXT(entry2);\n\t}\n\tif (!IS_LAST_ENTRY(entry2))\n\t\treturn 1;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_259": {
        "vulnerable_code": {
            "Code": "ext4_xattr_delete_inode(handle_t *handle, struct inode *inode,\n\t\t\t    struct ext4_xattr_inode_array **ea_inode_array,\n\t\t\t    int extra_credits){\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_iloc iloc = { .bh = NULL };\n\tstruct ext4_xattr_entry *entry;\n\tstruct inode *ea_inode;\n\tint error;\n\n\terror = ext4_xattr_ensure_credits(handle, inode, extra_credits,\n\t\t\t\t\t  NULL /* bh */,\n\t\t\t\t\t  false /* dirty */,\n\t\t\t\t\t  false /* block_csum */);\n\tif (error) {\n\t\tEXT4_ERROR_INODE(inode, \"ensure credits (error %d)\", error);\n\t\tgoto cleanup;\n\t}\n\n\tif (ext4_has_feature_ea_inode(inode->i_sb) &&\n\t    ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error) {\n\t\t\tEXT4_ERROR_INODE(inode, \"inode loc (error %d)\", error);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\terror = ext4_journal_get_write_access(handle, iloc.bh);\n\t\tif (error) {\n\t\t\tEXT4_ERROR_INODE(inode, \"write access (error %d)\",\n\t\t\t\t\t error);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\theader = IHDR(inode, ext4_raw_inode(&iloc));\n\t\tif (header->h_magic == cpu_to_le32(EXT4_XATTR_MAGIC))\n\t\t\text4_xattr_inode_dec_ref_all(handle, inode, iloc.bh,\n\t\t\t\t\t\t     IFIRST(header),\n\t\t\t\t\t\t     false /* block_csum */,\n\t\t\t\t\t\t     ea_inode_array,\n\t\t\t\t\t\t     extra_credits,\n\t\t\t\t\t\t     false /* skip_quota */);\n\t}\n\n\tif (EXT4_I(inode)->i_file_acl) {\n\t\tbh = sb_bread(inode->i_sb, EXT4_I(inode)->i_file_acl);\n\t\tif (!bh) {\n\t\t\tEXT4_ERROR_INODE(inode, \"block %llu read error\",\n\t\t\t\t\t EXT4_I(inode)->i_file_acl);\n\t\t\terror = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\terror = ext4_xattr_check_block(inode, bh);\n\t\tif (error)\n\t\t\tgoto cleanup;\n\n\t\tif (ext4_has_feature_ea_inode(inode->i_sb)) {\n\t\t\tfor (entry = BFIRST(bh); !IS_LAST_ENTRY(entry);\n\t\t\t     entry = EXT4_XATTR_NEXT(entry)) {\n\t\t\t\tif (!entry->e_value_inum)\n\t\t\t\t\tcontinue;\n\t\t\t\terror = ext4_xattr_inode_iget(inode,\n\t\t\t\t\t      le32_to_cpu(entry->e_value_inum),\n\t\t\t\t\t      le32_to_cpu(entry->e_hash),\n\t\t\t\t\t      &ea_inode);\n\t\t\t\tif (error)\n\t\t\t\t\tcontinue;\n\t\t\t\text4_xattr_inode_free_quota(inode, ea_inode,\n\t\t\t\t\t      le32_to_cpu(entry->e_value_size));\n\t\t\t\tiput(ea_inode);\n\t\t\t}\n\n\t\t}\n\n\t\text4_xattr_release_block(handle, inode, bh, ea_inode_array,\n\t\t\t\t\t extra_credits);\n\t\t/*\n\t\t * Update i_file_acl value in the same transaction that releases\n\t\t * block.\n\t\t */\n\t\tEXT4_I(inode)->i_file_acl = 0;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\tif (error) {\n\t\t\tEXT4_ERROR_INODE(inode, \"mark inode dirty (error %d)\",\n\t\t\t\t\t error);\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\terror = 0;\ncleanup:\n\tbrelse(iloc.bh);\n\tbrelse(bh);\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_delete_inode(handle_t *handle, struct inode *inode,\n\t\t\t    struct ext4_xattr_inode_array **ea_inode_array,\n\t\t\t    int extra_credits){\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_iloc iloc = { .bh = NULL };\n\tstruct ext4_xattr_entry *entry;\n\tstruct inode *ea_inode;\n\tint error;\n\n\terror = ext4_xattr_ensure_credits(handle, inode, extra_credits,\n\t\t\t\t\t  NULL /* bh */,\n\t\t\t\t\t  false /* dirty */,\n\t\t\t\t\t  false /* block_csum */);\n\tif (error) {\n\t\tEXT4_ERROR_INODE(inode, \"ensure credits (error %d)\", error);\n\t\tgoto cleanup;\n\t}\n\n\tif (ext4_has_feature_ea_inode(inode->i_sb) &&\n\t    ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\n\t\terror = ext4_get_inode_loc(inode, &iloc);\n\t\tif (error) {\n\t\t\tEXT4_ERROR_INODE(inode, \"inode loc (error %d)\", error);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\terror = ext4_journal_get_write_access(handle, iloc.bh);\n\t\tif (error) {\n\t\t\tEXT4_ERROR_INODE(inode, \"write access (error %d)\",\n\t\t\t\t\t error);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\theader = IHDR(inode, ext4_raw_inode(&iloc));\n\t\tif (header->h_magic == cpu_to_le32(EXT4_XATTR_MAGIC))\n\t\t\text4_xattr_inode_dec_ref_all(handle, inode, iloc.bh,\n\t\t\t\t\t\t     IFIRST(header),\n\t\t\t\t\t\t     false /* block_csum */,\n\t\t\t\t\t\t     ea_inode_array,\n\t\t\t\t\t\t     extra_credits,\n\t\t\t\t\t\t     false /* skip_quota */);\n\t}\n\n\tif (EXT4_I(inode)->i_file_acl) {\n\t\tbh = sb_bread(inode->i_sb, EXT4_I(inode)->i_file_acl);\n\t\tif (!bh) {\n\t\t\tEXT4_ERROR_INODE(inode, \"block %llu read error\",\n\t\t\t\t\t EXT4_I(inode)->i_file_acl);\n\t\t\terror = -EIO;\n\t\t\tgoto cleanup;\n\t\t}\n\t\terror = ext4_xattr_check_block(inode, bh);\n\t\tif (error)\n\t\t\tgoto cleanup;\n\n\t\tif (ext4_has_feature_ea_inode(inode->i_sb)) {\n\t\t\tfor (entry = BFIRST(bh); !IS_LAST_ENTRY(entry);\n\t\t\t     entry = EXT4_XATTR_NEXT(entry)) {\n\t\t\t\tif (!entry->e_value_inum)\n\t\t\t\t\tcontinue;\n\t\t\t\terror = ext4_xattr_inode_iget(inode,\n\t\t\t\t\t      le32_to_cpu(entry->e_value_inum),\n\t\t\t\t\t      le32_to_cpu(entry->e_hash),\n\t\t\t\t\t      &ea_inode);\n\t\t\t\tif (error)\n\t\t\t\t\tcontinue;\n\t\t\t\text4_xattr_inode_free_quota(inode, ea_inode,\n\t\t\t\t\t      le32_to_cpu(entry->e_value_size));\n\t\t\t\tiput(ea_inode);\n\t\t\t}\n\n\t\t}\n\n\t\text4_xattr_release_block(handle, inode, bh, ea_inode_array,\n\t\t\t\t\t extra_credits);\n\t\t/*\n\t\t * Update i_file_acl value in the same transaction that releases\n\t\t * block.\n\t\t */\n\t\tEXT4_I(inode)->i_file_acl = 0;\n\t\terror = ext4_mark_inode_dirty(handle, inode);\n\t\tif (error) {\n\t\t\tEXT4_ERROR_INODE(inode, \"mark inode dirty (error %d)\",\n\t\t\t\t\t error);\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\terror = 0;\ncleanup:\n\tbrelse(iloc.bh);\n\tbrelse(bh);\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_260": {
        "vulnerable_code": {
            "Code": "ext4_xattr_destroy_cache(struct mb_cache *cache){\n\tif (cache)\n\t\tmb_cache_destroy(cache);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_destroy_cache(struct mb_cache *cache){\n\tif (cache)\n\t\tmb_cache_destroy(cache);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_261": {
        "vulnerable_code": {
            "Code": "ext4_xattr_handler(int name_index){\n\tconst struct xattr_handler *handler = NULL;\n\n\tif (name_index > 0 && name_index < ARRAY_SIZE(ext4_xattr_handler_map))\n\t\thandler = ext4_xattr_handler_map[name_index];\n\treturn handler;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_handler(int name_index){\n\tconst struct xattr_handler *handler = NULL;\n\n\tif (name_index > 0 && name_index < ARRAY_SIZE(ext4_xattr_handler_map))\n\t\thandler = ext4_xattr_handler_map[name_index];\n\treturn handler;\n}",
            "Size": 5,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_262": {
        "vulnerable_code": {
            "Code": "ext4_xattr_ibody_inline_set(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_xattr_info *i,\n\t\t\t\tstruct ext4_xattr_ibody_find *is){\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_search *s = &is->s;\n\tint error;\n\n\tif (EXT4_I(inode)->i_extra_isize == 0)\n\t\treturn -ENOSPC;\n\terror = ext4_xattr_set_entry(i, s, handle, inode, false /* is_block */);\n\tif (error) {\n\t\tif (error == -ENOSPC &&\n\t\t    ext4_has_inline_data(inode)) {\n\t\t\terror = ext4_try_to_evict_inline_data(handle, inode,\n\t\t\t\t\tEXT4_XATTR_LEN(strlen(i->name) +\n\t\t\t\t\tEXT4_XATTR_SIZE(i->value_len)));\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\terror = ext4_xattr_ibody_find(inode, i, is);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\terror = ext4_xattr_set_entry(i, s, handle, inode,\n\t\t\t\t\t\t     false /* is_block */);\n\t\t}\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\theader = IHDR(inode, ext4_raw_inode(&is->iloc));\n\tif (!IS_LAST_ENTRY(s->first)) {\n\t\theader->h_magic = cpu_to_le32(EXT4_XATTR_MAGIC);\n\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t} else {\n\t\theader->h_magic = cpu_to_le32(0);\n\t\text4_clear_inode_state(inode, EXT4_STATE_XATTR);\n\t}\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_ibody_inline_set(handle_t *handle, struct inode *inode,\n\t\t\t\tstruct ext4_xattr_info *i,\n\t\t\t\tstruct ext4_xattr_ibody_find *is){\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_search *s = &is->s;\n\tint error;\n\n\tif (EXT4_I(inode)->i_extra_isize == 0)\n\t\treturn -ENOSPC;\n\terror = ext4_xattr_set_entry(i, s, handle, inode, false /* is_block */);\n\tif (error) {\n\t\tif (error == -ENOSPC &&\n\t\t    ext4_has_inline_data(inode)) {\n\t\t\terror = ext4_try_to_evict_inline_data(handle, inode,\n\t\t\t\t\tEXT4_XATTR_LEN(strlen(i->name) +\n\t\t\t\t\tEXT4_XATTR_SIZE(i->value_len)));\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\terror = ext4_xattr_ibody_find(inode, i, is);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t\terror = ext4_xattr_set_entry(i, s, handle, inode,\n\t\t\t\t\t\t     false /* is_block */);\n\t\t}\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\theader = IHDR(inode, ext4_raw_inode(&is->iloc));\n\tif (!IS_LAST_ENTRY(s->first)) {\n\t\theader->h_magic = cpu_to_le32(EXT4_XATTR_MAGIC);\n\t\text4_set_inode_state(inode, EXT4_STATE_XATTR);\n\t} else {\n\t\theader->h_magic = cpu_to_le32(0);\n\t\text4_clear_inode_state(inode, EXT4_STATE_XATTR);\n\t}\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_263": {
        "vulnerable_code": {
            "Code": "ext4_xattr_inode_iget(struct inode *parent, unsigned long ea_ino,\n\t\t\t\t u32 ea_inode_hash, struct inode **ea_inode){\n\tstruct inode *inode;\n\tint err;\n\n\tinode = ext4_iget(parent->i_sb, ea_ino);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\text4_error(parent->i_sb,\n\t\t\t   \"error while reading EA inode %lu err=%d\", ea_ino,\n\t\t\t   err);\n\t\treturn err;\n\t}\n\n\tif (is_bad_inode(inode)) {\n\t\text4_error(parent->i_sb,\n\t\t\t   \"error while reading EA inode %lu is_bad_inode\",\n\t\t\t   ea_ino);\n\t\terr = -EIO;\n\t\tgoto error;\n\t}\n\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL)) {\n\t\text4_error(parent->i_sb,\n\t\t\t   \"EA inode %lu does not have EXT4_EA_INODE_FL flag\",\n\t\t\t    ea_ino);\n\t\terr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\text4_xattr_inode_set_class(inode);\n\n\t/*\n\t * Check whether this is an old Lustre-style xattr inode. Lustre\n\t * implementation does not have hash validation, rather it has a\n\t * backpointer from ea_inode to the parent inode.\n\t */\n\tif (ea_inode_hash != ext4_xattr_inode_get_hash(inode) &&\n\t    EXT4_XATTR_INODE_GET_PARENT(inode) == parent->i_ino &&\n\t    inode->i_generation == parent->i_generation) {\n\t\text4_set_inode_state(inode, EXT4_STATE_LUSTRE_EA_INODE);\n\t\text4_xattr_inode_set_ref(inode, 1);\n\t} else {\n\t\tinode_lock(inode);\n\t\tinode->i_flags |= S_NOQUOTA;\n\t\tinode_unlock(inode);\n\t}\n\n\t*ea_inode = inode;\n\treturn 0;\nerror:\n\tiput(inode);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_inode_iget(struct inode *parent, unsigned long ea_ino,\n\t\t\t\t u32 ea_inode_hash, struct inode **ea_inode){\n\tstruct inode *inode;\n\tint err;\n\n\tinode = ext4_iget(parent->i_sb, ea_ino);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\text4_error(parent->i_sb,\n\t\t\t   \"error while reading EA inode %lu err=%d\", ea_ino,\n\t\t\t   err);\n\t\treturn err;\n\t}\n\n\tif (is_bad_inode(inode)) {\n\t\text4_error(parent->i_sb,\n\t\t\t   \"error while reading EA inode %lu is_bad_inode\",\n\t\t\t   ea_ino);\n\t\terr = -EIO;\n\t\tgoto error;\n\t}\n\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL)) {\n\t\text4_error(parent->i_sb,\n\t\t\t   \"EA inode %lu does not have EXT4_EA_INODE_FL flag\",\n\t\t\t    ea_ino);\n\t\terr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\text4_xattr_inode_set_class(inode);\n\n\t/*\n\t * Check whether this is an old Lustre-style xattr inode. Lustre\n\t * implementation does not have hash validation, rather it has a\n\t * backpointer from ea_inode to the parent inode.\n\t */\n\tif (ea_inode_hash != ext4_xattr_inode_get_hash(inode) &&\n\t    EXT4_XATTR_INODE_GET_PARENT(inode) == parent->i_ino &&\n\t    inode->i_generation == parent->i_generation) {\n\t\text4_set_inode_state(inode, EXT4_STATE_LUSTRE_EA_INODE);\n\t\text4_xattr_inode_set_ref(inode, 1);\n\t} else {\n\t\tinode_lock(inode);\n\t\tinode->i_flags |= S_NOQUOTA;\n\t\tinode_unlock(inode);\n\t}\n\n\t*ea_inode = inode;\n\treturn 0;\nerror:\n\tiput(inode);\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_264": {
        "vulnerable_code": {
            "Code": "ext4_xattr_inode_set_hash(struct inode *ea_inode, u32 hash){\n\tea_inode->i_atime.tv_sec = hash;\n}",
            "Size": 2,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 2,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_inode_set_hash(struct inode *ea_inode, u32 hash){\n\tea_inode->i_atime.tv_sec = hash;\n}",
            "Size": 3,
            "Code Complexity": 1,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_265": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_266": {
        "vulnerable_code": {
            "Code": "ext4_xattr_set_handle(handle_t *handle, struct inode *inode, int name_index,\n\t\t      const char *name, const void *value, size_t value_len,\n\t\t      int flags){\n\tstruct ext4_xattr_info i = {\n\t\t.name_index = name_index,\n\t\t.name = name,\n\t\t.value = value,\n\t\t.value_len = value_len,\n\t\t.in_inode = 0,\n\t};\n\tstruct ext4_xattr_ibody_find is = {\n\t\t.s = { .not_found = -ENODATA, },\n\t};\n\tstruct ext4_xattr_block_find bs = {\n\t\t.s = { .not_found = -ENODATA, },\n\t};\n\tint no_expand;\n\tint error;\n\n\tif (!name)\n\t\treturn -EINVAL;\n\tif (strlen(name) > 255)\n\t\treturn -ERANGE;\n\n\text4_write_lock_xattr(inode, &no_expand);\n\n\t/* Check journal credits under write lock. */\n\tif (ext4_handle_valid(handle)) {\n\t\tstruct buffer_head *bh;\n\t\tint credits;\n\n\t\tbh = ext4_xattr_get_block(inode);\n\t\tif (IS_ERR(bh)) {\n\t\t\terror = PTR_ERR(bh);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tcredits = __ext4_xattr_set_credits(inode->i_sb, inode, bh,\n\t\t\t\t\t\t   value_len,\n\t\t\t\t\t\t   flags & XATTR_CREATE);\n\t\tbrelse(bh);\n\n\t\tif (!ext4_handle_has_enough_credits(handle, credits)) {\n\t\t\terror = -ENOSPC;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\terror = ext4_reserve_inode_write(handle, inode, &is.iloc);\n\tif (error)\n\t\tgoto cleanup;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NEW)) {\n\t\tstruct ext4_inode *raw_inode = ext4_raw_inode(&is.iloc);\n\t\tmemset(raw_inode, 0, EXT4_SB(inode->i_sb)->s_inode_size);\n\t\text4_clear_inode_state(inode, EXT4_STATE_NEW);\n\t}\n\n\terror = ext4_xattr_ibody_find(inode, &i, &is);\n\tif (error)\n\t\tgoto cleanup;\n\tif (is.s.not_found)\n\t\terror = ext4_xattr_block_find(inode, &i, &bs);\n\tif (error)\n\t\tgoto cleanup;\n\tif (is.s.not_found && bs.s.not_found) {\n\t\terror = -ENODATA;\n\t\tif (flags & XATTR_REPLACE)\n\t\t\tgoto cleanup;\n\t\terror = 0;\n\t\tif (!value)\n\t\t\tgoto cleanup;\n\t} else {\n\t\terror = -EEXIST;\n\t\tif (flags & XATTR_CREATE)\n\t\t\tgoto cleanup;\n\t}\n\n\tif (!value) {\n\t\tif (!is.s.not_found)\n\t\t\terror = ext4_xattr_ibody_set(handle, inode, &i, &is);\n\t\telse if (!bs.s.not_found)\n\t\t\terror = ext4_xattr_block_set(handle, inode, &i, &bs);\n\t} else {\n\t\terror = 0;\n\t\t/* Xattr value did not change? Save us some work and bail out */\n\t\tif (!is.s.not_found && ext4_xattr_value_same(&is.s, &i))\n\t\t\tgoto cleanup;\n\t\tif (!bs.s.not_found && ext4_xattr_value_same(&bs.s, &i))\n\t\t\tgoto cleanup;\n\n\t\tif (ext4_has_feature_ea_inode(inode->i_sb) &&\n\t\t    (EXT4_XATTR_SIZE(i.value_len) >\n\t\t\tEXT4_XATTR_MIN_LARGE_EA_SIZE(inode->i_sb->s_blocksize)))\n\t\t\ti.in_inode = 1;\nretry_inode:\n\t\terror = ext4_xattr_ibody_set(handle, inode, &i, &is);\n\t\tif (!error && !bs.s.not_found) {\n\t\t\ti.value = NULL;\n\t\t\terror = ext4_xattr_block_set(handle, inode, &i, &bs);\n\t\t} else if (error == -ENOSPC) {\n\t\t\tif (EXT4_I(inode)->i_file_acl && !bs.s.base) {\n\t\t\t\terror = ext4_xattr_block_find(inode, &i, &bs);\n\t\t\t\tif (error)\n\t\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t\terror = ext4_xattr_block_set(handle, inode, &i, &bs);\n\t\t\tif (!error && !is.s.not_found) {\n\t\t\t\ti.value = NULL;\n\t\t\t\terror = ext4_xattr_ibody_set(handle, inode, &i,\n\t\t\t\t\t\t\t     &is);\n\t\t\t} else if (error == -ENOSPC) {\n\t\t\t\t/*\n\t\t\t\t * Xattr does not fit in the block, store at\n\t\t\t\t * external inode if possible.\n\t\t\t\t */\n\t\t\t\tif (ext4_has_feature_ea_inode(inode->i_sb) &&\n\t\t\t\t    !i.in_inode) {\n\t\t\t\t\ti.in_inode = 1;\n\t\t\t\t\tgoto retry_inode;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif (!error) {\n\t\text4_xattr_update_super_block(handle, inode->i_sb);\n\t\tinode->i_ctime = current_time(inode);\n\t\tif (!value)\n\t\t\tno_expand = 0;\n\t\terror = ext4_mark_iloc_dirty(handle, inode, &is.iloc);\n\t\t/*\n\t\t * The bh is consumed by ext4_mark_iloc_dirty, even with\n\t\t * error != 0.\n\t\t */\n\t\tis.iloc.bh = NULL;\n\t\tif (IS_SYNC(inode))\n\t\t\text4_handle_sync(handle);\n\t}\n\ncleanup:\n\tbrelse(is.iloc.bh);\n\tbrelse(bs.bh);\n\text4_write_unlock_xattr(inode, &no_expand);\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_set_handle(handle_t *handle, struct inode *inode, int name_index,\n\t\t      const char *name, const void *value, size_t value_len,\n\t\t      int flags){\n\tstruct ext4_xattr_info i = {\n\t\t.name_index = name_index,\n\t\t.name = name,\n\t\t.value = value,\n\t\t.value_len = value_len,\n\t\t.in_inode = 0,\n\t};\n\tstruct ext4_xattr_ibody_find is = {\n\t\t.s = { .not_found = -ENODATA, },\n\t};\n\tstruct ext4_xattr_block_find bs = {\n\t\t.s = { .not_found = -ENODATA, },\n\t};\n\tint no_expand;\n\tint error;\n\n\tif (!name)\n\t\treturn -EINVAL;\n\tif (strlen(name) > 255)\n\t\treturn -ERANGE;\n\n\text4_write_lock_xattr(inode, &no_expand);\n\n\t/* Check journal credits under write lock. */\n\tif (ext4_handle_valid(handle)) {\n\t\tstruct buffer_head *bh;\n\t\tint credits;\n\n\t\tbh = ext4_xattr_get_block(inode);\n\t\tif (IS_ERR(bh)) {\n\t\t\terror = PTR_ERR(bh);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tcredits = __ext4_xattr_set_credits(inode->i_sb, inode, bh,\n\t\t\t\t\t\t   value_len,\n\t\t\t\t\t\t   flags & XATTR_CREATE);\n\t\tbrelse(bh);\n\n\t\tif (!ext4_handle_has_enough_credits(handle, credits)) {\n\t\t\terror = -ENOSPC;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\terror = ext4_reserve_inode_write(handle, inode, &is.iloc);\n\tif (error)\n\t\tgoto cleanup;\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_NEW)) {\n\t\tstruct ext4_inode *raw_inode = ext4_raw_inode(&is.iloc);\n\t\tmemset(raw_inode, 0, EXT4_SB(inode->i_sb)->s_inode_size);\n\t\text4_clear_inode_state(inode, EXT4_STATE_NEW);\n\t}\n\n\terror = ext4_xattr_ibody_find(inode, &i, &is);\n\tif (error)\n\t\tgoto cleanup;\n\tif (is.s.not_found)\n\t\terror = ext4_xattr_block_find(inode, &i, &bs);\n\tif (error)\n\t\tgoto cleanup;\n\tif (is.s.not_found && bs.s.not_found) {\n\t\terror = -ENODATA;\n\t\tif (flags & XATTR_REPLACE)\n\t\t\tgoto cleanup;\n\t\terror = 0;\n\t\tif (!value)\n\t\t\tgoto cleanup;\n\t} else {\n\t\terror = -EEXIST;\n\t\tif (flags & XATTR_CREATE)\n\t\t\tgoto cleanup;\n\t}\n\n\tif (!value) {\n\t\tif (!is.s.not_found)\n\t\t\terror = ext4_xattr_ibody_set(handle, inode, &i, &is);\n\t\telse if (!bs.s.not_found)\n\t\t\terror = ext4_xattr_block_set(handle, inode, &i, &bs);\n\t} else {\n\t\terror = 0;\n\t\t/* Xattr value did not change? Save us some work and bail out */\n\t\tif (!is.s.not_found && ext4_xattr_value_same(&is.s, &i))\n\t\t\tgoto cleanup;\n\t\tif (!bs.s.not_found && ext4_xattr_value_same(&bs.s, &i))\n\t\t\tgoto cleanup;\n\n\t\tif (ext4_has_feature_ea_inode(inode->i_sb) &&\n\t\t    (EXT4_XATTR_SIZE(i.value_len) >\n\t\t\tEXT4_XATTR_MIN_LARGE_EA_SIZE(inode->i_sb->s_blocksize)))\n\t\t\ti.in_inode = 1;\nretry_inode:\n\t\terror = ext4_xattr_ibody_set(handle, inode, &i, &is);\n\t\tif (!error && !bs.s.not_found) {\n\t\t\ti.value = NULL;\n\t\t\terror = ext4_xattr_block_set(handle, inode, &i, &bs);\n\t\t} else if (error == -ENOSPC) {\n\t\t\tif (EXT4_I(inode)->i_file_acl && !bs.s.base) {\n\t\t\t\terror = ext4_xattr_block_find(inode, &i, &bs);\n\t\t\t\tif (error)\n\t\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t\terror = ext4_xattr_block_set(handle, inode, &i, &bs);\n\t\t\tif (!error && !is.s.not_found) {\n\t\t\t\ti.value = NULL;\n\t\t\t\terror = ext4_xattr_ibody_set(handle, inode, &i,\n\t\t\t\t\t\t\t     &is);\n\t\t\t} else if (error == -ENOSPC) {\n\t\t\t\t/*\n\t\t\t\t * Xattr does not fit in the block, store at\n\t\t\t\t * external inode if possible.\n\t\t\t\t */\n\t\t\t\tif (ext4_has_feature_ea_inode(inode->i_sb) &&\n\t\t\t\t    !i.in_inode) {\n\t\t\t\t\ti.in_inode = 1;\n\t\t\t\t\tgoto retry_inode;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif (!error) {\n\t\text4_xattr_update_super_block(handle, inode->i_sb);\n\t\tinode->i_ctime = current_time(inode);\n\t\tif (!value)\n\t\t\tno_expand = 0;\n\t\terror = ext4_mark_iloc_dirty(handle, inode, &is.iloc);\n\t\t/*\n\t\t * The bh is consumed by ext4_mark_iloc_dirty, even with\n\t\t * error != 0.\n\t\t */\n\t\tis.iloc.bh = NULL;\n\t\tif (IS_SYNC(inode))\n\t\t\text4_handle_sync(handle);\n\t}\n\ncleanup:\n\tbrelse(is.iloc.bh);\n\tbrelse(bs.bh);\n\text4_write_unlock_xattr(inode, &no_expand);\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_267": {
        "vulnerable_code": {
            "Code": "ext4_xattr_shift_entries(struct ext4_xattr_entry *entry,\n\t\t\t\t     int value_offs_shift, void *to,\n\t\t\t\t     void *from, size_t n){\n\tstruct ext4_xattr_entry *last = entry;\n\tint new_offs;\n\n\t/* We always shift xattr headers further thus offsets get lower */\n\tBUG_ON(value_offs_shift > 0);\n\n\t/* Adjust the value offsets of the entries */\n\tfor (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) {\n\t\tif (!last->e_value_inum && last->e_value_size) {\n\t\t\tnew_offs = le16_to_cpu(last->e_value_offs) +\n\t\t\t\t\t\t\tvalue_offs_shift;\n\t\t\tlast->e_value_offs = cpu_to_le16(new_offs);\n\t\t}\n\t}\n\t/* Shift the entries by n bytes */\n\tmemmove(to, from, n);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_shift_entries(struct ext4_xattr_entry *entry,\n\t\t\t\t     int value_offs_shift, void *to,\n\t\t\t\t     void *from, size_t n){\n\tstruct ext4_xattr_entry *last = entry;\n\tint new_offs;\n\n\t/* We always shift xattr headers further thus offsets get lower */\n\tBUG_ON(value_offs_shift > 0);\n\n\t/* Adjust the value offsets of the entries */\n\tfor (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) {\n\t\tif (!last->e_value_inum && last->e_value_size) {\n\t\t\tnew_offs = le16_to_cpu(last->e_value_offs) +\n\t\t\t\t\t\t\tvalue_offs_shift;\n\t\t\tlast->e_value_offs = cpu_to_le16(new_offs);\n\t\t}\n\t}\n\t/* Shift the entries by n bytes */\n\tmemmove(to, from, n);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_268": {
        "vulnerable_code": {
            "Code": "ext4_xattr_value_same(struct ext4_xattr_search *s,\n\t\t\t\t struct ext4_xattr_info *i){\n\tvoid *value;\n\n\t/* When e_value_inum is set the value is stored externally. */\n\tif (s->here->e_value_inum)\n\t\treturn 0;\n\tif (le32_to_cpu(s->here->e_value_size) != i->value_len)\n\t\treturn 0;\n\tvalue = ((void *)s->base) + le16_to_cpu(s->here->e_value_offs);\n\treturn !memcmp(value, i->value, i->value_len);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_value_same(struct ext4_xattr_search *s,\n\t\t\t\t struct ext4_xattr_info *i){\n\tvoid *value;\n\n\t/* When e_value_inum is set the value is stored externally. */\n\tif (s->here->e_value_inum)\n\t\treturn 0;\n\tif (le32_to_cpu(s->here->e_value_size) != i->value_len)\n\t\treturn 0;\n\tvalue = ((void *)s->base) + le16_to_cpu(s->here->e_value_offs);\n\treturn !memcmp(value, i->value, i->value_len);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_269": {
        "vulnerable_code": {
            "Code": "__ext4_error(struct super_block *sb, const char *function,\n\t\t  unsigned int line, const char *fmt, ...){\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\ttrace_ext4_error(sb, function, line);\n\tif (ext4_error_ratelimit(sb)) {\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tprintk(KERN_CRIT\n\t\t       \"EXT4-fs error (device %s): %s:%d: comm %s: %pV\\n\",\n\t\t       sb->s_id, function, line, current->comm, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(sb, function, line);\n\text4_handle_error(sb);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__ext4_error(struct super_block *sb, const char *function,\n\t\t  unsigned int line, const char *fmt, ...){\n\tstruct va_format vaf;\n\tva_list args;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\ttrace_ext4_error(sb, function, line);\n\tif (ext4_error_ratelimit(sb)) {\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tprintk(KERN_CRIT\n\t\t       \"EXT4-fs error (device %s): %s:%d: comm %s: %pV\\n\",\n\t\t       sb->s_id, function, line, current->comm, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(sb, function, line);\n\text4_handle_error(sb);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_270": {
        "vulnerable_code": {
            "Code": "__ext4_error_file(struct file *file, const char *function,\n\t\t       unsigned int line, ext4_fsblk_t block,\n\t\t       const char *fmt, ...){\n\tva_list args;\n\tstruct va_format vaf;\n\tstruct ext4_super_block *es;\n\tstruct inode *inode = file_inode(file);\n\tchar pathname[80], *path;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn;\n\n\ttrace_ext4_error(inode->i_sb, function, line);\n\tes = EXT4_SB(inode->i_sb)->s_es;\n\tes->s_last_error_ino = cpu_to_le32(inode->i_ino);\n\tif (ext4_error_ratelimit(inode->i_sb)) {\n\t\tpath = file_path(file, pathname, sizeof(pathname));\n\t\tif (IS_ERR(path))\n\t\t\tpath = \"(unknown)\";\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tif (block)\n\t\t\tprintk(KERN_CRIT\n\t\t\t       \"EXT4-fs error (device %s): %s:%d: inode #%lu: \"\n\t\t\t       \"block %llu: comm %s: path %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       block, current->comm, path, &vaf);\n\t\telse\n\t\t\tprintk(KERN_CRIT\n\t\t\t       \"EXT4-fs error (device %s): %s:%d: inode #%lu: \"\n\t\t\t       \"comm %s: path %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       current->comm, path, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(inode->i_sb, function, line);\n\text4_handle_error(inode->i_sb);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__ext4_error_file(struct file *file, const char *function,\n\t\t       unsigned int line, ext4_fsblk_t block,\n\t\t       const char *fmt, ...){\n\tva_list args;\n\tstruct va_format vaf;\n\tstruct ext4_super_block *es;\n\tstruct inode *inode = file_inode(file);\n\tchar pathname[80], *path;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(inode->i_sb))))\n\t\treturn;\n\n\ttrace_ext4_error(inode->i_sb, function, line);\n\tes = EXT4_SB(inode->i_sb)->s_es;\n\tes->s_last_error_ino = cpu_to_le32(inode->i_ino);\n\tif (ext4_error_ratelimit(inode->i_sb)) {\n\t\tpath = file_path(file, pathname, sizeof(pathname));\n\t\tif (IS_ERR(path))\n\t\t\tpath = \"(unknown)\";\n\t\tva_start(args, fmt);\n\t\tvaf.fmt = fmt;\n\t\tvaf.va = &args;\n\t\tif (block)\n\t\t\tprintk(KERN_CRIT\n\t\t\t       \"EXT4-fs error (device %s): %s:%d: inode #%lu: \"\n\t\t\t       \"block %llu: comm %s: path %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       block, current->comm, path, &vaf);\n\t\telse\n\t\t\tprintk(KERN_CRIT\n\t\t\t       \"EXT4-fs error (device %s): %s:%d: inode #%lu: \"\n\t\t\t       \"comm %s: path %s: %pV\\n\",\n\t\t\t       inode->i_sb->s_id, function, line, inode->i_ino,\n\t\t\t       current->comm, path, &vaf);\n\t\tva_end(args);\n\t}\n\tsave_error_info(inode->i_sb, function, line);\n\text4_handle_error(inode->i_sb);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_271": {
        "vulnerable_code": {
            "Code": "__ext4_std_error(struct super_block *sb, const char *function,\n\t\t      unsigned int line, int errno){\n\tchar nbuf[16];\n\tconst char *errstr;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\t/* Special case: if the error is EROFS, and we're not already\n\t * inside a transaction, then there's really no point in logging\n\t * an error. */\n\tif (errno == -EROFS && journal_current_handle() == NULL && sb_rdonly(sb))\n\t\treturn;\n\n\tif (ext4_error_ratelimit(sb)) {\n\t\terrstr = ext4_decode_error(sb, errno, nbuf);\n\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s) in %s:%d: %s\\n\",\n\t\t       sb->s_id, function, line, errstr);\n\t}\n\n\tsave_error_info(sb, function, line);\n\text4_handle_error(sb);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__ext4_std_error(struct super_block *sb, const char *function,\n\t\t      unsigned int line, int errno){\n\tchar nbuf[16];\n\tconst char *errstr;\n\n\tif (unlikely(ext4_forced_shutdown(EXT4_SB(sb))))\n\t\treturn;\n\n\t/* Special case: if the error is EROFS, and we're not already\n\t * inside a transaction, then there's really no point in logging\n\t * an error. */\n\tif (errno == -EROFS && journal_current_handle() == NULL && sb_rdonly(sb))\n\t\treturn;\n\n\tif (ext4_error_ratelimit(sb)) {\n\t\terrstr = ext4_decode_error(sb, errno, nbuf);\n\t\tprintk(KERN_CRIT \"EXT4-fs error (device %s) in %s:%d: %s\\n\",\n\t\t       sb->s_id, function, line, errstr);\n\t}\n\n\tsave_error_info(sb, function, line);\n\text4_handle_error(sb);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_272": {
        "vulnerable_code": {
            "Code": "ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk){\n\tbg->bg_block_bitmap_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_block_bitmap_hi = cpu_to_le32(blk >> 32);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 5,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_block_bitmap_set(struct super_block *sb,\n\t\t\t   struct ext4_group_desc *bg, ext4_fsblk_t blk){\n\tbg->bg_block_bitmap_lo = cpu_to_le32((u32)blk);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_block_bitmap_hi = cpu_to_le32(blk >> 32);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_273": {
        "vulnerable_code": {
            "Code": "ext4_check_descriptors(struct super_block *sb,\n\t\t\t\t  ext4_fsblk_t sb_block,\n\t\t\t\t  ext4_group_t *first_not_zeroed){\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n\text4_fsblk_t last_block;\n\text4_fsblk_t block_bitmap;\n\text4_fsblk_t inode_bitmap;\n\text4_fsblk_t inode_table;\n\tint flexbg_flag = 0;\n\text4_group_t i, grp = sbi->s_groups_count;\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tflexbg_flag = 1;\n\n\text4_debug(\"Checking group descriptors\");\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tif (i == sbi->s_groups_count - 1 || flexbg_flag)\n\t\t\tlast_block = ext4_blocks_count(sbi->s_es) - 1;\n\t\telse\n\t\t\tlast_block = first_block +\n\t\t\t\t(EXT4_BLOCKS_PER_GROUP(sb) - 1);\n\n\t\tif ((grp == sbi->s_groups_count) &&\n\t\t   !(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tgrp = i;\n\n\t\tblock_bitmap = ext4_block_bitmap(sb, gdp);\n\t\tif (block_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Block bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, block_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_bitmap = ext4_inode_bitmap(sb, gdp);\n\t\tif (inode_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_table = ext4_inode_table(sb, gdp);\n\t\tif (inode_table == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table < first_block ||\n\t\t    inode_table + sbi->s_itb_per_group - 1 > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode table for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_table);\n\t\t\treturn 0;\n\t\t}\n\t\text4_lock_group(sb, i);\n\t\tif (!ext4_group_desc_csum_verify(sb, i, gdp)) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Checksum for group %u failed (%u!=%u)\",\n\t\t\t\t i, le16_to_cpu(ext4_group_desc_csum(sb, i,\n\t\t\t\t     gdp)), le16_to_cpu(gdp->bg_checksum));\n\t\t\tif (!sb_rdonly(sb)) {\n\t\t\t\text4_unlock_group(sb, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tif (!flexbg_flag)\n\t\t\tfirst_block += EXT4_BLOCKS_PER_GROUP(sb);\n\t}\n\tif (NULL != first_not_zeroed)\n\t\t*first_not_zeroed = grp;\n\treturn 1;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_check_descriptors(struct super_block *sb,\n\t\t\t\t  ext4_fsblk_t sb_block,\n\t\t\t\t  ext4_group_t *first_not_zeroed){\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t first_block = le32_to_cpu(sbi->s_es->s_first_data_block);\n\text4_fsblk_t last_block;\n\text4_fsblk_t last_bg_block = sb_block + ext4_bg_num_gdb(sb, 0) + 1;\n\text4_fsblk_t block_bitmap;\n\text4_fsblk_t inode_bitmap;\n\text4_fsblk_t inode_table;\n\tint flexbg_flag = 0;\n\text4_group_t i, grp = sbi->s_groups_count;\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tflexbg_flag = 1;\n\n\text4_debug(\"Checking group descriptors\");\n\n\tfor (i = 0; i < sbi->s_groups_count; i++) {\n\t\tstruct ext4_group_desc *gdp = ext4_get_group_desc(sb, i, NULL);\n\n\t\tif (i == sbi->s_groups_count - 1 || flexbg_flag)\n\t\t\tlast_block = ext4_blocks_count(sbi->s_es) - 1;\n\t\telse\n\t\t\tlast_block = first_block +\n\t\t\t\t(EXT4_BLOCKS_PER_GROUP(sb) - 1);\n\n\t\tif ((grp == sbi->s_groups_count) &&\n\t\t   !(gdp->bg_flags & cpu_to_le16(EXT4_BG_INODE_ZEROED)))\n\t\t\tgrp = i;\n\n\t\tblock_bitmap = ext4_block_bitmap(sb, gdp);\n\t\tif (block_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap >= sb_block + 1 &&\n\t\t    block_bitmap <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Block bitmap for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (block_bitmap < first_block || block_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Block bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, block_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_bitmap = ext4_inode_bitmap(sb, gdp);\n\t\tif (inode_bitmap == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap >= sb_block + 1 &&\n\t\t    inode_bitmap <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode bitmap for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_bitmap < first_block || inode_bitmap > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode bitmap for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_bitmap);\n\t\t\treturn 0;\n\t\t}\n\t\tinode_table = ext4_inode_table(sb, gdp);\n\t\tif (inode_table == sb_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"superblock\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table >= sb_block + 1 &&\n\t\t    inode_table <= last_bg_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Inode table for group %u overlaps \"\n\t\t\t\t \"block group descriptors\", i);\n\t\t\tif (!sb_rdonly(sb))\n\t\t\t\treturn 0;\n\t\t}\n\t\tif (inode_table < first_block ||\n\t\t    inode_table + sbi->s_itb_per_group - 1 > last_block) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t       \"Inode table for group %u not in group \"\n\t\t\t       \"(block %llu)!\", i, inode_table);\n\t\t\treturn 0;\n\t\t}\n\t\text4_lock_group(sb, i);\n\t\tif (!ext4_group_desc_csum_verify(sb, i, gdp)) {\n\t\t\text4_msg(sb, KERN_ERR, \"ext4_check_descriptors: \"\n\t\t\t\t \"Checksum for group %u failed (%u!=%u)\",\n\t\t\t\t i, le16_to_cpu(ext4_group_desc_csum(sb, i,\n\t\t\t\t     gdp)), le16_to_cpu(gdp->bg_checksum));\n\t\t\tif (!sb_rdonly(sb)) {\n\t\t\t\text4_unlock_group(sb, i);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\text4_unlock_group(sb, i);\n\t\tif (!flexbg_flag)\n\t\t\tfirst_block += EXT4_BLOCKS_PER_GROUP(sb);\n\t}\n\tif (NULL != first_not_zeroed)\n\t\t*first_not_zeroed = grp;\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 0,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_274": {
        "vulnerable_code": {
            "Code": "ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es){\n\tjournal_t *journal;\n\tint j_errno;\n\tconst char *errstr;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\t/*\n\t * Now check for any error status which may have been recorded in the\n\t * journal by a prior ext4_error() or ext4_abort()\n\t */\n\n\tj_errno = jbd2_journal_errno(journal);\n\tif (j_errno) {\n\t\tchar nbuf[16];\n\n\t\terrstr = ext4_decode_error(sb, j_errno, nbuf);\n\t\text4_warning(sb, \"Filesystem error recorded \"\n\t\t\t     \"from previous mount: %s\", errstr);\n\t\text4_warning(sb, \"Marking fs in need of filesystem check.\");\n\n\t\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\t\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\t\text4_commit_super(sb, 1);\n\n\t\tjbd2_journal_clear_err(journal);\n\t\tjbd2_journal_update_sb_errno(journal);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_clear_journal_err(struct super_block *sb,\n\t\t\t\t   struct ext4_super_block *es){\n\tjournal_t *journal;\n\tint j_errno;\n\tconst char *errstr;\n\n\tBUG_ON(!ext4_has_feature_journal(sb));\n\n\tjournal = EXT4_SB(sb)->s_journal;\n\n\t/*\n\t * Now check for any error status which may have been recorded in the\n\t * journal by a prior ext4_error() or ext4_abort()\n\t */\n\n\tj_errno = jbd2_journal_errno(journal);\n\tif (j_errno) {\n\t\tchar nbuf[16];\n\n\t\terrstr = ext4_decode_error(sb, j_errno, nbuf);\n\t\text4_warning(sb, \"Filesystem error recorded \"\n\t\t\t     \"from previous mount: %s\", errstr);\n\t\text4_warning(sb, \"Marking fs in need of filesystem check.\");\n\n\t\tEXT4_SB(sb)->s_mount_state |= EXT4_ERROR_FS;\n\t\tes->s_state |= cpu_to_le16(EXT4_ERROR_FS);\n\t\text4_commit_super(sb, 1);\n\n\t\tjbd2_journal_clear_err(journal);\n\t\tjbd2_journal_update_sb_errno(journal);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_275": {
        "vulnerable_code": {
            "Code": "ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t      char nbuf[16]){\n\tchar *errstr = NULL;\n\n\tswitch (errno) {\n\tcase -EFSCORRUPTED:\n\t\terrstr = \"Corrupt filesystem\";\n\t\tbreak;\n\tcase -EFSBADCRC:\n\t\terrstr = \"Filesystem failed CRC\";\n\t\tbreak;\n\tcase -EIO:\n\t\terrstr = \"IO failure\";\n\t\tbreak;\n\tcase -ENOMEM:\n\t\terrstr = \"Out of memory\";\n\t\tbreak;\n\tcase -EROFS:\n\t\tif (!sb || (EXT4_SB(sb)->s_journal &&\n\t\t\t    EXT4_SB(sb)->s_journal->j_flags & JBD2_ABORT))\n\t\t\terrstr = \"Journal has aborted\";\n\t\telse\n\t\t\terrstr = \"Readonly filesystem\";\n\t\tbreak;\n\tdefault:\n\t\t/* If the caller passed in an extra buffer for unknown\n\t\t * errors, textualise them now.  Else we just return\n\t\t * NULL. */\n\t\tif (nbuf) {\n\t\t\t/* Check for truncated error codes... */\n\t\t\tif (snprintf(nbuf, 16, \"error %d\", -errno) >= 0)\n\t\t\t\terrstr = nbuf;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn errstr;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_decode_error(struct super_block *sb, int errno,\n\t\t\t      char nbuf[16]){\n\tchar *errstr = NULL;\n\n\tswitch (errno) {\n\tcase -EFSCORRUPTED:\n\t\terrstr = \"Corrupt filesystem\";\n\t\tbreak;\n\tcase -EFSBADCRC:\n\t\terrstr = \"Filesystem failed CRC\";\n\t\tbreak;\n\tcase -EIO:\n\t\terrstr = \"IO failure\";\n\t\tbreak;\n\tcase -ENOMEM:\n\t\terrstr = \"Out of memory\";\n\t\tbreak;\n\tcase -EROFS:\n\t\tif (!sb || (EXT4_SB(sb)->s_journal &&\n\t\t\t    EXT4_SB(sb)->s_journal->j_flags & JBD2_ABORT))\n\t\t\terrstr = \"Journal has aborted\";\n\t\telse\n\t\t\terrstr = \"Readonly filesystem\";\n\t\tbreak;\n\tdefault:\n\t\t/* If the caller passed in an extra buffer for unknown\n\t\t * errors, textualise them now.  Else we just return\n\t\t * NULL. */\n\t\tif (nbuf) {\n\t\t\t/* Check for truncated error codes... */\n\t\t\tif (snprintf(nbuf, 16, \"error %d\", -errno) >= 0)\n\t\t\t\terrstr = nbuf;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn errstr;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_276": {
        "vulnerable_code": {
            "Code": "ext4_feature_set_ok(struct super_block *sb, int readonly){\n\tif (ext4_has_unknown_ext4_incompat_features(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Couldn't mount because of \"\n\t\t\t\"unsupported optional features (%x)\",\n\t\t\t(le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_incompat) &\n\t\t\t~EXT4_FEATURE_INCOMPAT_SUPP));\n\t\treturn 0;\n\t}\n\n\tif (readonly)\n\t\treturn 1;\n\n\tif (ext4_has_feature_readonly(sb)) {\n\t\text4_msg(sb, KERN_INFO, \"filesystem is read-only\");\n\t\tsb->s_flags |= SB_RDONLY;\n\t\treturn 1;\n\t}\n\n\t/* Check that feature set is OK for a read-write mount */\n\tif (ext4_has_unknown_ext4_ro_compat_features(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't mount RDWR because of \"\n\t\t\t \"unsupported optional features (%x)\",\n\t\t\t (le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_ro_compat) &\n\t\t\t\t~EXT4_FEATURE_RO_COMPAT_SUPP));\n\t\treturn 0;\n\t}\n\t/*\n\t * Large file size enabled file system can only be mounted\n\t * read-write on 32-bit systems if kernel is built with CONFIG_LBDAF\n\t */\n\tif (ext4_has_feature_huge_file(sb)) {\n\t\tif (sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Filesystem with huge files \"\n\t\t\t\t \"cannot be mounted RDWR without \"\n\t\t\t\t \"CONFIG_LBDAF\");\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (ext4_has_feature_bigalloc(sb) && !ext4_has_feature_extents(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Can't support bigalloc feature without \"\n\t\t\t \"extents feature\\n\");\n\t\treturn 0;\n\t}\n\n#ifndef CONFIG_QUOTA\n\tif (ext4_has_feature_quota(sb) && !readonly) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with quota feature cannot be mounted RDWR \"\n\t\t\t \"without CONFIG_QUOTA\");\n\t\treturn 0;\n\t}\n\tif (ext4_has_feature_project(sb) && !readonly) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with project quota feature cannot be mounted RDWR \"\n\t\t\t \"without CONFIG_QUOTA\");\n\t\treturn 0;\n\t}\n#endif  /* CONFIG_QUOTA */\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_feature_set_ok(struct super_block *sb, int readonly){\n\tif (ext4_has_unknown_ext4_incompat_features(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t\"Couldn't mount because of \"\n\t\t\t\"unsupported optional features (%x)\",\n\t\t\t(le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_incompat) &\n\t\t\t~EXT4_FEATURE_INCOMPAT_SUPP));\n\t\treturn 0;\n\t}\n\n\tif (readonly)\n\t\treturn 1;\n\n\tif (ext4_has_feature_readonly(sb)) {\n\t\text4_msg(sb, KERN_INFO, \"filesystem is read-only\");\n\t\tsb->s_flags |= SB_RDONLY;\n\t\treturn 1;\n\t}\n\n\t/* Check that feature set is OK for a read-write mount */\n\tif (ext4_has_unknown_ext4_ro_compat_features(sb)) {\n\t\text4_msg(sb, KERN_ERR, \"couldn't mount RDWR because of \"\n\t\t\t \"unsupported optional features (%x)\",\n\t\t\t (le32_to_cpu(EXT4_SB(sb)->s_es->s_feature_ro_compat) &\n\t\t\t\t~EXT4_FEATURE_RO_COMPAT_SUPP));\n\t\treturn 0;\n\t}\n\t/*\n\t * Large file size enabled file system can only be mounted\n\t * read-write on 32-bit systems if kernel is built with CONFIG_LBDAF\n\t */\n\tif (ext4_has_feature_huge_file(sb)) {\n\t\tif (sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t\text4_msg(sb, KERN_ERR, \"Filesystem with huge files \"\n\t\t\t\t \"cannot be mounted RDWR without \"\n\t\t\t\t \"CONFIG_LBDAF\");\n\t\t\treturn 0;\n\t\t}\n\t}\n\tif (ext4_has_feature_bigalloc(sb) && !ext4_has_feature_extents(sb)) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Can't support bigalloc feature without \"\n\t\t\t \"extents feature\\n\");\n\t\treturn 0;\n\t}\n\n#ifndef CONFIG_QUOTA\n\tif (ext4_has_feature_quota(sb) && !readonly) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with quota feature cannot be mounted RDWR \"\n\t\t\t \"without CONFIG_QUOTA\");\n\t\treturn 0;\n\t}\n\tif (ext4_has_feature_project(sb) && !readonly) {\n\t\text4_msg(sb, KERN_ERR,\n\t\t\t \"Filesystem with project quota feature cannot be mounted RDWR \"\n\t\t\t \"without CONFIG_QUOTA\");\n\t\treturn 0;\n\t}\n#endif  /* CONFIG_QUOTA */\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_277": {
        "vulnerable_code": {
            "Code": "ext4_group_desc_csum(struct super_block *sb, __u32 block_group,\n\t\t\t\t   struct ext4_group_desc *gdp){\n\tint offset = offsetof(struct ext4_group_desc, bg_checksum);\n\t__u16 crc = 0;\n\t__le32 le_group = cpu_to_le32(block_group);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (ext4_has_metadata_csum(sbi->s_sb)) {\n\t\t/* Use new metadata_csum algorithm */\n\t\t__u32 csum32;\n\t\t__u16 dummy_csum = 0;\n\n\t\tcsum32 = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&le_group,\n\t\t\t\t     sizeof(le_group));\n\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)gdp, offset);\n\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)&dummy_csum,\n\t\t\t\t     sizeof(dummy_csum));\n\t\toffset += sizeof(dummy_csum);\n\t\tif (offset < sbi->s_desc_size)\n\t\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)gdp + offset,\n\t\t\t\t\t     sbi->s_desc_size - offset);\n\n\t\tcrc = csum32 & 0xFFFF;\n\t\tgoto out;\n\t}\n\n\t/* old crc16 code */\n\tif (!ext4_has_feature_gdt_csum(sb))\n\t\treturn 0;\n\n\tcrc = crc16(~0, sbi->s_es->s_uuid, sizeof(sbi->s_es->s_uuid));\n\tcrc = crc16(crc, (__u8 *)&le_group, sizeof(le_group));\n\tcrc = crc16(crc, (__u8 *)gdp, offset);\n\toffset += sizeof(gdp->bg_checksum); /* skip checksum */\n\t/* for checksum of struct ext4_group_desc do the rest...*/\n\tif (ext4_has_feature_64bit(sb) &&\n\t    offset < le16_to_cpu(sbi->s_es->s_desc_size))\n\t\tcrc = crc16(crc, (__u8 *)gdp + offset,\n\t\t\t    le16_to_cpu(sbi->s_es->s_desc_size) -\n\t\t\t\toffset);\n\nout:\n\treturn cpu_to_le16(crc);\n}",
            "Size": 3,
            "Code Complexity": 1,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_group_desc_csum(struct super_block *sb, __u32 block_group,\n\t\t\t\t   struct ext4_group_desc *gdp){\n\tint offset = offsetof(struct ext4_group_desc, bg_checksum);\n\t__u16 crc = 0;\n\t__le32 le_group = cpu_to_le32(block_group);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\n\tif (ext4_has_metadata_csum(sbi->s_sb)) {\n\t\t/* Use new metadata_csum algorithm */\n\t\t__u32 csum32;\n\t\t__u16 dummy_csum = 0;\n\n\t\tcsum32 = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&le_group,\n\t\t\t\t     sizeof(le_group));\n\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)gdp, offset);\n\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)&dummy_csum,\n\t\t\t\t     sizeof(dummy_csum));\n\t\toffset += sizeof(dummy_csum);\n\t\tif (offset < sbi->s_desc_size)\n\t\t\tcsum32 = ext4_chksum(sbi, csum32, (__u8 *)gdp + offset,\n\t\t\t\t\t     sbi->s_desc_size - offset);\n\n\t\tcrc = csum32 & 0xFFFF;\n\t\tgoto out;\n\t}\n\n\t/* old crc16 code */\n\tif (!ext4_has_feature_gdt_csum(sb))\n\t\treturn 0;\n\n\tcrc = crc16(~0, sbi->s_es->s_uuid, sizeof(sbi->s_es->s_uuid));\n\tcrc = crc16(crc, (__u8 *)&le_group, sizeof(le_group));\n\tcrc = crc16(crc, (__u8 *)gdp, offset);\n\toffset += sizeof(gdp->bg_checksum); /* skip checksum */\n\t/* for checksum of struct ext4_group_desc do the rest...*/\n\tif (ext4_has_feature_64bit(sb) &&\n\t    offset < le16_to_cpu(sbi->s_es->s_desc_size))\n\t\tcrc = crc16(crc, (__u8 *)gdp + offset,\n\t\t\t    le16_to_cpu(sbi->s_es->s_desc_size) -\n\t\t\t\toffset);\n\nout:\n\treturn cpu_to_le16(crc);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_278": {
        "vulnerable_code": {
            "Code": "ext4_group_desc_csum_set(struct super_block *sb, __u32 block_group,\n\t\t\t      struct ext4_group_desc *gdp){\n\tif (!ext4_has_group_desc_csum(sb))\n\t\treturn;\n\tgdp->bg_checksum = ext4_group_desc_csum(sb, block_group, gdp);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_group_desc_csum_set(struct super_block *sb, __u32 block_group,\n\t\t\t      struct ext4_group_desc *gdp){\n\tif (!ext4_has_group_desc_csum(sb))\n\t\treturn;\n\tgdp->bg_checksum = ext4_group_desc_csum(sb, block_group, gdp);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_279": {
        "vulnerable_code": {
            "Code": "ext4_inode_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg){\n\treturn le32_to_cpu(bg->bg_inode_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_bitmap_hi) << 32 : 0);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_inode_bitmap(struct super_block *sb,\n\t\t\t       struct ext4_group_desc *bg){\n\treturn le32_to_cpu(bg->bg_inode_bitmap_lo) |\n\t\t(EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT ?\n\t\t (ext4_fsblk_t)le32_to_cpu(bg->bg_inode_bitmap_hi) << 32 : 0);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_280": {
        "vulnerable_code": {
            "Code": "ext4_itable_unused_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count){\n\tbg->bg_itable_unused_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_itable_unused_hi = cpu_to_le16(count >> 16);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_itable_unused_set(struct super_block *sb,\n\t\t\t  struct ext4_group_desc *bg, __u32 count){\n\tbg->bg_itable_unused_lo = cpu_to_le16((__u16)count);\n\tif (EXT4_DESC_SIZE(sb) >= EXT4_MIN_DESC_SIZE_64BIT)\n\t\tbg->bg_itable_unused_hi = cpu_to_le16(count >> 16);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_281": {
        "vulnerable_code": {
            "Code": "ext4_li_request_new(struct super_block *sb,\n\t\t\t\t\t    ext4_group_t start){\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_li_request *elr;\n\n\telr = kzalloc(sizeof(*elr), GFP_KERNEL);\n\tif (!elr)\n\t\treturn NULL;\n\n\telr->lr_super = sb;\n\telr->lr_sbi = sbi;\n\telr->lr_next_group = start;\n\n\t/*\n\t * Randomize first schedule time of the request to\n\t * spread the inode table initialization requests\n\t * better.\n\t */\n\telr->lr_next_sched = jiffies + (prandom_u32() %\n\t\t\t\t(EXT4_DEF_LI_MAX_START_DELAY * HZ));\n\treturn elr;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_li_request_new(struct super_block *sb,\n\t\t\t\t\t    ext4_group_t start){\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_li_request *elr;\n\n\telr = kzalloc(sizeof(*elr), GFP_KERNEL);\n\tif (!elr)\n\t\treturn NULL;\n\n\telr->lr_super = sb;\n\telr->lr_sbi = sbi;\n\telr->lr_next_group = start;\n\n\t/*\n\t * Randomize first schedule time of the request to\n\t * spread the inode table initialization requests\n\t * better.\n\t */\n\telr->lr_next_sched = jiffies + (prandom_u32() %\n\t\t\t\t(EXT4_DEF_LI_MAX_START_DELAY * HZ));\n\treturn elr;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_282": {
        "vulnerable_code": {
            "Code": "ext4_max_size(int blkbits, int has_huge_files){\n\tloff_t res;\n\tloff_t upper_limit = MAX_LFS_FILESIZE;\n\n\t/* small i_blocks in vfs inode? */\n\tif (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t/*\n\t\t * CONFIG_LBDAF is not enabled implies the inode\n\t\t * i_block represent total blocks in 512 bytes\n\t\t * 32 == size of vfs inode i_blocks * 8\n\t\t */\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (blkbits - 9);\n\t\tupper_limit <<= blkbits;\n\t}\n\n\t/*\n\t * 32-bit extent-start container, ee_block. We lower the maxbytes\n\t * by one fs block, so ee_len can cover the extent of maximum file\n\t * size\n\t */\n\tres = (1LL << 32) - 1;\n\tres <<= blkbits;\n\n\t/* Sanity check against vm- & vfs- imposed limits */\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\treturn res;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_max_size(int blkbits, int has_huge_files){\n\tloff_t res;\n\tloff_t upper_limit = MAX_LFS_FILESIZE;\n\n\t/* small i_blocks in vfs inode? */\n\tif (!has_huge_files || sizeof(blkcnt_t) < sizeof(u64)) {\n\t\t/*\n\t\t * CONFIG_LBDAF is not enabled implies the inode\n\t\t * i_block represent total blocks in 512 bytes\n\t\t * 32 == size of vfs inode i_blocks * 8\n\t\t */\n\t\tupper_limit = (1LL << 32) - 1;\n\n\t\t/* total blocks in file system block size */\n\t\tupper_limit >>= (blkbits - 9);\n\t\tupper_limit <<= blkbits;\n\t}\n\n\t/*\n\t * 32-bit extent-start container, ee_block. We lower the maxbytes\n\t * by one fs block, so ee_len can cover the extent of maximum file\n\t * size\n\t */\n\tres = (1LL << 32) - 1;\n\tres <<= blkbits;\n\n\t/* Sanity check against vm- & vfs- imposed limits */\n\tif (res > upper_limit)\n\t\tres = upper_limit;\n\n\treturn res;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_283": {
        "vulnerable_code": {
            "Code": "ext4_orphan_cleanup(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es){\n\tunsigned int s_flags = sb->s_flags;\n\tint ret, nr_orphans = 0, nr_truncates = 0;\n#ifdef CONFIG_QUOTA\n\tint quota_update = 0;\n\tint i;\n#endif\n\tif (!es->s_last_orphan) {\n\t\tjbd_debug(4, \"no orphan inodes to clean up\\n\");\n\t\treturn;\n\t}\n\n\tif (bdev_read_only(sb->s_bdev)) {\n\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\"unavailable, skipping orphan cleanup\");\n\t\treturn;\n\t}\n\n\t/* Check if feature set would not allow a r/w mount */\n\tif (!ext4_feature_set_ok(sb, 0)) {\n\t\text4_msg(sb, KERN_INFO, \"Skipping orphan cleanup due to \"\n\t\t\t \"unknown ROCOMPAT features\");\n\t\treturn;\n\t}\n\n\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\t/* don't clear list on RO mount w/ errors */\n\t\tif (es->s_last_orphan && !(s_flags & SB_RDONLY)) {\n\t\t\text4_msg(sb, KERN_INFO, \"Errors on filesystem, \"\n\t\t\t\t  \"clearing orphan list.\\n\");\n\t\t\tes->s_last_orphan = 0;\n\t\t}\n\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\treturn;\n\t}\n\n\tif (s_flags & SB_RDONLY) {\n\t\text4_msg(sb, KERN_INFO, \"orphan cleanup on readonly fs\");\n\t\tsb->s_flags &= ~SB_RDONLY;\n\t}\n#ifdef CONFIG_QUOTA\n\t/* Needed for iput() to work correctly and not trash data */\n\tsb->s_flags |= SB_ACTIVE;\n\n\t/*\n\t * Turn on quotas which were not enabled for read-only mounts if\n\t * filesystem has quota feature, so that they are updated correctly.\n\t */\n\tif (ext4_has_feature_quota(sb) && (s_flags & SB_RDONLY)) {\n\t\tint ret = ext4_enable_quotas(sb);\n\n\t\tif (!ret)\n\t\t\tquota_update = 1;\n\t\telse\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"Cannot turn on quotas: error %d\", ret);\n\t}\n\n\t/* Turn on journaled quotas used for old sytle */\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\tif (EXT4_SB(sb)->s_qf_names[i]) {\n\t\t\tint ret = ext4_quota_on_mount(sb, i);\n\n\t\t\tif (!ret)\n\t\t\t\tquota_update = 1;\n\t\t\telse\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"Cannot turn on journaled \"\n\t\t\t\t\t\"quota: type %d: error %d\", i, ret);\n\t\t}\n\t}\n#endif\n\n\twhile (es->s_last_orphan) {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * We may have encountered an error during cleanup; if\n\t\t * so, skip the rest.\n\t\t */\n\t\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tinode = ext4_orphan_get(sb, le32_to_cpu(es->s_last_orphan));\n\t\tif (IS_ERR(inode)) {\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add(&EXT4_I(inode)->i_orphan, &EXT4_SB(sb)->s_orphan);\n\t\tdquot_initialize(inode);\n\t\tif (inode->i_nlink) {\n\t\t\tif (test_opt(sb, DEBUG))\n\t\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\t\"%s: truncating inode %lu to %lld bytes\",\n\t\t\t\t\t__func__, inode->i_ino, inode->i_size);\n\t\t\tjbd_debug(2, \"truncating inode %lu to %lld bytes\\n\",\n\t\t\t\t  inode->i_ino, inode->i_size);\n\t\t\tinode_lock(inode);\n\t\t\ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n\t\t\tret = ext4_truncate(inode);\n\t\t\tif (ret)\n\t\t\t\text4_std_error(inode->i_sb, ret);\n\t\t\tinode_unlock(inode);\n\t\t\tnr_truncates++;\n\t\t} else {\n\t\t\tif (test_opt(sb, DEBUG))\n\t\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\t\"%s: deleting unreferenced inode %lu\",\n\t\t\t\t\t__func__, inode->i_ino);\n\t\t\tjbd_debug(2, \"deleting unreferenced inode %lu\\n\",\n\t\t\t\t  inode->i_ino);\n\t\t\tnr_orphans++;\n\t\t}\n\t\tiput(inode);  /* The delete magic happens here! */\n\t}\n\n#define PLURAL(x) (x), ((x) == 1) ? \"\" : \"s\"\n\n\tif (nr_orphans)\n\t\text4_msg(sb, KERN_INFO, \"%d orphan inode%s deleted\",\n\t\t       PLURAL(nr_orphans));\n\tif (nr_truncates)\n\t\text4_msg(sb, KERN_INFO, \"%d truncate%s cleaned up\",\n\t\t       PLURAL(nr_truncates));\n#ifdef CONFIG_QUOTA\n\t/* Turn off quotas if they were enabled for orphan cleanup */\n\tif (quota_update) {\n\t\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\t\tif (sb_dqopt(sb)->files[i])\n\t\t\t\tdquot_quota_off(sb, i);\n\t\t}\n\t}\n#endif\n\tsb->s_flags = s_flags; /* Restore SB_RDONLY status */\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_orphan_cleanup(struct super_block *sb,\n\t\t\t\tstruct ext4_super_block *es){\n\tunsigned int s_flags = sb->s_flags;\n\tint ret, nr_orphans = 0, nr_truncates = 0;\n#ifdef CONFIG_QUOTA\n\tint quota_update = 0;\n\tint i;\n#endif\n\tif (!es->s_last_orphan) {\n\t\tjbd_debug(4, \"no orphan inodes to clean up\\n\");\n\t\treturn;\n\t}\n\n\tif (bdev_read_only(sb->s_bdev)) {\n\t\text4_msg(sb, KERN_ERR, \"write access \"\n\t\t\t\"unavailable, skipping orphan cleanup\");\n\t\treturn;\n\t}\n\n\t/* Check if feature set would not allow a r/w mount */\n\tif (!ext4_feature_set_ok(sb, 0)) {\n\t\text4_msg(sb, KERN_INFO, \"Skipping orphan cleanup due to \"\n\t\t\t \"unknown ROCOMPAT features\");\n\t\treturn;\n\t}\n\n\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\t/* don't clear list on RO mount w/ errors */\n\t\tif (es->s_last_orphan && !(s_flags & SB_RDONLY)) {\n\t\t\text4_msg(sb, KERN_INFO, \"Errors on filesystem, \"\n\t\t\t\t  \"clearing orphan list.\\n\");\n\t\t\tes->s_last_orphan = 0;\n\t\t}\n\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\treturn;\n\t}\n\n\tif (s_flags & SB_RDONLY) {\n\t\text4_msg(sb, KERN_INFO, \"orphan cleanup on readonly fs\");\n\t\tsb->s_flags &= ~SB_RDONLY;\n\t}\n#ifdef CONFIG_QUOTA\n\t/* Needed for iput() to work correctly and not trash data */\n\tsb->s_flags |= SB_ACTIVE;\n\n\t/*\n\t * Turn on quotas which were not enabled for read-only mounts if\n\t * filesystem has quota feature, so that they are updated correctly.\n\t */\n\tif (ext4_has_feature_quota(sb) && (s_flags & SB_RDONLY)) {\n\t\tint ret = ext4_enable_quotas(sb);\n\n\t\tif (!ret)\n\t\t\tquota_update = 1;\n\t\telse\n\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\"Cannot turn on quotas: error %d\", ret);\n\t}\n\n\t/* Turn on journaled quotas used for old sytle */\n\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\tif (EXT4_SB(sb)->s_qf_names[i]) {\n\t\t\tint ret = ext4_quota_on_mount(sb, i);\n\n\t\t\tif (!ret)\n\t\t\t\tquota_update = 1;\n\t\t\telse\n\t\t\t\text4_msg(sb, KERN_ERR,\n\t\t\t\t\t\"Cannot turn on journaled \"\n\t\t\t\t\t\"quota: type %d: error %d\", i, ret);\n\t\t}\n\t}\n#endif\n\n\twhile (es->s_last_orphan) {\n\t\tstruct inode *inode;\n\n\t\t/*\n\t\t * We may have encountered an error during cleanup; if\n\t\t * so, skip the rest.\n\t\t */\n\t\tif (EXT4_SB(sb)->s_mount_state & EXT4_ERROR_FS) {\n\t\t\tjbd_debug(1, \"Skipping orphan recovery on fs with errors.\\n\");\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tinode = ext4_orphan_get(sb, le32_to_cpu(es->s_last_orphan));\n\t\tif (IS_ERR(inode)) {\n\t\t\tes->s_last_orphan = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tlist_add(&EXT4_I(inode)->i_orphan, &EXT4_SB(sb)->s_orphan);\n\t\tdquot_initialize(inode);\n\t\tif (inode->i_nlink) {\n\t\t\tif (test_opt(sb, DEBUG))\n\t\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\t\"%s: truncating inode %lu to %lld bytes\",\n\t\t\t\t\t__func__, inode->i_ino, inode->i_size);\n\t\t\tjbd_debug(2, \"truncating inode %lu to %lld bytes\\n\",\n\t\t\t\t  inode->i_ino, inode->i_size);\n\t\t\tinode_lock(inode);\n\t\t\ttruncate_inode_pages(inode->i_mapping, inode->i_size);\n\t\t\tret = ext4_truncate(inode);\n\t\t\tif (ret)\n\t\t\t\text4_std_error(inode->i_sb, ret);\n\t\t\tinode_unlock(inode);\n\t\t\tnr_truncates++;\n\t\t} else {\n\t\t\tif (test_opt(sb, DEBUG))\n\t\t\t\text4_msg(sb, KERN_DEBUG,\n\t\t\t\t\t\"%s: deleting unreferenced inode %lu\",\n\t\t\t\t\t__func__, inode->i_ino);\n\t\t\tjbd_debug(2, \"deleting unreferenced inode %lu\\n\",\n\t\t\t\t  inode->i_ino);\n\t\t\tnr_orphans++;\n\t\t}\n\t\tiput(inode);  /* The delete magic happens here! */\n\t}\n\n#define PLURAL(x) (x), ((x) == 1) ? \"\" : \"s\"\n\n\tif (nr_orphans)\n\t\text4_msg(sb, KERN_INFO, \"%d orphan inode%s deleted\",\n\t\t       PLURAL(nr_orphans));\n\tif (nr_truncates)\n\t\text4_msg(sb, KERN_INFO, \"%d truncate%s cleaned up\",\n\t\t       PLURAL(nr_truncates));\n#ifdef CONFIG_QUOTA\n\t/* Turn off quotas if they were enabled for orphan cleanup */\n\tif (quota_update) {\n\t\tfor (i = 0; i < EXT4_MAXQUOTAS; i++) {\n\t\t\tif (sb_dqopt(sb)->files[i])\n\t\t\t\tdquot_quota_off(sb, i);\n\t\t}\n\t}\n#endif\n\tsb->s_flags = s_flags; /* Restore SB_RDONLY status */\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_284": {
        "vulnerable_code": {
            "Code": "print_daily_error_info(struct timer_list *t){\n\tstruct ext4_sb_info *sbi = from_timer(sbi, t, s_err_report);\n\tstruct super_block *sb = sbi->s_sb;\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\tif (es->s_error_count)\n\t\t/* fsck newer than v1.41.13 is needed to clean this condition. */\n\t\text4_msg(sb, KERN_NOTICE, \"error count since last fsck: %u\",\n\t\t\t le32_to_cpu(es->s_error_count));\n\tif (es->s_first_error_time) {\n\t\tprintk(KERN_NOTICE \"EXT4-fs (%s): initial error at time %u: %.*s:%d\",\n\t\t       sb->s_id, le32_to_cpu(es->s_first_error_time),\n\t\t       (int) sizeof(es->s_first_error_func),\n\t\t       es->s_first_error_func,\n\t\t       le32_to_cpu(es->s_first_error_line));\n\t\tif (es->s_first_error_ino)\n\t\t\tprintk(KERN_CONT \": inode %u\",\n\t\t\t       le32_to_cpu(es->s_first_error_ino));\n\t\tif (es->s_first_error_block)\n\t\t\tprintk(KERN_CONT \": block %llu\", (unsigned long long)\n\t\t\t       le64_to_cpu(es->s_first_error_block));\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tif (es->s_last_error_time) {\n\t\tprintk(KERN_NOTICE \"EXT4-fs (%s): last error at time %u: %.*s:%d\",\n\t\t       sb->s_id, le32_to_cpu(es->s_last_error_time),\n\t\t       (int) sizeof(es->s_last_error_func),\n\t\t       es->s_last_error_func,\n\t\t       le32_to_cpu(es->s_last_error_line));\n\t\tif (es->s_last_error_ino)\n\t\t\tprintk(KERN_CONT \": inode %u\",\n\t\t\t       le32_to_cpu(es->s_last_error_ino));\n\t\tif (es->s_last_error_block)\n\t\t\tprintk(KERN_CONT \": block %llu\", (unsigned long long)\n\t\t\t       le64_to_cpu(es->s_last_error_block));\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tmod_timer(&sbi->s_err_report, jiffies + 24*60*60*HZ);  /* Once a day */\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "print_daily_error_info(struct timer_list *t){\n\tstruct ext4_sb_info *sbi = from_timer(sbi, t, s_err_report);\n\tstruct super_block *sb = sbi->s_sb;\n\tstruct ext4_super_block *es = sbi->s_es;\n\n\tif (es->s_error_count)\n\t\t/* fsck newer than v1.41.13 is needed to clean this condition. */\n\t\text4_msg(sb, KERN_NOTICE, \"error count since last fsck: %u\",\n\t\t\t le32_to_cpu(es->s_error_count));\n\tif (es->s_first_error_time) {\n\t\tprintk(KERN_NOTICE \"EXT4-fs (%s): initial error at time %u: %.*s:%d\",\n\t\t       sb->s_id, le32_to_cpu(es->s_first_error_time),\n\t\t       (int) sizeof(es->s_first_error_func),\n\t\t       es->s_first_error_func,\n\t\t       le32_to_cpu(es->s_first_error_line));\n\t\tif (es->s_first_error_ino)\n\t\t\tprintk(KERN_CONT \": inode %u\",\n\t\t\t       le32_to_cpu(es->s_first_error_ino));\n\t\tif (es->s_first_error_block)\n\t\t\tprintk(KERN_CONT \": block %llu\", (unsigned long long)\n\t\t\t       le64_to_cpu(es->s_first_error_block));\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tif (es->s_last_error_time) {\n\t\tprintk(KERN_NOTICE \"EXT4-fs (%s): last error at time %u: %.*s:%d\",\n\t\t       sb->s_id, le32_to_cpu(es->s_last_error_time),\n\t\t       (int) sizeof(es->s_last_error_func),\n\t\t       es->s_last_error_func,\n\t\t       le32_to_cpu(es->s_last_error_line));\n\t\tif (es->s_last_error_ino)\n\t\t\tprintk(KERN_CONT \": inode %u\",\n\t\t\t       le32_to_cpu(es->s_last_error_ino));\n\t\tif (es->s_last_error_block)\n\t\t\tprintk(KERN_CONT \": block %llu\", (unsigned long long)\n\t\t\t       le64_to_cpu(es->s_last_error_block));\n\t\tprintk(KERN_CONT \"\\n\");\n\t}\n\tmod_timer(&sbi->s_err_report, jiffies + 24*60*60*HZ);  /* Once a day */\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_285": {
        "vulnerable_code": {
            "Code": "ext4_expand_inode_array(struct ext4_xattr_inode_array **ea_inode_array,\n\t\t\tstruct inode *inode){\n\tif (*ea_inode_array == NULL) {\n\t\t/*\n\t\t * Start with 15 inodes, so it fits into a power-of-two size.\n\t\t * If *ea_inode_array is NULL, this is essentially offsetof()\n\t\t */\n\t\t(*ea_inode_array) =\n\t\t\tkmalloc(offsetof(struct ext4_xattr_inode_array,\n\t\t\t\t\t inodes[EIA_MASK]),\n\t\t\t\tGFP_NOFS);\n\t\tif (*ea_inode_array == NULL)\n\t\t\treturn -ENOMEM;\n\t\t(*ea_inode_array)->count = 0;\n\t} else if (((*ea_inode_array)->count & EIA_MASK) == EIA_MASK) {\n\t\t/* expand the array once all 15 + n * 16 slots are full */\n\t\tstruct ext4_xattr_inode_array *new_array = NULL;\n\t\tint count = (*ea_inode_array)->count;\n\n\t\t/* if new_array is NULL, this is essentially offsetof() */\n\t\tnew_array = kmalloc(\n\t\t\t\toffsetof(struct ext4_xattr_inode_array,\n\t\t\t\t\t inodes[count + EIA_INCR]),\n\t\t\t\tGFP_NOFS);\n\t\tif (new_array == NULL)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(new_array, *ea_inode_array,\n\t\t       offsetof(struct ext4_xattr_inode_array, inodes[count]));\n\t\tkfree(*ea_inode_array);\n\t\t*ea_inode_array = new_array;\n\t}\n\t(*ea_inode_array)->inodes[(*ea_inode_array)->count++] = inode;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_expand_inode_array(struct ext4_xattr_inode_array **ea_inode_array,\n\t\t\tstruct inode *inode){\n\tif (*ea_inode_array == NULL) {\n\t\t/*\n\t\t * Start with 15 inodes, so it fits into a power-of-two size.\n\t\t * If *ea_inode_array is NULL, this is essentially offsetof()\n\t\t */\n\t\t(*ea_inode_array) =\n\t\t\tkmalloc(offsetof(struct ext4_xattr_inode_array,\n\t\t\t\t\t inodes[EIA_MASK]),\n\t\t\t\tGFP_NOFS);\n\t\tif (*ea_inode_array == NULL)\n\t\t\treturn -ENOMEM;\n\t\t(*ea_inode_array)->count = 0;\n\t} else if (((*ea_inode_array)->count & EIA_MASK) == EIA_MASK) {\n\t\t/* expand the array once all 15 + n * 16 slots are full */\n\t\tstruct ext4_xattr_inode_array *new_array = NULL;\n\t\tint count = (*ea_inode_array)->count;\n\n\t\t/* if new_array is NULL, this is essentially offsetof() */\n\t\tnew_array = kmalloc(\n\t\t\t\toffsetof(struct ext4_xattr_inode_array,\n\t\t\t\t\t inodes[count + EIA_INCR]),\n\t\t\t\tGFP_NOFS);\n\t\tif (new_array == NULL)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(new_array, *ea_inode_array,\n\t\t       offsetof(struct ext4_xattr_inode_array, inodes[count]));\n\t\tkfree(*ea_inode_array);\n\t\t*ea_inode_array = new_array;\n\t}\n\t(*ea_inode_array)->inodes[(*ea_inode_array)->count++] = inode;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_286": {
        "vulnerable_code": {
            "Code": "ext4_get_inode_usage(struct inode *inode, qsize_t *usage){\n\tstruct ext4_iloc iloc = { .bh = NULL };\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_entry *entry;\n\tqsize_t ea_inode_refs = 0;\n\tvoid *end;\n\tint ret;\n\n\tlockdep_assert_held_read(&EXT4_I(inode)->xattr_sem);\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tret = ext4_get_inode_loc(inode, &iloc);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\traw_inode = ext4_raw_inode(&iloc);\n\t\theader = IHDR(inode, raw_inode);\n\t\tend = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\t\tret = xattr_check_inode(inode, header, end);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tfor (entry = IFIRST(header); !IS_LAST_ENTRY(entry);\n\t\t     entry = EXT4_XATTR_NEXT(entry))\n\t\t\tif (entry->e_value_inum)\n\t\t\t\tea_inode_refs++;\n\t}\n\n\tif (EXT4_I(inode)->i_file_acl) {\n\t\tbh = sb_bread(inode->i_sb, EXT4_I(inode)->i_file_acl);\n\t\tif (!bh) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = ext4_xattr_check_block(inode, bh);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tfor (entry = BFIRST(bh); !IS_LAST_ENTRY(entry);\n\t\t     entry = EXT4_XATTR_NEXT(entry))\n\t\t\tif (entry->e_value_inum)\n\t\t\t\tea_inode_refs++;\n\t}\n\t*usage = ea_inode_refs + 1;\n\tret = 0;\nout:\n\tbrelse(iloc.bh);\n\tbrelse(bh);\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_get_inode_usage(struct inode *inode, qsize_t *usage){\n\tstruct ext4_iloc iloc = { .bh = NULL };\n\tstruct buffer_head *bh = NULL;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_xattr_entry *entry;\n\tqsize_t ea_inode_refs = 0;\n\tvoid *end;\n\tint ret;\n\n\tlockdep_assert_held_read(&EXT4_I(inode)->xattr_sem);\n\n\tif (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {\n\t\tret = ext4_get_inode_loc(inode, &iloc);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\traw_inode = ext4_raw_inode(&iloc);\n\t\theader = IHDR(inode, raw_inode);\n\t\tend = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\t\tret = xattr_check_inode(inode, header, end);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tfor (entry = IFIRST(header); !IS_LAST_ENTRY(entry);\n\t\t     entry = EXT4_XATTR_NEXT(entry))\n\t\t\tif (entry->e_value_inum)\n\t\t\t\tea_inode_refs++;\n\t}\n\n\tif (EXT4_I(inode)->i_file_acl) {\n\t\tbh = sb_bread(inode->i_sb, EXT4_I(inode)->i_file_acl);\n\t\tif (!bh) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = ext4_xattr_check_block(inode, bh);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tfor (entry = BFIRST(bh); !IS_LAST_ENTRY(entry);\n\t\t     entry = EXT4_XATTR_NEXT(entry))\n\t\t\tif (entry->e_value_inum)\n\t\t\t\tea_inode_refs++;\n\t}\n\t*usage = ea_inode_refs + 1;\n\tret = 0;\nout:\n\tbrelse(iloc.bh);\n\tbrelse(bh);\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_287": {
        "vulnerable_code": {
            "Code": "ext4_listxattr(struct dentry *dentry, char *buffer, size_t buffer_size){\n\tint ret, ret2;\n\n\tdown_read(&EXT4_I(d_inode(dentry))->xattr_sem);\n\tret = ret2 = ext4_xattr_ibody_list(dentry, buffer, buffer_size);\n\tif (ret < 0)\n\t\tgoto errout;\n\tif (buffer) {\n\t\tbuffer += ret;\n\t\tbuffer_size -= ret;\n\t}\n\tret = ext4_xattr_block_list(dentry, buffer, buffer_size);\n\tif (ret < 0)\n\t\tgoto errout;\n\tret += ret2;\nerrout:\n\tup_read(&EXT4_I(d_inode(dentry))->xattr_sem);\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_listxattr(struct dentry *dentry, char *buffer, size_t buffer_size){\n\tint ret, ret2;\n\n\tdown_read(&EXT4_I(d_inode(dentry))->xattr_sem);\n\tret = ret2 = ext4_xattr_ibody_list(dentry, buffer, buffer_size);\n\tif (ret < 0)\n\t\tgoto errout;\n\tif (buffer) {\n\t\tbuffer += ret;\n\t\tbuffer_size -= ret;\n\t}\n\tret = ext4_xattr_block_list(dentry, buffer, buffer_size);\n\tif (ret < 0)\n\t\tgoto errout;\n\tret += ret2;\nerrout:\n\tup_read(&EXT4_I(d_inode(dentry))->xattr_sem);\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_288": {
        "vulnerable_code": {
            "Code": "ext4_xattr_block_csum(struct inode *inode,\n\t\t\t\t    sector_t block_nr,\n\t\t\t\t    struct ext4_xattr_header *hdr){\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\t__le64 dsk_block_nr = cpu_to_le64(block_nr);\n\t__u32 dummy_csum = 0;\n\tint offset = offsetof(struct ext4_xattr_header, h_checksum);\n\n\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&dsk_block_nr,\n\t\t\t   sizeof(dsk_block_nr));\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)hdr, offset);\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)&dummy_csum, sizeof(dummy_csum));\n\toffset += sizeof(dummy_csum);\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)hdr + offset,\n\t\t\t   EXT4_BLOCK_SIZE(inode->i_sb) - offset);\n\n\treturn cpu_to_le32(csum);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_block_csum(struct inode *inode,\n\t\t\t\t    sector_t block_nr,\n\t\t\t\t    struct ext4_xattr_header *hdr){\n\tstruct ext4_sb_info *sbi = EXT4_SB(inode->i_sb);\n\t__u32 csum;\n\t__le64 dsk_block_nr = cpu_to_le64(block_nr);\n\t__u32 dummy_csum = 0;\n\tint offset = offsetof(struct ext4_xattr_header, h_checksum);\n\n\tcsum = ext4_chksum(sbi, sbi->s_csum_seed, (__u8 *)&dsk_block_nr,\n\t\t\t   sizeof(dsk_block_nr));\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)hdr, offset);\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)&dummy_csum, sizeof(dummy_csum));\n\toffset += sizeof(dummy_csum);\n\tcsum = ext4_chksum(sbi, csum, (__u8 *)hdr + offset,\n\t\t\t   EXT4_BLOCK_SIZE(inode->i_sb) - offset);\n\n\treturn cpu_to_le32(csum);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_289": {
        "vulnerable_code": {
            "Code": "ext4_xattr_block_csum_set(struct inode *inode,\n\t\t\t\t      struct buffer_head *bh){\n\tif (ext4_has_metadata_csum(inode->i_sb))\n\t\tBHDR(bh)->h_checksum = ext4_xattr_block_csum(inode,\n\t\t\t\t\t\tbh->b_blocknr, BHDR(bh));\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_block_csum_set(struct inode *inode,\n\t\t\t\t      struct buffer_head *bh){\n\tif (ext4_has_metadata_csum(inode->i_sb))\n\t\tBHDR(bh)->h_checksum = ext4_xattr_block_csum(inode,\n\t\t\t\t\t\tbh->b_blocknr, BHDR(bh));\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_290": {
        "vulnerable_code": {
            "Code": "ext4_xattr_block_find(struct inode *inode, struct ext4_xattr_info *i,\n\t\t      struct ext4_xattr_block_find *bs){\n\tstruct super_block *sb = inode->i_sb;\n\tint error;\n\n\tea_idebug(inode, \"name=%d.%s, value=%p, value_len=%ld\",\n\t\t  i->name_index, i->name, i->value, (long)i->value_len);\n\n\tif (EXT4_I(inode)->i_file_acl) {\n\t\t/* The inode already has an extended attribute block. */\n\t\tbs->bh = sb_bread(sb, EXT4_I(inode)->i_file_acl);\n\t\terror = -EIO;\n\t\tif (!bs->bh)\n\t\t\tgoto cleanup;\n\t\tea_bdebug(bs->bh, \"b_count=%d, refcount=%d\",\n\t\t\tatomic_read(&(bs->bh->b_count)),\n\t\t\tle32_to_cpu(BHDR(bs->bh)->h_refcount));\n\t\terror = ext4_xattr_check_block(inode, bs->bh);\n\t\tif (error)\n\t\t\tgoto cleanup;\n\t\t/* Find the named attribute. */\n\t\tbs->s.base = BHDR(bs->bh);\n\t\tbs->s.first = BFIRST(bs->bh);\n\t\tbs->s.end = bs->bh->b_data + bs->bh->b_size;\n\t\tbs->s.here = bs->s.first;\n\t\terror = xattr_find_entry(inode, &bs->s.here, bs->s.end,\n\t\t\t\t\t i->name_index, i->name, 1);\n\t\tif (error && error != -ENODATA)\n\t\t\tgoto cleanup;\n\t\tbs->s.not_found = error;\n\t}\n\terror = 0;\n\ncleanup:\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_block_find(struct inode *inode, struct ext4_xattr_info *i,\n\t\t      struct ext4_xattr_block_find *bs){\n\tstruct super_block *sb = inode->i_sb;\n\tint error;\n\n\tea_idebug(inode, \"name=%d.%s, value=%p, value_len=%ld\",\n\t\t  i->name_index, i->name, i->value, (long)i->value_len);\n\n\tif (EXT4_I(inode)->i_file_acl) {\n\t\t/* The inode already has an extended attribute block. */\n\t\tbs->bh = sb_bread(sb, EXT4_I(inode)->i_file_acl);\n\t\terror = -EIO;\n\t\tif (!bs->bh)\n\t\t\tgoto cleanup;\n\t\tea_bdebug(bs->bh, \"b_count=%d, refcount=%d\",\n\t\t\tatomic_read(&(bs->bh->b_count)),\n\t\t\tle32_to_cpu(BHDR(bs->bh)->h_refcount));\n\t\terror = ext4_xattr_check_block(inode, bs->bh);\n\t\tif (error)\n\t\t\tgoto cleanup;\n\t\t/* Find the named attribute. */\n\t\tbs->s.base = BHDR(bs->bh);\n\t\tbs->s.first = BFIRST(bs->bh);\n\t\tbs->s.end = bs->bh->b_data + bs->bh->b_size;\n\t\tbs->s.here = bs->s.first;\n\t\terror = xattr_find_entry(inode, &bs->s.here, bs->s.end,\n\t\t\t\t\t i->name_index, i->name, 1);\n\t\tif (error && error != -ENODATA)\n\t\t\tgoto cleanup;\n\t\tbs->s.not_found = error;\n\t}\n\terror = 0;\n\ncleanup:\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_291": {
        "vulnerable_code": {
            "Code": "ext4_xattr_check_entries(struct ext4_xattr_entry *entry, void *end,\n\t\t\t void *value_start){\n\tstruct ext4_xattr_entry *e = entry;\n\n\t/* Find the end of the names list */\n\twhile (!IS_LAST_ENTRY(e)) {\n\t\tstruct ext4_xattr_entry *next = EXT4_XATTR_NEXT(e);\n\t\tif ((void *)next >= end)\n\t\t\treturn -EFSCORRUPTED;\n\t\te = next;\n\t}\n\n\t/* Check the values */\n\twhile (!IS_LAST_ENTRY(entry)) {\n\t\tu32 size = le32_to_cpu(entry->e_value_size);\n\n\t\tif (size > EXT4_XATTR_SIZE_MAX)\n\t\t\treturn -EFSCORRUPTED;\n\n\t\tif (size != 0 && entry->e_value_inum == 0) {\n\t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n\t\t\tvoid *value;\n\n\t\t\t/*\n\t\t\t * The value cannot overlap the names, and the value\n\t\t\t * with padding cannot extend beyond 'end'.  Check both\n\t\t\t * the padded and unpadded sizes, since the size may\n\t\t\t * overflow to 0 when adding padding.\n\t\t\t */\n\t\t\tif (offs > end - value_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tvalue = value_start + offs;\n\t\t\tif (value < (void *)e + sizeof(u32) ||\n\t\t\t    size > end - value ||\n\t\t\t    EXT4_XATTR_SIZE(size) > end - value)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tentry = EXT4_XATTR_NEXT(entry);\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_check_entries(struct ext4_xattr_entry *entry, void *end,\n\t\t\t void *value_start){\n\tstruct ext4_xattr_entry *e = entry;\n\n\t/* Find the end of the names list */\n\twhile (!IS_LAST_ENTRY(e)) {\n\t\tstruct ext4_xattr_entry *next = EXT4_XATTR_NEXT(e);\n\t\tif ((void *)next >= end)\n\t\t\treturn -EFSCORRUPTED;\n\t\te = next;\n\t}\n\n\t/* Check the values */\n\twhile (!IS_LAST_ENTRY(entry)) {\n\t\tu32 size = le32_to_cpu(entry->e_value_size);\n\n\t\tif (size > EXT4_XATTR_SIZE_MAX)\n\t\t\treturn -EFSCORRUPTED;\n\n\t\tif (size != 0 && entry->e_value_inum == 0) {\n\t\t\tu16 offs = le16_to_cpu(entry->e_value_offs);\n\t\t\tvoid *value;\n\n\t\t\t/*\n\t\t\t * The value cannot overlap the names, and the value\n\t\t\t * with padding cannot extend beyond 'end'.  Check both\n\t\t\t * the padded and unpadded sizes, since the size may\n\t\t\t * overflow to 0 when adding padding.\n\t\t\t */\n\t\t\tif (offs > end - value_start)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\tvalue = value_start + offs;\n\t\t\tif (value < (void *)e + sizeof(u32) ||\n\t\t\t    size > end - value ||\n\t\t\t    EXT4_XATTR_SIZE(size) > end - value)\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\tentry = EXT4_XATTR_NEXT(entry);\n\t}\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_292": {
        "vulnerable_code": {
            "Code": "ext4_xattr_ibody_list(struct dentry *dentry, char *buffer, size_t buffer_size){\n\tstruct inode *inode = d_inode(dentry);\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_iloc iloc;\n\tvoid *end;\n\tint error;\n\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR))\n\t\treturn 0;\n\terror = ext4_get_inode_loc(inode, &iloc);\n\tif (error)\n\t\treturn error;\n\traw_inode = ext4_raw_inode(&iloc);\n\theader = IHDR(inode, raw_inode);\n\tend = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\terror = xattr_check_inode(inode, header, end);\n\tif (error)\n\t\tgoto cleanup;\n\terror = ext4_xattr_list_entries(dentry, IFIRST(header),\n\t\t\t\t\tbuffer, buffer_size);\n\ncleanup:\n\tbrelse(iloc.bh);\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_ibody_list(struct dentry *dentry, char *buffer, size_t buffer_size){\n\tstruct inode *inode = d_inode(dentry);\n\tstruct ext4_xattr_ibody_header *header;\n\tstruct ext4_inode *raw_inode;\n\tstruct ext4_iloc iloc;\n\tvoid *end;\n\tint error;\n\n\tif (!ext4_test_inode_state(inode, EXT4_STATE_XATTR))\n\t\treturn 0;\n\terror = ext4_get_inode_loc(inode, &iloc);\n\tif (error)\n\t\treturn error;\n\traw_inode = ext4_raw_inode(&iloc);\n\theader = IHDR(inode, raw_inode);\n\tend = (void *)raw_inode + EXT4_SB(inode->i_sb)->s_inode_size;\n\terror = xattr_check_inode(inode, header, end);\n\tif (error)\n\t\tgoto cleanup;\n\terror = ext4_xattr_list_entries(dentry, IFIRST(header),\n\t\t\t\t\tbuffer, buffer_size);\n\ncleanup:\n\tbrelse(iloc.bh);\n\treturn error;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_293": {
        "vulnerable_code": {
            "Code": "ext4_xattr_inode_array_free(struct ext4_xattr_inode_array *ea_inode_array){\n\tint idx;\n\n\tif (ea_inode_array == NULL)\n\t\treturn;\n\n\tfor (idx = 0; idx < ea_inode_array->count; ++idx)\n\t\tiput(ea_inode_array->inodes[idx]);\n\tkfree(ea_inode_array);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_inode_array_free(struct ext4_xattr_inode_array *ea_inode_array){\n\tint idx;\n\n\tif (ea_inode_array == NULL)\n\t\treturn;\n\n\tfor (idx = 0; idx < ea_inode_array->count; ++idx)\n\t\tiput(ea_inode_array->inodes[idx]);\n\tkfree(ea_inode_array);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_294": {
        "vulnerable_code": {
            "Code": "ext4_xattr_inode_cache_find(struct inode *inode, const void *value,\n\t\t\t    size_t value_len, u32 hash){\n\tstruct inode *ea_inode;\n\tstruct mb_cache_entry *ce;\n\tstruct mb_cache *ea_inode_cache = EA_INODE_CACHE(inode);\n\tvoid *ea_data;\n\n\tif (!ea_inode_cache)\n\t\treturn NULL;\n\n\tce = mb_cache_entry_find_first(ea_inode_cache, hash);\n\tif (!ce)\n\t\treturn NULL;\n\n\tea_data = ext4_kvmalloc(value_len, GFP_NOFS);\n\tif (!ea_data) {\n\t\tmb_cache_entry_put(ea_inode_cache, ce);\n\t\treturn NULL;\n\t}\n\n\twhile (ce) {\n\t\tea_inode = ext4_iget(inode->i_sb, ce->e_value);\n\t\tif (!IS_ERR(ea_inode) &&\n\t\t    !is_bad_inode(ea_inode) &&\n\t\t    (EXT4_I(ea_inode)->i_flags & EXT4_EA_INODE_FL) &&\n\t\t    i_size_read(ea_inode) == value_len &&\n\t\t    !ext4_xattr_inode_read(ea_inode, ea_data, value_len) &&\n\t\t    !ext4_xattr_inode_verify_hashes(ea_inode, NULL, ea_data,\n\t\t\t\t\t\t    value_len) &&\n\t\t    !memcmp(value, ea_data, value_len)) {\n\t\t\tmb_cache_entry_touch(ea_inode_cache, ce);\n\t\t\tmb_cache_entry_put(ea_inode_cache, ce);\n\t\t\tkvfree(ea_data);\n\t\t\treturn ea_inode;\n\t\t}\n\n\t\tif (!IS_ERR(ea_inode))\n\t\t\tiput(ea_inode);\n\t\tce = mb_cache_entry_find_next(ea_inode_cache, ce);\n\t}\n\tkvfree(ea_data);\n\treturn NULL;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_inode_cache_find(struct inode *inode, const void *value,\n\t\t\t    size_t value_len, u32 hash){\n\tstruct inode *ea_inode;\n\tstruct mb_cache_entry *ce;\n\tstruct mb_cache *ea_inode_cache = EA_INODE_CACHE(inode);\n\tvoid *ea_data;\n\n\tif (!ea_inode_cache)\n\t\treturn NULL;\n\n\tce = mb_cache_entry_find_first(ea_inode_cache, hash);\n\tif (!ce)\n\t\treturn NULL;\n\n\tea_data = ext4_kvmalloc(value_len, GFP_NOFS);\n\tif (!ea_data) {\n\t\tmb_cache_entry_put(ea_inode_cache, ce);\n\t\treturn NULL;\n\t}\n\n\twhile (ce) {\n\t\tea_inode = ext4_iget(inode->i_sb, ce->e_value);\n\t\tif (!IS_ERR(ea_inode) &&\n\t\t    !is_bad_inode(ea_inode) &&\n\t\t    (EXT4_I(ea_inode)->i_flags & EXT4_EA_INODE_FL) &&\n\t\t    i_size_read(ea_inode) == value_len &&\n\t\t    !ext4_xattr_inode_read(ea_inode, ea_data, value_len) &&\n\t\t    !ext4_xattr_inode_verify_hashes(ea_inode, NULL, ea_data,\n\t\t\t\t\t\t    value_len) &&\n\t\t    !memcmp(value, ea_data, value_len)) {\n\t\t\tmb_cache_entry_touch(ea_inode_cache, ce);\n\t\t\tmb_cache_entry_put(ea_inode_cache, ce);\n\t\t\tkvfree(ea_data);\n\t\t\treturn ea_inode;\n\t\t}\n\n\t\tif (!IS_ERR(ea_inode))\n\t\t\tiput(ea_inode);\n\t\tce = mb_cache_entry_find_next(ea_inode_cache, ce);\n\t}\n\tkvfree(ea_data);\n\treturn NULL;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_295": {
        "vulnerable_code": {
            "Code": "ext4_xattr_inode_hash(struct ext4_sb_info *sbi, const void *buffer, size_t size){\n\treturn ext4_chksum(sbi, sbi->s_csum_seed, buffer, size);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_inode_hash(struct ext4_sb_info *sbi, const void *buffer, size_t size){\n\treturn ext4_chksum(sbi, sbi->s_csum_seed, buffer, size);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_296": {
        "vulnerable_code": {
            "Code": "ext4_xattr_inode_iget(struct inode *parent, unsigned long ea_ino,\n\t\t\t\t u32 ea_inode_hash, struct inode **ea_inode){\n\tstruct inode *inode;\n\tint err;\n\n\tinode = ext4_iget(parent->i_sb, ea_ino);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\text4_error(parent->i_sb,\n\t\t\t   \"error while reading EA inode %lu err=%d\", ea_ino,\n\t\t\t   err);\n\t\treturn err;\n\t}\n\n\tif (is_bad_inode(inode)) {\n\t\text4_error(parent->i_sb,\n\t\t\t   \"error while reading EA inode %lu is_bad_inode\",\n\t\t\t   ea_ino);\n\t\terr = -EIO;\n\t\tgoto error;\n\t}\n\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL)) {\n\t\text4_error(parent->i_sb,\n\t\t\t   \"EA inode %lu does not have EXT4_EA_INODE_FL flag\",\n\t\t\t    ea_ino);\n\t\terr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\text4_xattr_inode_set_class(inode);\n\n\t/*\n\t * Check whether this is an old Lustre-style xattr inode. Lustre\n\t * implementation does not have hash validation, rather it has a\n\t * backpointer from ea_inode to the parent inode.\n\t */\n\tif (ea_inode_hash != ext4_xattr_inode_get_hash(inode) &&\n\t    EXT4_XATTR_INODE_GET_PARENT(inode) == parent->i_ino &&\n\t    inode->i_generation == parent->i_generation) {\n\t\text4_set_inode_state(inode, EXT4_STATE_LUSTRE_EA_INODE);\n\t\text4_xattr_inode_set_ref(inode, 1);\n\t} else {\n\t\tinode_lock(inode);\n\t\tinode->i_flags |= S_NOQUOTA;\n\t\tinode_unlock(inode);\n\t}\n\n\t*ea_inode = inode;\n\treturn 0;\nerror:\n\tiput(inode);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_inode_iget(struct inode *parent, unsigned long ea_ino,\n\t\t\t\t u32 ea_inode_hash, struct inode **ea_inode){\n\tstruct inode *inode;\n\tint err;\n\n\tinode = ext4_iget(parent->i_sb, ea_ino);\n\tif (IS_ERR(inode)) {\n\t\terr = PTR_ERR(inode);\n\t\text4_error(parent->i_sb,\n\t\t\t   \"error while reading EA inode %lu err=%d\", ea_ino,\n\t\t\t   err);\n\t\treturn err;\n\t}\n\n\tif (is_bad_inode(inode)) {\n\t\text4_error(parent->i_sb,\n\t\t\t   \"error while reading EA inode %lu is_bad_inode\",\n\t\t\t   ea_ino);\n\t\terr = -EIO;\n\t\tgoto error;\n\t}\n\n\tif (!(EXT4_I(inode)->i_flags & EXT4_EA_INODE_FL)) {\n\t\text4_error(parent->i_sb,\n\t\t\t   \"EA inode %lu does not have EXT4_EA_INODE_FL flag\",\n\t\t\t    ea_ino);\n\t\terr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\text4_xattr_inode_set_class(inode);\n\n\t/*\n\t * Check whether this is an old Lustre-style xattr inode. Lustre\n\t * implementation does not have hash validation, rather it has a\n\t * backpointer from ea_inode to the parent inode.\n\t */\n\tif (ea_inode_hash != ext4_xattr_inode_get_hash(inode) &&\n\t    EXT4_XATTR_INODE_GET_PARENT(inode) == parent->i_ino &&\n\t    inode->i_generation == parent->i_generation) {\n\t\text4_set_inode_state(inode, EXT4_STATE_LUSTRE_EA_INODE);\n\t\text4_xattr_inode_set_ref(inode, 1);\n\t} else {\n\t\tinode_lock(inode);\n\t\tinode->i_flags |= S_NOQUOTA;\n\t\tinode_unlock(inode);\n\t}\n\n\t*ea_inode = inode;\n\treturn 0;\nerror:\n\tiput(inode);\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_297": {
        "vulnerable_code": {
            "Code": "ext4_xattr_inode_inc_ref_all(handle_t *handle, struct inode *parent,\n\t\t\t\t\tstruct ext4_xattr_entry *first){\n\tstruct inode *ea_inode;\n\tstruct ext4_xattr_entry *entry;\n\tstruct ext4_xattr_entry *failed_entry;\n\tunsigned int ea_ino;\n\tint err, saved_err;\n\n\tfor (entry = first; !IS_LAST_ENTRY(entry);\n\t     entry = EXT4_XATTR_NEXT(entry)) {\n\t\tif (!entry->e_value_inum)\n\t\t\tcontinue;\n\t\tea_ino = le32_to_cpu(entry->e_value_inum);\n\t\terr = ext4_xattr_inode_iget(parent, ea_ino,\n\t\t\t\t\t    le32_to_cpu(entry->e_hash),\n\t\t\t\t\t    &ea_inode);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\terr = ext4_xattr_inode_inc_ref(handle, ea_inode);\n\t\tif (err) {\n\t\t\text4_warning_inode(ea_inode, \"inc ref error %d\", err);\n\t\t\tiput(ea_inode);\n\t\t\tgoto cleanup;\n\t\t}\n\t\tiput(ea_inode);\n\t}\n\treturn 0;\n\ncleanup:\n\tsaved_err = err;\n\tfailed_entry = entry;\n\n\tfor (entry = first; entry != failed_entry;\n\t     entry = EXT4_XATTR_NEXT(entry)) {\n\t\tif (!entry->e_value_inum)\n\t\t\tcontinue;\n\t\tea_ino = le32_to_cpu(entry->e_value_inum);\n\t\terr = ext4_xattr_inode_iget(parent, ea_ino,\n\t\t\t\t\t    le32_to_cpu(entry->e_hash),\n\t\t\t\t\t    &ea_inode);\n\t\tif (err) {\n\t\t\text4_warning(parent->i_sb,\n\t\t\t\t     \"cleanup ea_ino %u iget error %d\", ea_ino,\n\t\t\t\t     err);\n\t\t\tcontinue;\n\t\t}\n\t\terr = ext4_xattr_inode_dec_ref(handle, ea_inode);\n\t\tif (err)\n\t\t\text4_warning_inode(ea_inode, \"cleanup dec ref error %d\",\n\t\t\t\t\t   err);\n\t\tiput(ea_inode);\n\t}\n\treturn saved_err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_inode_inc_ref_all(handle_t *handle, struct inode *parent,\n\t\t\t\t\tstruct ext4_xattr_entry *first){\n\tstruct inode *ea_inode;\n\tstruct ext4_xattr_entry *entry;\n\tstruct ext4_xattr_entry *failed_entry;\n\tunsigned int ea_ino;\n\tint err, saved_err;\n\n\tfor (entry = first; !IS_LAST_ENTRY(entry);\n\t     entry = EXT4_XATTR_NEXT(entry)) {\n\t\tif (!entry->e_value_inum)\n\t\t\tcontinue;\n\t\tea_ino = le32_to_cpu(entry->e_value_inum);\n\t\terr = ext4_xattr_inode_iget(parent, ea_ino,\n\t\t\t\t\t    le32_to_cpu(entry->e_hash),\n\t\t\t\t\t    &ea_inode);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t\terr = ext4_xattr_inode_inc_ref(handle, ea_inode);\n\t\tif (err) {\n\t\t\text4_warning_inode(ea_inode, \"inc ref error %d\", err);\n\t\t\tiput(ea_inode);\n\t\t\tgoto cleanup;\n\t\t}\n\t\tiput(ea_inode);\n\t}\n\treturn 0;\n\ncleanup:\n\tsaved_err = err;\n\tfailed_entry = entry;\n\n\tfor (entry = first; entry != failed_entry;\n\t     entry = EXT4_XATTR_NEXT(entry)) {\n\t\tif (!entry->e_value_inum)\n\t\t\tcontinue;\n\t\tea_ino = le32_to_cpu(entry->e_value_inum);\n\t\terr = ext4_xattr_inode_iget(parent, ea_ino,\n\t\t\t\t\t    le32_to_cpu(entry->e_hash),\n\t\t\t\t\t    &ea_inode);\n\t\tif (err) {\n\t\t\text4_warning(parent->i_sb,\n\t\t\t\t     \"cleanup ea_ino %u iget error %d\", ea_ino,\n\t\t\t\t     err);\n\t\t\tcontinue;\n\t\t}\n\t\terr = ext4_xattr_inode_dec_ref(handle, ea_inode);\n\t\tif (err)\n\t\t\text4_warning_inode(ea_inode, \"cleanup dec ref error %d\",\n\t\t\t\t\t   err);\n\t\tiput(ea_inode);\n\t}\n\treturn saved_err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_298": {
        "vulnerable_code": {
            "Code": "ext4_xattr_set_credits(struct inode *inode, size_t value_len,\n\t\t\t   bool is_create, int *credits){\n\tstruct buffer_head *bh;\n\tint err;\n\n\t*credits = 0;\n\n\tif (!EXT4_SB(inode->i_sb)->s_journal)\n\t\treturn 0;\n\n\tdown_read(&EXT4_I(inode)->xattr_sem);\n\n\tbh = ext4_xattr_get_block(inode);\n\tif (IS_ERR(bh)) {\n\t\terr = PTR_ERR(bh);\n\t} else {\n\t\t*credits = __ext4_xattr_set_credits(inode->i_sb, inode, bh,\n\t\t\t\t\t\t    value_len, is_create);\n\t\tbrelse(bh);\n\t\terr = 0;\n\t}\n\n\tup_read(&EXT4_I(inode)->xattr_sem);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_xattr_set_credits(struct inode *inode, size_t value_len,\n\t\t\t   bool is_create, int *credits){\n\tstruct buffer_head *bh;\n\tint err;\n\n\t*credits = 0;\n\n\tif (!EXT4_SB(inode->i_sb)->s_journal)\n\t\treturn 0;\n\n\tdown_read(&EXT4_I(inode)->xattr_sem);\n\n\tbh = ext4_xattr_get_block(inode);\n\tif (IS_ERR(bh)) {\n\t\terr = PTR_ERR(bh);\n\t} else {\n\t\t*credits = __ext4_xattr_set_credits(inode->i_sb, inode, bh,\n\t\t\t\t\t\t    value_len, is_create);\n\t\tbrelse(bh);\n\t\terr = 0;\n\t}\n\n\tup_read(&EXT4_I(inode)->xattr_sem);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_299": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_300": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_301": {
        "vulnerable_code": {
            "Code": "ext4_decode_extra_time(struct timespec *time, __le32 extra){\n\tif (unlikely(sizeof(time->tv_sec) > 4 &&\n\t\t\t(extra & cpu_to_le32(EXT4_EPOCH_MASK)))) {\n\n#if 1\n\t\t/* Handle legacy encoding of pre-1970 dates with epoch\n\t\t * bits 1,1. (This backwards compatibility may be removed\n\t\t * at the discretion of the ext4 developers.)\n\t\t */\n\t\tu64 extra_bits = le32_to_cpu(extra) & EXT4_EPOCH_MASK;\n\t\tif (extra_bits == 3 && ((time->tv_sec) & 0x80000000) != 0)\n\t\t\textra_bits = 0;\n\t\ttime->tv_sec += extra_bits << 32;\n#else\n\t\ttime->tv_sec += (u64)(le32_to_cpu(extra) & EXT4_EPOCH_MASK) << 32;\n#endif\n\t}\n\ttime->tv_nsec = (le32_to_cpu(extra) & EXT4_NSEC_MASK) >> EXT4_EPOCH_BITS;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_decode_extra_time(struct timespec *time, __le32 extra){\n\tif (unlikely(sizeof(time->tv_sec) > 4 &&\n\t\t\t(extra & cpu_to_le32(EXT4_EPOCH_MASK)))) {\n\n#if 1\n\t\t/* Handle legacy encoding of pre-1970 dates with epoch\n\t\t * bits 1,1. (This backwards compatibility may be removed\n\t\t * at the discretion of the ext4 developers.)\n\t\t */\n\t\tu64 extra_bits = le32_to_cpu(extra) & EXT4_EPOCH_MASK;\n\t\tif (extra_bits == 3 && ((time->tv_sec) & 0x80000000) != 0)\n\t\t\textra_bits = 0;\n\t\ttime->tv_sec += extra_bits << 32;\n#else\n\t\ttime->tv_sec += (u64)(le32_to_cpu(extra) & EXT4_EPOCH_MASK) << 32;\n#endif\n\t}\n\ttime->tv_nsec = (le32_to_cpu(extra) & EXT4_NSEC_MASK) >> EXT4_EPOCH_BITS;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_302": {
        "vulnerable_code": {
            "Code": "ext4_has_compat_features(struct super_block *sb){\n\treturn (EXT4_SB(sb)->s_es->s_feature_compat != 0);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 5,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ext4_has_compat_features(struct super_block *sb){\n\treturn (EXT4_SB(sb)->s_es->s_feature_compat != 0);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_303": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_304": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_305": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_306": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_307": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_308": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_309": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_310": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_311": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_312": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_313": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_314": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_315": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_316": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_317": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_318": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_319": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_320": {
        "vulnerable_code": {
            "Code": "chap_binaryhex_to_asciihex(char *dst, char *src, int src_len){\n\tint i;\n\n\tfor (i = 0; i < src_len; i++) {\n\t\tsprintf(&dst[i*2], \"%02x\", (int) src[i] & 0xff);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "chap_binaryhex_to_asciihex(char *dst, char *src, int src_len){\n\tint i;\n\n\tfor (i = 0; i < src_len; i++) {\n\t\tsprintf(&dst[i*2], \"%02x\", (int) src[i] & 0xff);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_321": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_322": {
        "vulnerable_code": {
            "Code": "get_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\t/*\n\t * Because the kvm_regs structure is a mix of 32, 64 and\n\t * 128bit fields, we index it as if it was a 32bit\n\t * array. Hence below, nr_regs is the number of entries, and\n\t * off the index in the \"array\".\n\t */\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\tu32 off;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\t/*\n\t * Because the kvm_regs structure is a mix of 32, 64 and\n\t * 128bit fields, we index it as if it was a 32bit\n\t * array. Hence below, nr_regs is the number of entries, and\n\t * off the index in the \"array\".\n\t */\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\tu32 off;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (validate_core_offset(reg))\n\t\treturn -EINVAL;\n\n\tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_323": {
        "vulnerable_code": {
            "Code": "get_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\tvoid __user *uaddr = (void __user *)(long)reg->addr;\n\tu64 val;\n\n\tval = kvm_arm_timer_get_reg(vcpu, reg->id);\n\treturn copy_to_user(uaddr, &val, KVM_REG_SIZE(reg->id)) ? -EFAULT : 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 2,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\tvoid __user *uaddr = (void __user *)(long)reg->addr;\n\tu64 val;\n\n\tval = kvm_arm_timer_get_reg(vcpu, reg->id);\n\treturn copy_to_user(uaddr, &val, KVM_REG_SIZE(reg->id)) ? -EFAULT : 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 2,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_324": {
        "vulnerable_code": {
            "Code": "kvm_arm_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\t/* We currently use nothing arch-specific in upper 32 bits */\n\tif ((reg->id & ~KVM_REG_SIZE_MASK) >> 32 != KVM_REG_ARM64 >> 32)\n\t\treturn -EINVAL;\n\n\t/* Register group 16 means we set a core register. */\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_CORE)\n\t\treturn set_core_reg(vcpu, reg);\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_FW)\n\t\treturn kvm_arm_set_fw_reg(vcpu, reg);\n\n\tif (is_timer_reg(reg->id))\n\t\treturn set_timer_reg(vcpu, reg);\n\n\treturn kvm_arm_sys_reg_set_reg(vcpu, reg);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "kvm_arm_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\t/* We currently use nothing arch-specific in upper 32 bits */\n\tif ((reg->id & ~KVM_REG_SIZE_MASK) >> 32 != KVM_REG_ARM64 >> 32)\n\t\treturn -EINVAL;\n\n\t/* Register group 16 means we set a core register. */\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_CORE)\n\t\treturn set_core_reg(vcpu, reg);\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_FW)\n\t\treturn kvm_arm_set_fw_reg(vcpu, reg);\n\n\tif (is_timer_reg(reg->id))\n\t\treturn set_timer_reg(vcpu, reg);\n\n\treturn kvm_arm_sys_reg_set_reg(vcpu, reg);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_325": {
        "vulnerable_code": {
            "Code": "kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_device_attr *attr){\n\tint ret;\n\n\tswitch (attr->group) {\n\tcase KVM_ARM_VCPU_PMU_V3_CTRL:\n\t\tret = kvm_arm_pmu_v3_get_attr(vcpu, attr);\n\t\tbreak;\n\tcase KVM_ARM_VCPU_TIMER_CTRL:\n\t\tret = kvm_arm_timer_get_attr(vcpu, attr);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENXIO;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_device_attr *attr){\n\tint ret;\n\n\tswitch (attr->group) {\n\tcase KVM_ARM_VCPU_PMU_V3_CTRL:\n\t\tret = kvm_arm_pmu_v3_get_attr(vcpu, attr);\n\t\tbreak;\n\tcase KVM_ARM_VCPU_TIMER_CTRL:\n\t\tret = kvm_arm_timer_get_attr(vcpu, attr);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENXIO;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_326": {
        "vulnerable_code": {
            "Code": "set_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\t__uint128_t tmp;\n\tvoid *valp = &tmp;\n\tu64 off;\n\tint err = 0;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (KVM_REG_SIZE(reg->id) > sizeof(tmp))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(valp, uaddr, KVM_REG_SIZE(reg->id))) {\n\t\terr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (off == KVM_REG_ARM_CORE_REG(regs.pstate)) {\n\t\tu32 mode = (*(u32 *)valp) & PSR_AA32_MODE_MASK;\n\t\tswitch (mode) {\n\t\tcase PSR_AA32_MODE_USR:\n\t\tcase PSR_AA32_MODE_FIQ:\n\t\tcase PSR_AA32_MODE_IRQ:\n\t\tcase PSR_AA32_MODE_SVC:\n\t\tcase PSR_AA32_MODE_ABT:\n\t\tcase PSR_AA32_MODE_UND:\n\t\tcase PSR_MODE_EL0t:\n\t\tcase PSR_MODE_EL1t:\n\t\tcase PSR_MODE_EL1h:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmemcpy((u32 *)regs + off, valp, KVM_REG_SIZE(reg->id));\nout:\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "set_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\t__uint128_t tmp;\n\tvoid *valp = &tmp;\n\tu64 off;\n\tint err = 0;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (validate_core_offset(reg))\n\t\treturn -EINVAL;\n\n\tif (KVM_REG_SIZE(reg->id) > sizeof(tmp))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(valp, uaddr, KVM_REG_SIZE(reg->id))) {\n\t\terr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (off == KVM_REG_ARM_CORE_REG(regs.pstate)) {\n\t\tu32 mode = (*(u32 *)valp) & PSR_AA32_MODE_MASK;\n\t\tswitch (mode) {\n\t\tcase PSR_AA32_MODE_USR:\n\t\tcase PSR_AA32_MODE_FIQ:\n\t\tcase PSR_AA32_MODE_IRQ:\n\t\tcase PSR_AA32_MODE_SVC:\n\t\tcase PSR_AA32_MODE_ABT:\n\t\tcase PSR_AA32_MODE_UND:\n\t\tcase PSR_MODE_EL0t:\n\t\tcase PSR_MODE_EL1t:\n\t\tcase PSR_MODE_EL1h:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmemcpy((u32 *)regs + off, valp, KVM_REG_SIZE(reg->id));\nout:\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_327": {
        "vulnerable_code": {
            "Code": "set_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\tvoid __user *uaddr = (void __user *)(long)reg->addr;\n\tu64 val;\n\tint ret;\n\n\tret = copy_from_user(&val, uaddr, KVM_REG_SIZE(reg->id));\n\tif (ret != 0)\n\t\treturn -EFAULT;\n\n\treturn kvm_arm_timer_set_reg(vcpu, reg->id, val);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "set_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg){\n\tvoid __user *uaddr = (void __user *)(long)reg->addr;\n\tu64 val;\n\tint ret;\n\n\tret = copy_from_user(&val, uaddr, KVM_REG_SIZE(reg->id));\n\tif (ret != 0)\n\t\treturn -EFAULT;\n\n\treturn kvm_arm_timer_set_reg(vcpu, reg->id, val);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_328": {
        "vulnerable_code": {
            "Code": "copy_out_args(struct fuse_copy_state *cs, struct fuse_out *out,\n\t\t\t unsigned nbytes){\n\tunsigned reqsize = sizeof(struct fuse_out_header);\n\n\tif (out->h.error)\n\t\treturn nbytes != reqsize ? -EINVAL : 0;\n\n\treqsize += len_args(out->numargs, out->args);\n\n\tif (reqsize < nbytes || (reqsize > nbytes && !out->argvar))\n\t\treturn -EINVAL;\n\telse if (reqsize > nbytes) {\n\t\tstruct fuse_arg *lastarg = &out->args[out->numargs-1];\n\t\tunsigned diffsize = reqsize - nbytes;\n\t\tif (diffsize > lastarg->size)\n\t\t\treturn -EINVAL;\n\t\tlastarg->size -= diffsize;\n\t}\n\treturn fuse_copy_args(cs, out->numargs, out->argpages, out->args,\n\t\t\t      out->page_zeroing);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "copy_out_args(struct fuse_copy_state *cs, struct fuse_out *out,\n\t\t\t unsigned nbytes){\n\tunsigned reqsize = sizeof(struct fuse_out_header);\n\n\tif (out->h.error)\n\t\treturn nbytes != reqsize ? -EINVAL : 0;\n\n\treqsize += len_args(out->numargs, out->args);\n\n\tif (reqsize < nbytes || (reqsize > nbytes && !out->argvar))\n\t\treturn -EINVAL;\n\telse if (reqsize > nbytes) {\n\t\tstruct fuse_arg *lastarg = &out->args[out->numargs-1];\n\t\tunsigned diffsize = reqsize - nbytes;\n\t\tif (diffsize > lastarg->size)\n\t\t\treturn -EINVAL;\n\t\tlastarg->size -= diffsize;\n\t}\n\treturn fuse_copy_args(cs, out->numargs, out->argpages, out->args,\n\t\t\t      out->page_zeroing);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_329": {
        "vulnerable_code": {
            "Code": "dequeue_forget(struct fuse_iqueue *fiq,\n\t\t\t\t\t       unsigned max,\n\t\t\t\t\t       unsigned *countp){\n\tstruct fuse_forget_link *head = fiq->forget_list_head.next;\n\tstruct fuse_forget_link **newhead = &head;\n\tunsigned count;\n\n\tfor (count = 0; *newhead != NULL && count < max; count++)\n\t\tnewhead = &(*newhead)->next;\n\n\tfiq->forget_list_head.next = *newhead;\n\t*newhead = NULL;\n\tif (fiq->forget_list_head.next == NULL)\n\t\tfiq->forget_list_tail = &fiq->forget_list_head;\n\n\tif (countp != NULL)\n\t\t*countp = count;\n\n\treturn head;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "dequeue_forget(struct fuse_iqueue *fiq,\n\t\t\t\t\t       unsigned max,\n\t\t\t\t\t       unsigned *countp){\n\tstruct fuse_forget_link *head = fiq->forget_list_head.next;\n\tstruct fuse_forget_link **newhead = &head;\n\tunsigned count;\n\n\tfor (count = 0; *newhead != NULL && count < max; count++)\n\t\tnewhead = &(*newhead)->next;\n\n\tfiq->forget_list_head.next = *newhead;\n\t*newhead = NULL;\n\tif (fiq->forget_list_head.next == NULL)\n\t\tfiq->forget_list_tail = &fiq->forget_list_head;\n\n\tif (countp != NULL)\n\t\t*countp = count;\n\n\treturn head;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_330": {
        "vulnerable_code": {
            "Code": "fuse_copy_one(struct fuse_copy_state *cs, void *val, unsigned size){\n\twhile (size) {\n\t\tif (!cs->len) {\n\t\t\tint err = fuse_copy_fill(cs);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tfuse_copy_do(cs, &val, &size);\n\t}\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_copy_one(struct fuse_copy_state *cs, void *val, unsigned size){\n\twhile (size) {\n\t\tif (!cs->len) {\n\t\t\tint err = fuse_copy_fill(cs);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tfuse_copy_do(cs, &val, &size);\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_331": {
        "vulnerable_code": {
            "Code": "fuse_dev_fasync(int fd, struct file *file, int on){\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\t/* No locking - fasync_helper does its own locking */\n\treturn fasync_helper(fd, file, on, &fud->fc->iq.fasync);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_dev_fasync(int fd, struct file *file, int on){\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\t/* No locking - fasync_helper does its own locking */\n\treturn fasync_helper(fd, file, on, &fud->fc->iq.fasync);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_332": {
        "vulnerable_code": {
            "Code": "fuse_dev_poll(struct file *file, poll_table *wait){\n\t__poll_t mask = EPOLLOUT | EPOLLWRNORM;\n\tstruct fuse_iqueue *fiq;\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn EPOLLERR;\n\n\tfiq = &fud->fc->iq;\n\tpoll_wait(file, &fiq->waitq, wait);\n\n\tspin_lock(&fiq->waitq.lock);\n\tif (!fiq->connected)\n\t\tmask = EPOLLERR;\n\telse if (request_pending(fiq))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\tspin_unlock(&fiq->waitq.lock);\n\n\treturn mask;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_dev_poll(struct file *file, poll_table *wait){\n\t__poll_t mask = EPOLLOUT | EPOLLWRNORM;\n\tstruct fuse_iqueue *fiq;\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (!fud)\n\t\treturn EPOLLERR;\n\n\tfiq = &fud->fc->iq;\n\tpoll_wait(file, &fiq->waitq, wait);\n\n\tspin_lock(&fiq->waitq.lock);\n\tif (!fiq->connected)\n\t\tmask = EPOLLERR;\n\telse if (request_pending(fiq))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\tspin_unlock(&fiq->waitq.lock);\n\n\treturn mask;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_333": {
        "vulnerable_code": {
            "Code": "fuse_dev_release(struct inode *inode, struct file *file){\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (fud) {\n\t\tstruct fuse_conn *fc = fud->fc;\n\t\tstruct fuse_pqueue *fpq = &fud->pq;\n\t\tLIST_HEAD(to_end);\n\t\tunsigned int i;\n\n\t\tspin_lock(&fpq->lock);\n\t\tWARN_ON(!list_empty(&fpq->io));\n\t\tfor (i = 0; i < FUSE_PQ_HASH_SIZE; i++)\n\t\t\tlist_splice_init(&fpq->processing[i], &to_end);\n\t\tspin_unlock(&fpq->lock);\n\n\t\tend_requests(fc, &to_end);\n\n\t\t/* Are we the last open device? */\n\t\tif (atomic_dec_and_test(&fc->dev_count)) {\n\t\t\tWARN_ON(fc->iq.fasync != NULL);\n\t\t\tfuse_abort_conn(fc, false);\n\t\t}\n\t\tfuse_dev_free(fud);\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_dev_release(struct inode *inode, struct file *file){\n\tstruct fuse_dev *fud = fuse_get_dev(file);\n\n\tif (fud) {\n\t\tstruct fuse_conn *fc = fud->fc;\n\t\tstruct fuse_pqueue *fpq = &fud->pq;\n\t\tLIST_HEAD(to_end);\n\t\tunsigned int i;\n\n\t\tspin_lock(&fpq->lock);\n\t\tWARN_ON(!list_empty(&fpq->io));\n\t\tfor (i = 0; i < FUSE_PQ_HASH_SIZE; i++)\n\t\t\tlist_splice_init(&fpq->processing[i], &to_end);\n\t\tspin_unlock(&fpq->lock);\n\n\t\tend_requests(fc, &to_end);\n\n\t\t/* Are we the last open device? */\n\t\tif (atomic_dec_and_test(&fc->dev_count)) {\n\t\t\tWARN_ON(fc->iq.fasync != NULL);\n\t\t\tfuse_abort_conn(fc, false);\n\t\t}\n\t\tfuse_dev_free(fud);\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_334": {
        "vulnerable_code": {
            "Code": "fuse_dev_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t    struct pipe_inode_info *pipe,\n\t\t\t\t    size_t len, unsigned int flags){\n\tint total, ret;\n\tint page_nr = 0;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud = fuse_get_dev(in);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tbufs = kvmalloc_array(pipe->buffers, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs)\n\t\treturn -ENOMEM;\n\n\tfuse_copy_init(&cs, 1, NULL);\n\tcs.pipebufs = bufs;\n\tcs.pipe = pipe;\n\tret = fuse_dev_do_read(fud, in, &cs, len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (pipe->nrbufs + cs.nr_segs > pipe->buffers) {\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\tfor (ret = total = 0; page_nr < cs.nr_segs; total += ret) {\n\t\t/*\n\t\t * Need to be careful about this.  Having buf->ops in module\n\t\t * code can Oops if the buffer persists after module unload.\n\t\t */\n\t\tbufs[page_nr].ops = &nosteal_pipe_buf_ops;\n\t\tbufs[page_nr].flags = 0;\n\t\tret = add_to_pipe(pipe, &bufs[page_nr++]);\n\t\tif (unlikely(ret < 0))\n\t\t\tbreak;\n\t}\n\tif (total)\n\t\tret = total;\nout:\n\tfor (; page_nr < cs.nr_segs; page_nr++)\n\t\tput_page(bufs[page_nr].page);\n\n\tkvfree(bufs);\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_dev_splice_read(struct file *in, loff_t *ppos,\n\t\t\t\t    struct pipe_inode_info *pipe,\n\t\t\t\t    size_t len, unsigned int flags){\n\tint total, ret;\n\tint page_nr = 0;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud = fuse_get_dev(in);\n\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tbufs = kvmalloc_array(pipe->buffers, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs)\n\t\treturn -ENOMEM;\n\n\tfuse_copy_init(&cs, 1, NULL);\n\tcs.pipebufs = bufs;\n\tcs.pipe = pipe;\n\tret = fuse_dev_do_read(fud, in, &cs, len);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (pipe->nrbufs + cs.nr_segs > pipe->buffers) {\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\tfor (ret = total = 0; page_nr < cs.nr_segs; total += ret) {\n\t\t/*\n\t\t * Need to be careful about this.  Having buf->ops in module\n\t\t * code can Oops if the buffer persists after module unload.\n\t\t */\n\t\tbufs[page_nr].ops = &nosteal_pipe_buf_ops;\n\t\tbufs[page_nr].flags = 0;\n\t\tret = add_to_pipe(pipe, &bufs[page_nr++]);\n\t\tif (unlikely(ret < 0))\n\t\t\tbreak;\n\t}\n\tif (total)\n\t\tret = total;\nout:\n\tfor (; page_nr < cs.nr_segs; page_nr++)\n\t\tput_page(bufs[page_nr].page);\n\n\tkvfree(bufs);\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_335": {
        "vulnerable_code": {
            "Code": "fuse_dev_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t     struct file *out, loff_t *ppos,\n\t\t\t\t     size_t len, unsigned int flags){\n\tunsigned nbuf;\n\tunsigned idx;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud;\n\tsize_t rem;\n\tssize_t ret;\n\n\tfud = fuse_get_dev(out);\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tpipe_lock(pipe);\n\n\tbufs = kvmalloc_array(pipe->nrbufs, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs) {\n\t\tpipe_unlock(pipe);\n\t\treturn -ENOMEM;\n\t}\n\n\tnbuf = 0;\n\trem = 0;\n\tfor (idx = 0; idx < pipe->nrbufs && rem < len; idx++)\n\t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n\n\tret = -EINVAL;\n\tif (rem < len) {\n\t\tpipe_unlock(pipe);\n\t\tgoto out;\n\t}\n\n\trem = len;\n\twhile (rem) {\n\t\tstruct pipe_buffer *ibuf;\n\t\tstruct pipe_buffer *obuf;\n\n\t\tBUG_ON(nbuf >= pipe->buffers);\n\t\tBUG_ON(!pipe->nrbufs);\n\t\tibuf = &pipe->bufs[pipe->curbuf];\n\t\tobuf = &bufs[nbuf];\n\n\t\tif (rem >= ibuf->len) {\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t} else {\n\t\t\tpipe_buf_get(pipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\t\t\tobuf->len = rem;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tnbuf++;\n\t\trem -= obuf->len;\n\t}\n\tpipe_unlock(pipe);\n\n\tfuse_copy_init(&cs, 0, NULL);\n\tcs.pipebufs = bufs;\n\tcs.nr_segs = nbuf;\n\tcs.pipe = pipe;\n\n\tif (flags & SPLICE_F_MOVE)\n\t\tcs.move_pages = 1;\n\n\tret = fuse_dev_do_write(fud, &cs, len);\n\n\tpipe_lock(pipe);\n\tfor (idx = 0; idx < nbuf; idx++)\n\t\tpipe_buf_release(pipe, &bufs[idx]);\n\tpipe_unlock(pipe);\n\nout:\n\tkvfree(bufs);\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_dev_splice_write(struct pipe_inode_info *pipe,\n\t\t\t\t     struct file *out, loff_t *ppos,\n\t\t\t\t     size_t len, unsigned int flags){\n\tunsigned nbuf;\n\tunsigned idx;\n\tstruct pipe_buffer *bufs;\n\tstruct fuse_copy_state cs;\n\tstruct fuse_dev *fud;\n\tsize_t rem;\n\tssize_t ret;\n\n\tfud = fuse_get_dev(out);\n\tif (!fud)\n\t\treturn -EPERM;\n\n\tpipe_lock(pipe);\n\n\tbufs = kvmalloc_array(pipe->nrbufs, sizeof(struct pipe_buffer),\n\t\t\t      GFP_KERNEL);\n\tif (!bufs) {\n\t\tpipe_unlock(pipe);\n\t\treturn -ENOMEM;\n\t}\n\n\tnbuf = 0;\n\trem = 0;\n\tfor (idx = 0; idx < pipe->nrbufs && rem < len; idx++)\n\t\trem += pipe->bufs[(pipe->curbuf + idx) & (pipe->buffers - 1)].len;\n\n\tret = -EINVAL;\n\tif (rem < len)\n\t\tgoto out_free;\n\n\trem = len;\n\twhile (rem) {\n\t\tstruct pipe_buffer *ibuf;\n\t\tstruct pipe_buffer *obuf;\n\n\t\tBUG_ON(nbuf >= pipe->buffers);\n\t\tBUG_ON(!pipe->nrbufs);\n\t\tibuf = &pipe->bufs[pipe->curbuf];\n\t\tobuf = &bufs[nbuf];\n\n\t\tif (rem >= ibuf->len) {\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\tpipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);\n\t\t\tpipe->nrbufs--;\n\t\t} else {\n\t\t\tif (!pipe_buf_get(pipe, ibuf))\n\t\t\t\tgoto out_free;\n\n\t\t\t*obuf = *ibuf;\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\t\t\tobuf->len = rem;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tnbuf++;\n\t\trem -= obuf->len;\n\t}\n\tpipe_unlock(pipe);\n\n\tfuse_copy_init(&cs, 0, NULL);\n\tcs.pipebufs = bufs;\n\tcs.nr_segs = nbuf;\n\tcs.pipe = pipe;\n\n\tif (flags & SPLICE_F_MOVE)\n\t\tcs.move_pages = 1;\n\n\tret = fuse_dev_do_write(fud, &cs, len);\n\n\tpipe_lock(pipe);\nout_free:\n\tfor (idx = 0; idx < nbuf; idx++)\n\t\tpipe_buf_release(pipe, &bufs[idx]);\n\tpipe_unlock(pipe);\n\n\tkvfree(bufs);\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_336": {
        "vulnerable_code": {
            "Code": "fuse_device_clone(struct fuse_conn *fc, struct file *new){\n\tstruct fuse_dev *fud;\n\n\tif (new->private_data)\n\t\treturn -EINVAL;\n\n\tfud = fuse_dev_alloc(fc);\n\tif (!fud)\n\t\treturn -ENOMEM;\n\n\tnew->private_data = fud;\n\tatomic_inc(&fc->dev_count);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_device_clone(struct fuse_conn *fc, struct file *new){\n\tstruct fuse_dev *fud;\n\n\tif (new->private_data)\n\t\treturn -EINVAL;\n\n\tfud = fuse_dev_alloc(fc);\n\tif (!fud)\n\t\treturn -ENOMEM;\n\n\tnew->private_data = fud;\n\tatomic_inc(&fc->dev_count);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_337": {
        "vulnerable_code": {
            "Code": "fuse_notify(struct fuse_conn *fc, enum fuse_notify_code code,\n\t\t       unsigned int size, struct fuse_copy_state *cs){\n\t/* Don't try to move pages (yet) */\n\tcs->move_pages = 0;\n\n\tswitch (code) {\n\tcase FUSE_NOTIFY_POLL:\n\t\treturn fuse_notify_poll(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_INODE:\n\t\treturn fuse_notify_inval_inode(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_ENTRY:\n\t\treturn fuse_notify_inval_entry(fc, size, cs);\n\n\tcase FUSE_NOTIFY_STORE:\n\t\treturn fuse_notify_store(fc, size, cs);\n\n\tcase FUSE_NOTIFY_RETRIEVE:\n\t\treturn fuse_notify_retrieve(fc, size, cs);\n\n\tcase FUSE_NOTIFY_DELETE:\n\t\treturn fuse_notify_delete(fc, size, cs);\n\n\tdefault:\n\t\tfuse_copy_finish(cs);\n\t\treturn -EINVAL;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_notify(struct fuse_conn *fc, enum fuse_notify_code code,\n\t\t       unsigned int size, struct fuse_copy_state *cs){\n\t/* Don't try to move pages (yet) */\n\tcs->move_pages = 0;\n\n\tswitch (code) {\n\tcase FUSE_NOTIFY_POLL:\n\t\treturn fuse_notify_poll(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_INODE:\n\t\treturn fuse_notify_inval_inode(fc, size, cs);\n\n\tcase FUSE_NOTIFY_INVAL_ENTRY:\n\t\treturn fuse_notify_inval_entry(fc, size, cs);\n\n\tcase FUSE_NOTIFY_STORE:\n\t\treturn fuse_notify_store(fc, size, cs);\n\n\tcase FUSE_NOTIFY_RETRIEVE:\n\t\treturn fuse_notify_retrieve(fc, size, cs);\n\n\tcase FUSE_NOTIFY_DELETE:\n\t\treturn fuse_notify_delete(fc, size, cs);\n\n\tdefault:\n\t\tfuse_copy_finish(cs);\n\t\treturn -EINVAL;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_338": {
        "vulnerable_code": {
            "Code": "fuse_req_hash(u64 unique){\n\treturn hash_long(unique & ~FUSE_INT_REQ_BIT, FUSE_PQ_HASH_BITS);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_req_hash(u64 unique){\n\treturn hash_long(unique & ~FUSE_INT_REQ_BIT, FUSE_PQ_HASH_BITS);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_339": {
        "vulnerable_code": {
            "Code": "fuse_req_pages_free(struct fuse_req *req){\n\tif (req->pages != req->inline_pages)\n\t\tkfree(req->pages);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_req_pages_free(struct fuse_req *req){\n\tif (req->pages != req->inline_pages)\n\t\tkfree(req->pages);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_340": {
        "vulnerable_code": {
            "Code": "fuse_req_realloc_pages(struct fuse_conn *fc, struct fuse_req *req,\n\t\t\t    gfp_t flags){\n\tstruct page **pages;\n\tstruct fuse_page_desc *page_descs;\n\tunsigned int npages = min_t(unsigned int,\n\t\t\t\t    max_t(unsigned int, req->max_pages * 2,\n\t\t\t\t\t  FUSE_DEFAULT_MAX_PAGES_PER_REQ),\n\t\t\t\t    fc->max_pages);\n\tWARN_ON(npages <= req->max_pages);\n\n\tpages = fuse_req_pages_alloc(npages, flags, &page_descs);\n\tif (!pages)\n\t\treturn false;\n\n\tmemcpy(pages, req->pages, sizeof(struct page *) * req->max_pages);\n\tmemcpy(page_descs, req->page_descs,\n\t       sizeof(struct fuse_page_desc) * req->max_pages);\n\tfuse_req_pages_free(req);\n\treq->pages = pages;\n\treq->page_descs = page_descs;\n\treq->max_pages = npages;\n\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_req_realloc_pages(struct fuse_conn *fc, struct fuse_req *req,\n\t\t\t    gfp_t flags){\n\tstruct page **pages;\n\tstruct fuse_page_desc *page_descs;\n\tunsigned int npages = min_t(unsigned int,\n\t\t\t\t    max_t(unsigned int, req->max_pages * 2,\n\t\t\t\t\t  FUSE_DEFAULT_MAX_PAGES_PER_REQ),\n\t\t\t\t    fc->max_pages);\n\tWARN_ON(npages <= req->max_pages);\n\n\tpages = fuse_req_pages_alloc(npages, flags, &page_descs);\n\tif (!pages)\n\t\treturn false;\n\n\tmemcpy(pages, req->pages, sizeof(struct page *) * req->max_pages);\n\tmemcpy(page_descs, req->page_descs,\n\t       sizeof(struct fuse_page_desc) * req->max_pages);\n\tfuse_req_pages_free(req);\n\treq->pages = pages;\n\treq->page_descs = page_descs;\n\treq->max_pages = npages;\n\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_341": {
        "vulnerable_code": {
            "Code": "fuse_request_init(struct fuse_req *req, struct page **pages,\n\t\t\t      struct fuse_page_desc *page_descs,\n\t\t\t      unsigned npages){\n\tINIT_LIST_HEAD(&req->list);\n\tINIT_LIST_HEAD(&req->intr_entry);\n\tinit_waitqueue_head(&req->waitq);\n\trefcount_set(&req->count, 1);\n\treq->pages = pages;\n\treq->page_descs = page_descs;\n\treq->max_pages = npages;\n\t__set_bit(FR_PENDING, &req->flags);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fuse_request_init(struct fuse_req *req, struct page **pages,\n\t\t\t      struct fuse_page_desc *page_descs,\n\t\t\t      unsigned npages){\n\tINIT_LIST_HEAD(&req->list);\n\tINIT_LIST_HEAD(&req->intr_entry);\n\tinit_waitqueue_head(&req->waitq);\n\trefcount_set(&req->count, 1);\n\treq->pages = pages;\n\treq->page_descs = page_descs;\n\treq->max_pages = npages;\n\t__set_bit(FR_PENDING, &req->flags);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_342": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_343": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_344": {
        "vulnerable_code": {
            "Code": "lock_request(struct fuse_req *req){\n\tint err = 0;\n\tif (req) {\n\t\tspin_lock(&req->waitq.lock);\n\t\tif (test_bit(FR_ABORTED, &req->flags))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\tset_bit(FR_LOCKED, &req->flags);\n\t\tspin_unlock(&req->waitq.lock);\n\t}\n\treturn err;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "lock_request(struct fuse_req *req){\n\tint err = 0;\n\tif (req) {\n\t\tspin_lock(&req->waitq.lock);\n\t\tif (test_bit(FR_ABORTED, &req->flags))\n\t\t\terr = -ENOENT;\n\t\telse\n\t\t\tset_bit(FR_LOCKED, &req->flags);\n\t\tspin_unlock(&req->waitq.lock);\n\t}\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_345": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_346": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_347": {
        "vulnerable_code": {
            "Code": "pipe_fcntl(struct file *file, unsigned int cmd, unsigned long arg){\n\tstruct pipe_inode_info *pipe;\n\tlong ret;\n\n\tpipe = get_pipe_info(file);\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\t__pipe_lock(pipe);\n\n\tswitch (cmd) {\n\tcase F_SETPIPE_SZ:\n\t\tret = pipe_set_size(pipe, arg);\n\t\tbreak;\n\tcase F_GETPIPE_SZ:\n\t\tret = pipe->buffers * PAGE_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\t__pipe_unlock(pipe);\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "pipe_fcntl(struct file *file, unsigned int cmd, unsigned long arg){\n\tstruct pipe_inode_info *pipe;\n\tlong ret;\n\n\tpipe = get_pipe_info(file);\n\tif (!pipe)\n\t\treturn -EBADF;\n\n\t__pipe_lock(pipe);\n\n\tswitch (cmd) {\n\tcase F_SETPIPE_SZ:\n\t\tret = pipe_set_size(pipe, arg);\n\t\tbreak;\n\tcase F_GETPIPE_SZ:\n\t\tret = pipe->buffers * PAGE_SIZE;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\t__pipe_unlock(pipe);\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_348": {
        "vulnerable_code": {
            "Code": "pipe_release(struct inode *inode, struct file *file){\n\tstruct pipe_inode_info *pipe = file->private_data;\n\n\t__pipe_lock(pipe);\n\tif (file->f_mode & FMODE_READ)\n\t\tpipe->readers--;\n\tif (file->f_mode & FMODE_WRITE)\n\t\tpipe->writers--;\n\n\tif (pipe->readers || pipe->writers) {\n\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLIN | EPOLLOUT | EPOLLRDNORM | EPOLLWRNORM | EPOLLERR | EPOLLHUP);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t}\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "pipe_release(struct inode *inode, struct file *file){\n\tstruct pipe_inode_info *pipe = file->private_data;\n\n\t__pipe_lock(pipe);\n\tif (file->f_mode & FMODE_READ)\n\t\tpipe->readers--;\n\tif (file->f_mode & FMODE_WRITE)\n\t\tpipe->writers--;\n\n\tif (pipe->readers || pipe->writers) {\n\t\twake_up_interruptible_sync_poll(&pipe->wait, EPOLLIN | EPOLLOUT | EPOLLRDNORM | EPOLLWRNORM | EPOLLERR | EPOLLHUP);\n\t\tkill_fasync(&pipe->fasync_readers, SIGIO, POLL_IN);\n\t\tkill_fasync(&pipe->fasync_writers, SIGIO, POLL_OUT);\n\t}\n\t__pipe_unlock(pipe);\n\n\tput_pipe_info(inode, pipe);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_349": {
        "vulnerable_code": {
            "Code": "too_many_pipe_buffers_soft(unsigned long user_bufs){\n\tunsigned long soft_limit = READ_ONCE(pipe_user_pages_soft);\n\n\treturn soft_limit && user_bufs > soft_limit;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "too_many_pipe_buffers_soft(unsigned long user_bufs){\n\tunsigned long soft_limit = READ_ONCE(pipe_user_pages_soft);\n\n\treturn soft_limit && user_bufs > soft_limit;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_350": {
        "vulnerable_code": {
            "Code": "add_to_pipe(struct pipe_inode_info *pipe, struct pipe_buffer *buf){\n\tint ret;\n\n\tif (unlikely(!pipe->readers)) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t} else if (pipe->nrbufs == pipe->buffers) {\n\t\tret = -EAGAIN;\n\t} else {\n\t\tint newbuf = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\tpipe->bufs[newbuf] = *buf;\n\t\tpipe->nrbufs++;\n\t\treturn buf->len;\n\t}\n\tpipe_buf_release(pipe, buf);\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "add_to_pipe(struct pipe_inode_info *pipe, struct pipe_buffer *buf){\n\tint ret;\n\n\tif (unlikely(!pipe->readers)) {\n\t\tsend_sig(SIGPIPE, current, 0);\n\t\tret = -EPIPE;\n\t} else if (pipe->nrbufs == pipe->buffers) {\n\t\tret = -EAGAIN;\n\t} else {\n\t\tint newbuf = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);\n\t\tpipe->bufs[newbuf] = *buf;\n\t\tpipe->nrbufs++;\n\t\treturn buf->len;\n\t}\n\tpipe_buf_release(pipe, buf);\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_351": {
        "vulnerable_code": {
            "Code": "",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "do_splice_from(struct pipe_inode_info *pipe, struct file *out,\n\t\t\t   loff_t *ppos, size_t len, unsigned int flags){\n\tssize_t (*splice_write)(struct pipe_inode_info *, struct file *,\n\t\t\t\tloff_t *, size_t, unsigned int);\n\n\tif (out->f_op->splice_write)\n\t\tsplice_write = out->f_op->splice_write;\n\telse\n\t\tsplice_write = default_file_splice_write;\n\n\treturn splice_write(pipe, out, ppos, len, flags);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_352": {
        "vulnerable_code": {
            "Code": "do_splice_to(struct file *in, loff_t *ppos,\n\t\t\t struct pipe_inode_info *pipe, size_t len,\n\t\t\t unsigned int flags){\n\tssize_t (*splice_read)(struct file *, loff_t *,\n\t\t\t       struct pipe_inode_info *, size_t, unsigned int);\n\tint ret;\n\n\tif (unlikely(!(in->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\n\tret = rw_verify_area(READ, in, ppos, len);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (unlikely(len > MAX_RW_COUNT))\n\t\tlen = MAX_RW_COUNT;\n\n\tif (in->f_op->splice_read)\n\t\tsplice_read = in->f_op->splice_read;\n\telse\n\t\tsplice_read = default_file_splice_read;\n\n\treturn splice_read(in, ppos, pipe, len, flags);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "do_splice_to(struct file *in, loff_t *ppos,\n\t\t\t struct pipe_inode_info *pipe, size_t len,\n\t\t\t unsigned int flags){\n\tssize_t (*splice_read)(struct file *, loff_t *,\n\t\t\t       struct pipe_inode_info *, size_t, unsigned int);\n\tint ret;\n\n\tif (unlikely(!(in->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\n\tret = rw_verify_area(READ, in, ppos, len);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (unlikely(len > MAX_RW_COUNT))\n\t\tlen = MAX_RW_COUNT;\n\n\tif (in->f_op->splice_read)\n\t\tsplice_read = in->f_op->splice_read;\n\telse\n\t\tsplice_read = default_file_splice_read;\n\n\treturn splice_read(in, ppos, pipe, len, flags);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_353": {
        "vulnerable_code": {
            "Code": "do_tee(struct file *in, struct file *out, size_t len,\n\t\t   unsigned int flags){\n\tstruct pipe_inode_info *ipipe = get_pipe_info(in);\n\tstruct pipe_inode_info *opipe = get_pipe_info(out);\n\tint ret = -EINVAL;\n\n\t/*\n\t * Duplicate the contents of ipipe to opipe without actually\n\t * copying the data.\n\t */\n\tif (ipipe && opipe && ipipe != opipe) {\n\t\t/*\n\t\t * Keep going, unless we encounter an error. The ipipe/opipe\n\t\t * ordering doesn't really matter.\n\t\t */\n\t\tret = ipipe_prep(ipipe, flags);\n\t\tif (!ret) {\n\t\t\tret = opipe_prep(opipe, flags);\n\t\t\tif (!ret)\n\t\t\t\tret = link_pipe(ipipe, opipe, len, flags);\n\t\t}\n\t}\n\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "do_tee(struct file *in, struct file *out, size_t len,\n\t\t   unsigned int flags){\n\tstruct pipe_inode_info *ipipe = get_pipe_info(in);\n\tstruct pipe_inode_info *opipe = get_pipe_info(out);\n\tint ret = -EINVAL;\n\n\t/*\n\t * Duplicate the contents of ipipe to opipe without actually\n\t * copying the data.\n\t */\n\tif (ipipe && opipe && ipipe != opipe) {\n\t\t/*\n\t\t * Keep going, unless we encounter an error. The ipipe/opipe\n\t\t * ordering doesn't really matter.\n\t\t */\n\t\tret = ipipe_prep(ipipe, flags);\n\t\tif (!ret) {\n\t\t\tret = opipe_prep(opipe, flags);\n\t\t\tif (!ret)\n\t\t\t\tret = link_pipe(ipipe, opipe, len, flags);\n\t\t}\n\t}\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_354": {
        "vulnerable_code": {
            "Code": "do_vmsplice(struct file *f, struct iov_iter *iter, unsigned int flags){\n\tif (unlikely(flags & ~SPLICE_F_ALL))\n\t\treturn -EINVAL;\n\n\tif (!iov_iter_count(iter))\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == WRITE)\n\t\treturn vmsplice_to_pipe(f, iter, flags);\n\telse\n\t\treturn vmsplice_to_user(f, iter, flags);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "do_vmsplice(struct file *f, struct iov_iter *iter, unsigned int flags){\n\tif (unlikely(flags & ~SPLICE_F_ALL))\n\t\treturn -EINVAL;\n\n\tif (!iov_iter_count(iter))\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == WRITE)\n\t\treturn vmsplice_to_pipe(f, iter, flags);\n\telse\n\t\treturn vmsplice_to_user(f, iter, flags);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_355": {
        "vulnerable_code": {
            "Code": "link_pipe(struct pipe_inode_info *ipipe,\n\t\t     struct pipe_inode_info *opipe,\n\t\t     size_t len, unsigned int flags){\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, i = 0, nbuf;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If we have iterated all input buffers or ran out of\n\t\t * output room, break.\n\t\t */\n\t\tif (i >= ipipe->nrbufs || opipe->nrbufs >= opipe->buffers)\n\t\t\tbreak;\n\n\t\tibuf = ipipe->bufs + ((ipipe->curbuf + i) & (ipipe->buffers-1));\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\n\t\t/*\n\t\t * Get a reference to this pipe buffer,\n\t\t * so we can copy the contents over.\n\t\t */\n\t\tpipe_buf_get(ipipe, ibuf);\n\n\t\tobuf = opipe->bufs + nbuf;\n\t\t*obuf = *ibuf;\n\n\t\t/*\n\t\t * Don't inherit the gift flag, we need to\n\t\t * prevent multiple steals of this page.\n\t\t */\n\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\tif (obuf->len > len)\n\t\t\tobuf->len = len;\n\n\t\topipe->nrbufs++;\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t\ti++;\n\t} while (len);\n\n\t/*\n\t * return EAGAIN if we have the potential of some data in the\n\t * future, otherwise just return 0\n\t */\n\tif (!ret && ipipe->waiting_writers && (flags & SPLICE_F_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "link_pipe(struct pipe_inode_info *ipipe,\n\t\t     struct pipe_inode_info *opipe,\n\t\t     size_t len, unsigned int flags){\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, i = 0, nbuf;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * If we have iterated all input buffers or ran out of\n\t\t * output room, break.\n\t\t */\n\t\tif (i >= ipipe->nrbufs || opipe->nrbufs >= opipe->buffers)\n\t\t\tbreak;\n\n\t\tibuf = ipipe->bufs + ((ipipe->curbuf + i) & (ipipe->buffers-1));\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\n\t\t/*\n\t\t * Get a reference to this pipe buffer,\n\t\t * so we can copy the contents over.\n\t\t */\n\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n\t\t\tif (ret == 0)\n\t\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tobuf = opipe->bufs + nbuf;\n\t\t*obuf = *ibuf;\n\n\t\t/*\n\t\t * Don't inherit the gift flag, we need to\n\t\t * prevent multiple steals of this page.\n\t\t */\n\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\tif (obuf->len > len)\n\t\t\tobuf->len = len;\n\n\t\topipe->nrbufs++;\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t\ti++;\n\t} while (len);\n\n\t/*\n\t * return EAGAIN if we have the potential of some data in the\n\t * future, otherwise just return 0\n\t */\n\tif (!ret && ipipe->waiting_writers && (flags & SPLICE_F_NONBLOCK))\n\t\tret = -EAGAIN;\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_356": {
        "vulnerable_code": {
            "Code": "page_cache_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t     struct pipe_buffer *buf){\n\tstruct page *page = buf->page;\n\tstruct address_space *mapping;\n\n\tlock_page(page);\n\n\tmapping = page_mapping(page);\n\tif (mapping) {\n\t\tWARN_ON(!PageUptodate(page));\n\n\t\t/*\n\t\t * At least for ext2 with nobh option, we need to wait on\n\t\t * writeback completing on this page, since we'll remove it\n\t\t * from the pagecache.  Otherwise truncate wont wait on the\n\t\t * page, allowing the disk blocks to be reused by someone else\n\t\t * before we actually wrote our data to them. fs corruption\n\t\t * ensues.\n\t\t */\n\t\twait_on_page_writeback(page);\n\n\t\tif (page_has_private(page) &&\n\t\t    !try_to_release_page(page, GFP_KERNEL))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If we succeeded in removing the mapping, set LRU flag\n\t\t * and return good.\n\t\t */\n\t\tif (remove_mapping(mapping, page)) {\n\t\t\tbuf->flags |= PIPE_BUF_FLAG_LRU;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Raced with truncate or failed to remove page from current\n\t * address space, unlock and return failure.\n\t */\nout_unlock:\n\tunlock_page(page);\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "page_cache_pipe_buf_steal(struct pipe_inode_info *pipe,\n\t\t\t\t     struct pipe_buffer *buf){\n\tstruct page *page = buf->page;\n\tstruct address_space *mapping;\n\n\tlock_page(page);\n\n\tmapping = page_mapping(page);\n\tif (mapping) {\n\t\tWARN_ON(!PageUptodate(page));\n\n\t\t/*\n\t\t * At least for ext2 with nobh option, we need to wait on\n\t\t * writeback completing on this page, since we'll remove it\n\t\t * from the pagecache.  Otherwise truncate wont wait on the\n\t\t * page, allowing the disk blocks to be reused by someone else\n\t\t * before we actually wrote our data to them. fs corruption\n\t\t * ensues.\n\t\t */\n\t\twait_on_page_writeback(page);\n\n\t\tif (page_has_private(page) &&\n\t\t    !try_to_release_page(page, GFP_KERNEL))\n\t\t\tgoto out_unlock;\n\n\t\t/*\n\t\t * If we succeeded in removing the mapping, set LRU flag\n\t\t * and return good.\n\t\t */\n\t\tif (remove_mapping(mapping, page)) {\n\t\t\tbuf->flags |= PIPE_BUF_FLAG_LRU;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Raced with truncate or failed to remove page from current\n\t * address space, unlock and return failure.\n\t */\nout_unlock:\n\tunlock_page(page);\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_357": {
        "vulnerable_code": {
            "Code": "splice_from_pipe_end(struct pipe_inode_info *pipe, struct splice_desc *sd){\n\tif (sd->need_wakeup)\n\t\twakeup_pipe_writers(pipe);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "splice_from_pipe_end(struct pipe_inode_info *pipe, struct splice_desc *sd){\n\tif (sd->need_wakeup)\n\t\twakeup_pipe_writers(pipe);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_358": {
        "vulnerable_code": {
            "Code": "splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags){\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, nbuf;\n\tbool input_wakeup = false;\n\n\nretry:\n\tret = ipipe_prep(ipipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = opipe_prep(opipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipipe->nrbufs && !ipipe->writers)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Cannot make any progress, because either the input\n\t\t * pipe is empty or the output pipe is full.\n\t\t */\n\t\tif (!ipipe->nrbufs || opipe->nrbufs >= opipe->buffers) {\n\t\t\t/* Already processed some buffers, break */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We raced with another reader/writer and haven't\n\t\t\t * managed to process any buffers.  A zero return\n\t\t\t * value means EOF, so retry instead.\n\t\t\t */\n\t\t\tpipe_unlock(ipipe);\n\t\t\tpipe_unlock(opipe);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tibuf = ipipe->bufs + ipipe->curbuf;\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\t\tobuf = opipe->bufs + nbuf;\n\n\t\tif (len >= ibuf->len) {\n\t\t\t/*\n\t\t\t * Simply move the whole buffer from ipipe to opipe\n\t\t\t */\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\topipe->nrbufs++;\n\t\t\tipipe->curbuf = (ipipe->curbuf + 1) & (ipipe->buffers - 1);\n\t\t\tipipe->nrbufs--;\n\t\t\tinput_wakeup = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Get a reference to this pipe buffer,\n\t\t\t * so we can copy the contents over.\n\t\t\t */\n\t\t\tpipe_buf_get(ipipe, ibuf);\n\t\t\t*obuf = *ibuf;\n\n\t\t\t/*\n\t\t\t * Don't inherit the gift flag, we need to\n\t\t\t * prevent multiple steals of this page.\n\t\t\t */\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\t\tobuf->len = len;\n\t\t\topipe->nrbufs++;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t} while (len);\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\tif (input_wakeup)\n\t\twakeup_pipe_writers(ipipe);\n\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "splice_pipe_to_pipe(struct pipe_inode_info *ipipe,\n\t\t\t       struct pipe_inode_info *opipe,\n\t\t\t       size_t len, unsigned int flags){\n\tstruct pipe_buffer *ibuf, *obuf;\n\tint ret = 0, nbuf;\n\tbool input_wakeup = false;\n\n\nretry:\n\tret = ipipe_prep(ipipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = opipe_prep(opipe, flags);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Potential ABBA deadlock, work around it by ordering lock\n\t * grabbing by pipe info address. Otherwise two different processes\n\t * could deadlock (one doing tee from A -> B, the other from B -> A).\n\t */\n\tpipe_double_lock(ipipe, opipe);\n\n\tdo {\n\t\tif (!opipe->readers) {\n\t\t\tsend_sig(SIGPIPE, current, 0);\n\t\t\tif (!ret)\n\t\t\t\tret = -EPIPE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!ipipe->nrbufs && !ipipe->writers)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Cannot make any progress, because either the input\n\t\t * pipe is empty or the output pipe is full.\n\t\t */\n\t\tif (!ipipe->nrbufs || opipe->nrbufs >= opipe->buffers) {\n\t\t\t/* Already processed some buffers, break */\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (flags & SPLICE_F_NONBLOCK) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * We raced with another reader/writer and haven't\n\t\t\t * managed to process any buffers.  A zero return\n\t\t\t * value means EOF, so retry instead.\n\t\t\t */\n\t\t\tpipe_unlock(ipipe);\n\t\t\tpipe_unlock(opipe);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tibuf = ipipe->bufs + ipipe->curbuf;\n\t\tnbuf = (opipe->curbuf + opipe->nrbufs) & (opipe->buffers - 1);\n\t\tobuf = opipe->bufs + nbuf;\n\n\t\tif (len >= ibuf->len) {\n\t\t\t/*\n\t\t\t * Simply move the whole buffer from ipipe to opipe\n\t\t\t */\n\t\t\t*obuf = *ibuf;\n\t\t\tibuf->ops = NULL;\n\t\t\topipe->nrbufs++;\n\t\t\tipipe->curbuf = (ipipe->curbuf + 1) & (ipipe->buffers - 1);\n\t\t\tipipe->nrbufs--;\n\t\t\tinput_wakeup = true;\n\t\t} else {\n\t\t\t/*\n\t\t\t * Get a reference to this pipe buffer,\n\t\t\t * so we can copy the contents over.\n\t\t\t */\n\t\t\tif (!pipe_buf_get(ipipe, ibuf)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t*obuf = *ibuf;\n\n\t\t\t/*\n\t\t\t * Don't inherit the gift flag, we need to\n\t\t\t * prevent multiple steals of this page.\n\t\t\t */\n\t\t\tobuf->flags &= ~PIPE_BUF_FLAG_GIFT;\n\n\t\t\tobuf->len = len;\n\t\t\topipe->nrbufs++;\n\t\t\tibuf->offset += obuf->len;\n\t\t\tibuf->len -= obuf->len;\n\t\t}\n\t\tret += obuf->len;\n\t\tlen -= obuf->len;\n\t} while (len);\n\n\tpipe_unlock(ipipe);\n\tpipe_unlock(opipe);\n\n\t/*\n\t * If we put data in the output pipe, wakeup any potential readers.\n\t */\n\tif (ret > 0)\n\t\twakeup_pipe_readers(opipe);\n\n\tif (input_wakeup)\n\t\twakeup_pipe_writers(ipipe);\n\n\treturn ret;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_359": {
        "vulnerable_code": {
            "Code": "__ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tint skip, int pc, struct pt_regs *regs){\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__ftrace_trace_stack(struct ring_buffer *buffer,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tint skip, int pc, struct pt_regs *regs){\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_360": {
        "vulnerable_code": {
            "Code": "__trace_puts(unsigned long ip, const char *str, int size){\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tint alloc;\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\talloc = sizeof(*entry) + size + 2; /* possible \\n added */\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc, \n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, str, size);\n\n\t/* Add a newline if necessary */\n\tif (entry->buf[size - 1] != '\\n') {\n\t\tentry->buf[size] = '\\n';\n\t\tentry->buf[size + 1] = '\\0';\n\t} else\n\t\tentry->buf[size] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn size;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__trace_puts(unsigned long ip, const char *str, int size){\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned long irq_flags;\n\tint alloc;\n\tint pc;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tpc = preempt_count();\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\talloc = sizeof(*entry) + size + 2; /* possible \\n added */\n\n\tlocal_save_flags(irq_flags);\n\tbuffer = global_trace.trace_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc, \n\t\t\t\t\t    irq_flags, pc);\n\tif (!event)\n\t\treturn 0;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, str, size);\n\n\t/* Add a newline if necessary */\n\tif (entry->buf[size - 1] != '\\n') {\n\t\tentry->buf[size] = '\\n';\n\t\tentry->buf[size + 1] = '\\0';\n\t} else\n\t\tentry->buf[size] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, irq_flags, 4, pc, NULL);\n\n\treturn size;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_361": {
        "vulnerable_code": {
            "Code": "add_trace_export(struct trace_export **list, struct trace_export *export){\n\trcu_assign_pointer(export->next, *list);\n\t/*\n\t * We are entering export into the list but another\n\t * CPU might be walking that list. We need to make sure\n\t * the export->next pointer is valid before another CPU sees\n\t * the export pointer included into the list.\n\t */\n\trcu_assign_pointer(*list, export);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "add_trace_export(struct trace_export **list, struct trace_export *export){\n\trcu_assign_pointer(export->next, *list);\n\t/*\n\t * We are entering export into the list but another\n\t * CPU might be walking that list. We need to make sure\n\t * the export->next pointer is valid before another CPU sees\n\t * the export pointer included into the list.\n\t */\n\trcu_assign_pointer(*list, export);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_362": {
        "vulnerable_code": {
            "Code": "buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf){\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tref->ref++;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf){\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tif (ref->ref > INT_MAX/2)\n\t\treturn false;\n\n\tref->ref++;\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_363": {
        "vulnerable_code": {
            "Code": "create_trace_options_dir(struct trace_array *tr){\n\tstruct dentry *t_options;\n\tbool top_level = tr == &global_trace;\n\tint i;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (top_level ||\n\t\t    !((1 << i) & TOP_LEVEL_TRACE_FLAGS))\n\t\t\tcreate_trace_option_core_file(tr, trace_options[i], i);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "create_trace_options_dir(struct trace_array *tr){\n\tstruct dentry *t_options;\n\tbool top_level = tr == &global_trace;\n\tint i;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (top_level ||\n\t\t    !((1 << i) & TOP_LEVEL_TRACE_FLAGS))\n\t\t\tcreate_trace_option_core_file(tr, trace_options[i], i);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_364": {
        "vulnerable_code": {
            "Code": "free_saved_cmdlines_buffer(struct saved_cmdlines_buffer *s){\n\tkfree(s->saved_cmdlines);\n\tkfree(s->map_cmdline_to_pid);\n\tkfree(s);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "free_saved_cmdlines_buffer(struct saved_cmdlines_buffer *s){\n\tkfree(s->saved_cmdlines);\n\tkfree(s->map_cmdline_to_pid);\n\tkfree(s);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_365": {
        "vulnerable_code": {
            "Code": "ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs){\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct ring_buffer *buffer,\n\t\t\t\t      unsigned long flags,\n\t\t\t\t      int skip, int pc, struct pt_regs *regs){\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_366": {
        "vulnerable_code": {
            "Code": "get_trace_buf(void){\n\tstruct trace_buffer_struct *buffer = this_cpu_ptr(trace_percpu_buffer);\n\n\tif (!buffer || buffer->nesting >= 4)\n\t\treturn NULL;\n\n\tbuffer->nesting++;\n\n\t/* Interrupts must see nesting incremented before we use the buffer */\n\tbarrier();\n\treturn &buffer->buffer[buffer->nesting][0];\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_trace_buf(void){\n\tstruct trace_buffer_struct *buffer = this_cpu_ptr(trace_percpu_buffer);\n\n\tif (!buffer || buffer->nesting >= 4)\n\t\treturn NULL;\n\n\tbuffer->nesting++;\n\n\t/* Interrupts must see nesting incremented before we use the buffer */\n\tbarrier();\n\treturn &buffer->buffer[buffer->nesting][0];\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_367": {
        "vulnerable_code": {
            "Code": "instance_mkdir(const char *name){\n\tstruct trace_array *tr;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -EEXIST;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\tret = -ENOMEM;\n\ttr = kzalloc(sizeof(*tr), GFP_KERNEL);\n\tif (!tr)\n\t\tgoto out_unlock;\n\n\ttr->name = kstrdup(name, GFP_KERNEL);\n\tif (!tr->name)\n\t\tgoto out_free_tr;\n\n\tif (!alloc_cpumask_var(&tr->tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_tr;\n\n\ttr->trace_flags = global_trace.trace_flags & ~ZEROED_TRACE_FLAGS;\n\n\tcpumask_copy(tr->tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&tr->start_lock);\n\n\ttr->max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\ttr->current_trace = &nop_trace;\n\n\tINIT_LIST_HEAD(&tr->systems);\n\tINIT_LIST_HEAD(&tr->events);\n\tINIT_LIST_HEAD(&tr->hist_vars);\n\n\tif (allocate_trace_buffers(tr, trace_buf_size) < 0)\n\t\tgoto out_free_tr;\n\n\ttr->dir = tracefs_create_dir(name, trace_instance_dir);\n\tif (!tr->dir)\n\t\tgoto out_free_tr;\n\n\tret = event_trace_add_tracer(tr->dir, tr);\n\tif (ret) {\n\t\ttracefs_remove_recursive(tr->dir);\n\t\tgoto out_free_tr;\n\t}\n\n\tftrace_init_trace_array(tr);\n\n\tinit_tracer_tracefs(tr, tr->dir);\n\tinit_trace_flags_index(tr);\n\t__update_tracer_options(tr);\n\n\tlist_add(&tr->list, &ftrace_trace_arrays);\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn 0;\n\n out_free_tr:\n\tfree_trace_buffers(tr);\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "instance_mkdir(const char *name){\n\tstruct trace_array *tr;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -EEXIST;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\tret = -ENOMEM;\n\ttr = kzalloc(sizeof(*tr), GFP_KERNEL);\n\tif (!tr)\n\t\tgoto out_unlock;\n\n\ttr->name = kstrdup(name, GFP_KERNEL);\n\tif (!tr->name)\n\t\tgoto out_free_tr;\n\n\tif (!alloc_cpumask_var(&tr->tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_tr;\n\n\ttr->trace_flags = global_trace.trace_flags & ~ZEROED_TRACE_FLAGS;\n\n\tcpumask_copy(tr->tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&tr->start_lock);\n\n\ttr->max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\ttr->current_trace = &nop_trace;\n\n\tINIT_LIST_HEAD(&tr->systems);\n\tINIT_LIST_HEAD(&tr->events);\n\tINIT_LIST_HEAD(&tr->hist_vars);\n\n\tif (allocate_trace_buffers(tr, trace_buf_size) < 0)\n\t\tgoto out_free_tr;\n\n\ttr->dir = tracefs_create_dir(name, trace_instance_dir);\n\tif (!tr->dir)\n\t\tgoto out_free_tr;\n\n\tret = event_trace_add_tracer(tr->dir, tr);\n\tif (ret) {\n\t\ttracefs_remove_recursive(tr->dir);\n\t\tgoto out_free_tr;\n\t}\n\n\tftrace_init_trace_array(tr);\n\n\tinit_tracer_tracefs(tr, tr->dir);\n\tinit_trace_flags_index(tr);\n\t__update_tracer_options(tr);\n\n\tlist_add(&tr->list, &ftrace_trace_arrays);\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn 0;\n\n out_free_tr:\n\tfree_trace_buffers(tr);\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_368": {
        "vulnerable_code": {
            "Code": "instance_rmdir(const char *name){\n\tstruct trace_array *tr;\n\tint found = 0;\n\tint ret;\n\tint i;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -ENODEV;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\tgoto out_unlock;\n\n\tret = -EBUSY;\n\tif (tr->ref || (tr->current_trace && tr->current_trace->ref))\n\t\tgoto out_unlock;\n\n\tlist_del(&tr->list);\n\n\t/* Disable all the flags that were enabled coming in */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++) {\n\t\tif ((1 << i) & ZEROED_TRACE_FLAGS)\n\t\t\tset_tracer_flag(tr, 1 << i, 0);\n\t}\n\n\ttracing_set_nop(tr);\n\tclear_ftrace_function_probes(tr);\n\tevent_trace_del_tracer(tr);\n\tftrace_clear_pids(tr);\n\tftrace_destroy_function_files(tr);\n\ttracefs_remove_recursive(tr->dir);\n\tfree_trace_buffers(tr);\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\tkfree(tr->topts[i].topts);\n\t}\n\tkfree(tr->topts);\n\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n\tret = 0;\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "instance_rmdir(const char *name){\n\tstruct trace_array *tr;\n\tint found = 0;\n\tint ret;\n\tint i;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -ENODEV;\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\tgoto out_unlock;\n\n\tret = -EBUSY;\n\tif (tr->ref || (tr->current_trace && tr->current_trace->ref))\n\t\tgoto out_unlock;\n\n\tlist_del(&tr->list);\n\n\t/* Disable all the flags that were enabled coming in */\n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++) {\n\t\tif ((1 << i) & ZEROED_TRACE_FLAGS)\n\t\t\tset_tracer_flag(tr, 1 << i, 0);\n\t}\n\n\ttracing_set_nop(tr);\n\tclear_ftrace_function_probes(tr);\n\tevent_trace_del_tracer(tr);\n\tftrace_clear_pids(tr);\n\tftrace_destroy_function_files(tr);\n\ttracefs_remove_recursive(tr->dir);\n\tfree_trace_buffers(tr);\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\tkfree(tr->topts[i].topts);\n\t}\n\tkfree(tr->topts);\n\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n\tret = 0;\n\n out_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_369": {
        "vulnerable_code": {
            "Code": "saved_cmdlines_start(struct seq_file *m, loff_t *pos){\n\tvoid *v;\n\tloff_t l = 0;\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\tv = &savedcmd->map_cmdline_to_pid[0];\n\twhile (l <= *pos) {\n\t\tv = saved_cmdlines_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "saved_cmdlines_start(struct seq_file *m, loff_t *pos){\n\tvoid *v;\n\tloff_t l = 0;\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\tv = &savedcmd->map_cmdline_to_pid[0];\n\twhile (l <= *pos) {\n\t\tv = saved_cmdlines_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_370": {
        "vulnerable_code": {
            "Code": "show_traces_open(struct inode *inode, struct file *file){\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tret = seq_open(file, &show_traces_seq_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tm = file->private_data;\n\tm->private = tr;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "show_traces_open(struct inode *inode, struct file *file){\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m;\n\tint ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tret = seq_open(file, &show_traces_seq_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tm = file->private_data;\n\tm->private = tr;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_371": {
        "vulnerable_code": {
            "Code": "test_cpu_buff_start(struct trace_iterator *iter){\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (!(tr->trace_flags & TRACE_ITER_ANNOTATE))\n\t\treturn;\n\n\tif (!(iter->iter_flags & TRACE_FILE_ANNOTATE))\n\t\treturn;\n\n\tif (cpumask_available(iter->started) &&\n\t    cpumask_test_cpu(iter->cpu, iter->started))\n\t\treturn;\n\n\tif (per_cpu_ptr(iter->trace_buffer->data, iter->cpu)->skipped_entries)\n\t\treturn;\n\n\tif (cpumask_available(iter->started))\n\t\tcpumask_set_cpu(iter->cpu, iter->started);\n\n\t/* Don't print started cpu buffer for the first entry of the trace */\n\tif (iter->idx > 1)\n\t\ttrace_seq_printf(s, \"##### CPU %u buffer started ####\\n\",\n\t\t\t\titer->cpu);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "test_cpu_buff_start(struct trace_iterator *iter){\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (!(tr->trace_flags & TRACE_ITER_ANNOTATE))\n\t\treturn;\n\n\tif (!(iter->iter_flags & TRACE_FILE_ANNOTATE))\n\t\treturn;\n\n\tif (cpumask_available(iter->started) &&\n\t    cpumask_test_cpu(iter->cpu, iter->started))\n\t\treturn;\n\n\tif (per_cpu_ptr(iter->trace_buffer->data, iter->cpu)->skipped_entries)\n\t\treturn;\n\n\tif (cpumask_available(iter->started))\n\t\tcpumask_set_cpu(iter->cpu, iter->started);\n\n\t/* Don't print started cpu buffer for the first entry of the trace */\n\tif (iter->idx > 1)\n\t\ttrace_seq_printf(s, \"##### CPU %u buffer started ####\\n\",\n\t\t\t\titer->cpu);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_372": {
        "vulnerable_code": {
            "Code": "trace_access_lock_init(void){\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmutex_init(&per_cpu(cpu_access_lock, cpu));\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_access_lock_init(void){\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmutex_init(&per_cpu(cpu_access_lock, cpu));\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_373": {
        "vulnerable_code": {
            "Code": "trace_buffer_unlock_commit_nostack(struct ring_buffer *buffer,\n\t\t\t\t   struct ring_buffer_event *event){\n\t__buffer_unlock_commit(buffer, event);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_buffer_unlock_commit_nostack(struct ring_buffer *buffer,\n\t\t\t\t   struct ring_buffer_event *event){\n\t__buffer_unlock_commit(buffer, event);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_374": {
        "vulnerable_code": {
            "Code": "trace_buffered_event_enable(void){\n\tstruct ring_buffer_event *event;\n\tstruct page *page;\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (trace_buffered_event_ref++)\n\t\treturn;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\tif (!page)\n\t\t\tgoto failed;\n\n\t\tevent = page_address(page);\n\t\tmemset(event, 0, sizeof(*event));\n\n\t\tper_cpu(trace_buffered_event, cpu) = event;\n\n\t\tpreempt_disable();\n\t\tif (cpu == smp_processor_id() &&\n\t\t    this_cpu_read(trace_buffered_event) !=\n\t\t    per_cpu(trace_buffered_event, cpu))\n\t\t\tWARN_ON_ONCE(1);\n\t\tpreempt_enable();\n\t}\n\n\treturn;\n failed:\n\ttrace_buffered_event_disable();\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_buffered_event_enable(void){\n\tstruct ring_buffer_event *event;\n\tstruct page *page;\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (trace_buffered_event_ref++)\n\t\treturn;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\tif (!page)\n\t\t\tgoto failed;\n\n\t\tevent = page_address(page);\n\t\tmemset(event, 0, sizeof(*event));\n\n\t\tper_cpu(trace_buffered_event, cpu) = event;\n\n\t\tpreempt_disable();\n\t\tif (cpu == smp_processor_id() &&\n\t\t    this_cpu_read(trace_buffered_event) !=\n\t\t    per_cpu(trace_buffered_event, cpu))\n\t\t\tWARN_ON_ONCE(1);\n\t\tpreempt_enable();\n\t}\n\n\treturn;\n failed:\n\ttrace_buffered_event_disable();\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_375": {
        "vulnerable_code": {
            "Code": "trace_empty(struct trace_iterator *iter){\n\tstruct ring_buffer_iter *buf_iter;\n\tint cpu;\n\n\t/* If we are looking at one CPU buffer, only check that one */\n\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\tcpu = iter->cpu_file;\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_empty(struct trace_iterator *iter){\n\tstruct ring_buffer_iter *buf_iter;\n\tint cpu;\n\n\t/* If we are looking at one CPU buffer, only check that one */\n\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\tcpu = iter->cpu_file;\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->trace_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_376": {
        "vulnerable_code": {
            "Code": "trace_event_buffer_commit(struct trace_event_buffer *fbuffer){\n\tif (static_key_false(&tracepoint_printk_key.key))\n\t\toutput_printk(fbuffer);\n\n\tevent_trigger_unlock_commit(fbuffer->trace_file, fbuffer->buffer,\n\t\t\t\t    fbuffer->event, fbuffer->entry,\n\t\t\t\t    fbuffer->flags, fbuffer->pc);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_event_buffer_commit(struct trace_event_buffer *fbuffer){\n\tif (static_key_false(&tracepoint_printk_key.key))\n\t\toutput_printk(fbuffer);\n\n\tevent_trigger_unlock_commit(fbuffer->trace_file, fbuffer->buffer,\n\t\t\t\t    fbuffer->event, fbuffer->entry,\n\t\t\t\t    fbuffer->flags, fbuffer->pc);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_377": {
        "vulnerable_code": {
            "Code": "trace_function(struct trace_array *tr,\n\t       unsigned long ip, unsigned long parent_ip, unsigned long flags,\n\t       int pc){\n\tstruct trace_event_call *call = &event_function;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_entry *entry;\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->parent_ip\t\t= parent_ip;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\tif (static_branch_unlikely(&ftrace_exports_enabled))\n\t\t\tftrace_exports(event);\n\t\t__buffer_unlock_commit(buffer, event);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_function(struct trace_array *tr,\n\t       unsigned long ip, unsigned long parent_ip, unsigned long flags,\n\t       int pc){\n\tstruct trace_event_call *call = &event_function;\n\tstruct ring_buffer *buffer = tr->trace_buffer.buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_entry *entry;\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),\n\t\t\t\t\t    flags, pc);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->parent_ip\t\t= parent_ip;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\tif (static_branch_unlikely(&ftrace_exports_enabled))\n\t\t\tftrace_exports(event);\n\t\t__buffer_unlock_commit(buffer, event);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_378": {
        "vulnerable_code": {
            "Code": "trace_keep_overwrite(struct tracer *tracer, u32 mask, int set){\n\tif (tracer->enabled && (mask & TRACE_ITER_OVERWRITE) && !set)\n\t\treturn -1;\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_keep_overwrite(struct tracer *tracer, u32 mask, int set){\n\tif (tracer->enabled && (mask & TRACE_ITER_OVERWRITE) && !set)\n\t\treturn -1;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_379": {
        "vulnerable_code": {
            "Code": "trace_printk_start_stop_comm(int enabled){\n\tif (!buffers_allocated)\n\t\treturn;\n\n\tif (enabled)\n\t\ttracing_start_cmdline_record();\n\telse\n\t\ttracing_stop_cmdline_record();\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_printk_start_stop_comm(int enabled){\n\tif (!buffers_allocated)\n\t\treturn;\n\n\tif (enabled)\n\t\ttracing_start_cmdline_record();\n\telse\n\t\ttracing_stop_cmdline_record();\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_380": {
        "vulnerable_code": {
            "Code": "trace_seq_to_buffer(struct trace_seq *s, void *buf, size_t cnt){\n\tint len;\n\n\tif (trace_seq_used(s) <= s->seq.readpos)\n\t\treturn -EBUSY;\n\n\tlen = trace_seq_used(s) - s->seq.readpos;\n\tif (cnt > len)\n\t\tcnt = len;\n\tmemcpy(buf, s->buffer + s->seq.readpos, cnt);\n\n\ts->seq.readpos += cnt;\n\treturn cnt;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "trace_seq_to_buffer(struct trace_seq *s, void *buf, size_t cnt){\n\tint len;\n\n\tif (trace_seq_used(s) <= s->seq.readpos)\n\t\treturn -EBUSY;\n\n\tlen = trace_seq_used(s) - s->seq.readpos;\n\tif (cnt > len)\n\t\tcnt = len;\n\tmemcpy(buf, s->buffer + s->seq.readpos, cnt);\n\n\ts->seq.readpos += cnt;\n\treturn cnt;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_381": {
        "vulnerable_code": {
            "Code": "tracing_is_enabled(void){\n\t/*\n\t * For quick access (irqsoff uses this in fast path), just\n\t * return the mirror variable of the state of the ring buffer.\n\t * It's a little racy, but we don't really care.\n\t */\n\tsmp_rmb();\n\treturn !global_trace.buffer_disabled;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tracing_is_enabled(void){\n\t/*\n\t * For quick access (irqsoff uses this in fast path), just\n\t * return the mirror variable of the state of the ring buffer.\n\t * It's a little racy, but we don't really care.\n\t */\n\tsmp_rmb();\n\treturn !global_trace.buffer_disabled;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_382": {
        "vulnerable_code": {
            "Code": "tracing_is_on(void){\n\treturn tracer_tracing_is_on(&global_trace);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tracing_is_on(void){\n\treturn tracer_tracing_is_on(&global_trace);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_383": {
        "vulnerable_code": {
            "Code": "tracing_release(struct inode *inode, struct file *file){\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m = file->private_data;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (!(file->f_mode & FMODE_READ)) {\n\t\ttrace_array_put(tr);\n\t\treturn 0;\n\t}\n\n\t/* Writes do not use seq_file */\n\titer = m->private;\n\tmutex_lock(&trace_types_lock);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tif (iter->buffer_iter[cpu])\n\t\t\tring_buffer_read_finish(iter->buffer_iter[cpu]);\n\t}\n\n\tif (iter->trace && iter->trace->close)\n\t\titer->trace->close(iter);\n\n\tif (!iter->snapshot)\n\t\t/* reenable tracing if it was previously enabled */\n\t\ttracing_start_tr(tr);\n\n\t__trace_array_put(tr);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tmutex_destroy(&iter->mutex);\n\tfree_cpumask_var(iter->started);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\n\tseq_release_private(inode, file);\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tracing_release(struct inode *inode, struct file *file){\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m = file->private_data;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (!(file->f_mode & FMODE_READ)) {\n\t\ttrace_array_put(tr);\n\t\treturn 0;\n\t}\n\n\t/* Writes do not use seq_file */\n\titer = m->private;\n\tmutex_lock(&trace_types_lock);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tif (iter->buffer_iter[cpu])\n\t\t\tring_buffer_read_finish(iter->buffer_iter[cpu]);\n\t}\n\n\tif (iter->trace && iter->trace->close)\n\t\titer->trace->close(iter);\n\n\tif (!iter->snapshot)\n\t\t/* reenable tracing if it was previously enabled */\n\t\ttracing_start_tr(tr);\n\n\t__trace_array_put(tr);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tmutex_destroy(&iter->mutex);\n\tfree_cpumask_var(iter->started);\n\tkfree(iter->trace);\n\tkfree(iter->buffer_iter);\n\tseq_release_private(inode, file);\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_384": {
        "vulnerable_code": {
            "Code": "tracing_release_pipe(struct inode *inode, struct file *file){\n\tstruct trace_iterator *iter = file->private_data;\n\tstruct trace_array *tr = inode->i_private;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->current_trace->ref--;\n\n\tif (iter->trace->pipe_close)\n\t\titer->trace->pipe_close(iter);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tfree_cpumask_var(iter->started);\n\tmutex_destroy(&iter->mutex);\n\tkfree(iter);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tracing_release_pipe(struct inode *inode, struct file *file){\n\tstruct trace_iterator *iter = file->private_data;\n\tstruct trace_array *tr = inode->i_private;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->current_trace->ref--;\n\n\tif (iter->trace->pipe_close)\n\t\titer->trace->pipe_close(iter);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tfree_cpumask_var(iter->started);\n\tmutex_destroy(&iter->mutex);\n\tkfree(iter);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_385": {
        "vulnerable_code": {
            "Code": "tracing_resize_saved_cmdlines(unsigned int val){\n\tstruct saved_cmdlines_buffer *s, *savedcmd_temp;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tif (allocate_cmdlines_buffer(val, s) < 0) {\n\t\tkfree(s);\n\t\treturn -ENOMEM;\n\t}\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tsavedcmd_temp = savedcmd;\n\tsavedcmd = s;\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tfree_saved_cmdlines_buffer(savedcmd_temp);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tracing_resize_saved_cmdlines(unsigned int val){\n\tstruct saved_cmdlines_buffer *s, *savedcmd_temp;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tif (allocate_cmdlines_buffer(val, s) < 0) {\n\t\tkfree(s);\n\t\treturn -ENOMEM;\n\t}\n\n\tarch_spin_lock(&trace_cmdline_lock);\n\tsavedcmd_temp = savedcmd;\n\tsavedcmd = s;\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tfree_saved_cmdlines_buffer(savedcmd_temp);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_386": {
        "vulnerable_code": {
            "Code": "tracing_set_tracer(struct trace_array *tr, const char *buf){\n\tstruct tracer *t;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbool had_max_tr;\n#endif\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (!ring_buffer_expanded) {\n\t\tret = __tracing_resize_ring_buffer(tr, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = 0;\n\t}\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(t->name, buf) == 0)\n\t\t\tbreak;\n\t}\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (t == tr->current_trace)\n\t\tgoto out;\n\n\t/* Some tracers won't work on kernel command line */\n\tif (system_state < SYSTEM_RUNNING && t->noboot) {\n\t\tpr_warn(\"Tracer '%s' is not allowed on command line, ignored\\n\",\n\t\t\tt->name);\n\t\tgoto out;\n\t}\n\n\t/* Some tracers are only allowed for the top level buffer */\n\tif (!trace_ok_for_array(t, tr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If trace pipe files are being read, we can't change the tracer */\n\tif (tr->current_trace->ref) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\ttrace_branch_disable();\n\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\t/* Current trace needs to be nop_trace before synchronize_rcu */\n\ttr->current_trace = &nop_trace;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\thad_max_tr = tr->allocated_snapshot;\n\n\tif (had_max_tr && !t->use_max_tr) {\n\t\t/*\n\t\t * We need to make sure that the update_max_tr sees that\n\t\t * current_trace changed to nop_trace to keep it from\n\t\t * swapping the buffers after we resize it.\n\t\t * The update_max_tr is called from interrupts disabled\n\t\t * so a synchronized_sched() is sufficient.\n\t\t */\n\t\tsynchronize_rcu();\n\t\tfree_snapshot(tr);\n\t}\n#endif\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (t->use_max_tr && !had_max_tr) {\n\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\n\tif (t->init) {\n\t\tret = tracer_init(t, tr);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\ttr->current_trace = t;\n\ttr->current_trace->enabled++;\n\ttrace_branch_enable(tr);\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tracing_set_tracer(struct trace_array *tr, const char *buf){\n\tstruct tracer *t;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbool had_max_tr;\n#endif\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (!ring_buffer_expanded) {\n\t\tret = __tracing_resize_ring_buffer(tr, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = 0;\n\t}\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(t->name, buf) == 0)\n\t\t\tbreak;\n\t}\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (t == tr->current_trace)\n\t\tgoto out;\n\n\t/* Some tracers won't work on kernel command line */\n\tif (system_state < SYSTEM_RUNNING && t->noboot) {\n\t\tpr_warn(\"Tracer '%s' is not allowed on command line, ignored\\n\",\n\t\t\tt->name);\n\t\tgoto out;\n\t}\n\n\t/* Some tracers are only allowed for the top level buffer */\n\tif (!trace_ok_for_array(t, tr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t/* If trace pipe files are being read, we can't change the tracer */\n\tif (tr->current_trace->ref) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\ttrace_branch_disable();\n\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\t/* Current trace needs to be nop_trace before synchronize_rcu */\n\ttr->current_trace = &nop_trace;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\thad_max_tr = tr->allocated_snapshot;\n\n\tif (had_max_tr && !t->use_max_tr) {\n\t\t/*\n\t\t * We need to make sure that the update_max_tr sees that\n\t\t * current_trace changed to nop_trace to keep it from\n\t\t * swapping the buffers after we resize it.\n\t\t * The update_max_tr is called from interrupts disabled\n\t\t * so a synchronized_sched() is sufficient.\n\t\t */\n\t\tsynchronize_rcu();\n\t\tfree_snapshot(tr);\n\t}\n#endif\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (t->use_max_tr && !had_max_tr) {\n\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n#endif\n\n\tif (t->init) {\n\t\tret = tracer_init(t, tr);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\ttr->current_trace = t;\n\ttr->current_trace->enabled++;\n\ttrace_branch_enable(tr);\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_387": {
        "vulnerable_code": {
            "Code": "tracing_snapshot_alloc(void){\n\tint ret;\n\n\tret = tracing_alloc_snapshot();\n\tif (ret < 0)\n\t\treturn;\n\n\ttracing_snapshot();\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tracing_snapshot_alloc(void){\n\tint ret;\n\n\tret = tracing_alloc_snapshot();\n\tif (ret < 0)\n\t\treturn;\n\n\ttracing_snapshot();\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_388": {
        "vulnerable_code": {
            "Code": "__get_user_pages_locked(struct task_struct *tsk,\n\t\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\t\tunsigned long start,\n\t\t\t\t\t\tunsigned long nr_pages,\n\t\t\t\t\t\tstruct page **pages,\n\t\t\t\t\t\tstruct vm_area_struct **vmas,\n\t\t\t\t\t\tint *locked,\n\t\t\t\t\t\tunsigned int flags){\n\tlong ret, pages_done;\n\tbool lock_dropped;\n\n\tif (locked) {\n\t\t/* if VM_FAULT_RETRY can be returned, vmas become invalid */\n\t\tBUG_ON(vmas);\n\t\t/* check caller initialized locked */\n\t\tBUG_ON(*locked != 1);\n\t}\n\n\tif (pages)\n\t\tflags |= FOLL_GET;\n\n\tpages_done = 0;\n\tlock_dropped = false;\n\tfor (;;) {\n\t\tret = __get_user_pages(tsk, mm, start, nr_pages, flags, pages,\n\t\t\t\t       vmas, locked);\n\t\tif (!locked)\n\t\t\t/* VM_FAULT_RETRY couldn't trigger, bypass */\n\t\t\treturn ret;\n\n\t\t/* VM_FAULT_RETRY cannot return errors */\n\t\tif (!*locked) {\n\t\t\tBUG_ON(ret < 0);\n\t\t\tBUG_ON(ret >= nr_pages);\n\t\t}\n\n\t\tif (!pages)\n\t\t\t/* If it's a prefault don't insist harder */\n\t\t\treturn ret;\n\n\t\tif (ret > 0) {\n\t\t\tnr_pages -= ret;\n\t\t\tpages_done += ret;\n\t\t\tif (!nr_pages)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (*locked) {\n\t\t\t/*\n\t\t\t * VM_FAULT_RETRY didn't trigger or it was a\n\t\t\t * FOLL_NOWAIT.\n\t\t\t */\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\t/* VM_FAULT_RETRY triggered, so seek to the faulting offset */\n\t\tpages += ret;\n\t\tstart += ret << PAGE_SHIFT;\n\n\t\t/*\n\t\t * Repeat on the address that fired VM_FAULT_RETRY\n\t\t * without FAULT_FLAG_ALLOW_RETRY but with\n\t\t * FAULT_FLAG_TRIED.\n\t\t */\n\t\t*locked = 1;\n\t\tlock_dropped = true;\n\t\tdown_read(&mm->mmap_sem);\n\t\tret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,\n\t\t\t\t       pages, NULL, NULL);\n\t\tif (ret != 1) {\n\t\t\tBUG_ON(ret > 1);\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\tnr_pages--;\n\t\tpages_done++;\n\t\tif (!nr_pages)\n\t\t\tbreak;\n\t\tpages++;\n\t\tstart += PAGE_SIZE;\n\t}\n\tif (lock_dropped && *locked) {\n\t\t/*\n\t\t * We must let the caller know we temporarily dropped the lock\n\t\t * and so the critical section protected by it was lost.\n\t\t */\n\t\tup_read(&mm->mmap_sem);\n\t\t*locked = 0;\n\t}\n\treturn pages_done;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__get_user_pages_locked(struct task_struct *tsk,\n\t\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\t\tunsigned long start,\n\t\t\t\t\t\tunsigned long nr_pages,\n\t\t\t\t\t\tstruct page **pages,\n\t\t\t\t\t\tstruct vm_area_struct **vmas,\n\t\t\t\t\t\tint *locked,\n\t\t\t\t\t\tunsigned int flags){\n\tlong ret, pages_done;\n\tbool lock_dropped;\n\n\tif (locked) {\n\t\t/* if VM_FAULT_RETRY can be returned, vmas become invalid */\n\t\tBUG_ON(vmas);\n\t\t/* check caller initialized locked */\n\t\tBUG_ON(*locked != 1);\n\t}\n\n\tif (pages)\n\t\tflags |= FOLL_GET;\n\n\tpages_done = 0;\n\tlock_dropped = false;\n\tfor (;;) {\n\t\tret = __get_user_pages(tsk, mm, start, nr_pages, flags, pages,\n\t\t\t\t       vmas, locked);\n\t\tif (!locked)\n\t\t\t/* VM_FAULT_RETRY couldn't trigger, bypass */\n\t\t\treturn ret;\n\n\t\t/* VM_FAULT_RETRY cannot return errors */\n\t\tif (!*locked) {\n\t\t\tBUG_ON(ret < 0);\n\t\t\tBUG_ON(ret >= nr_pages);\n\t\t}\n\n\t\tif (!pages)\n\t\t\t/* If it's a prefault don't insist harder */\n\t\t\treturn ret;\n\n\t\tif (ret > 0) {\n\t\t\tnr_pages -= ret;\n\t\t\tpages_done += ret;\n\t\t\tif (!nr_pages)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (*locked) {\n\t\t\t/*\n\t\t\t * VM_FAULT_RETRY didn't trigger or it was a\n\t\t\t * FOLL_NOWAIT.\n\t\t\t */\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\t/* VM_FAULT_RETRY triggered, so seek to the faulting offset */\n\t\tpages += ret;\n\t\tstart += ret << PAGE_SHIFT;\n\n\t\t/*\n\t\t * Repeat on the address that fired VM_FAULT_RETRY\n\t\t * without FAULT_FLAG_ALLOW_RETRY but with\n\t\t * FAULT_FLAG_TRIED.\n\t\t */\n\t\t*locked = 1;\n\t\tlock_dropped = true;\n\t\tdown_read(&mm->mmap_sem);\n\t\tret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,\n\t\t\t\t       pages, NULL, NULL);\n\t\tif (ret != 1) {\n\t\t\tBUG_ON(ret > 1);\n\t\t\tif (!pages_done)\n\t\t\t\tpages_done = ret;\n\t\t\tbreak;\n\t\t}\n\t\tnr_pages--;\n\t\tpages_done++;\n\t\tif (!nr_pages)\n\t\t\tbreak;\n\t\tpages++;\n\t\tstart += PAGE_SIZE;\n\t}\n\tif (lock_dropped && *locked) {\n\t\t/*\n\t\t * We must let the caller know we temporarily dropped the lock\n\t\t * and so the critical section protected by it was lost.\n\t\t */\n\t\tup_read(&mm->mmap_sem);\n\t\t*locked = 0;\n\t}\n\treturn pages_done;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_389": {
        "vulnerable_code": {
            "Code": "can_follow_write_pte(pte_t pte, unsigned int flags){\n\treturn pte_write(pte) ||\n\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "can_follow_write_pte(pte_t pte, unsigned int flags){\n\treturn pte_write(pte) ||\n\t\t((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_390": {
        "vulnerable_code": {
            "Code": "faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, int *nonblocking){\n\tunsigned int fault_flags = 0;\n\tvm_fault_t ret;\n\n\t/* mlock all present pages, but do not fault in new pages */\n\tif ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)\n\t\treturn -ENOENT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (nonblocking)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\tVM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, *flags);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (tsk) {\n\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (nonblocking && !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\t\t*nonblocking = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t\t*flags |= FOLL_COW;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned int *flags, int *nonblocking){\n\tunsigned int fault_flags = 0;\n\tvm_fault_t ret;\n\n\t/* mlock all present pages, but do not fault in new pages */\n\tif ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)\n\t\treturn -ENOENT;\n\tif (*flags & FOLL_WRITE)\n\t\tfault_flags |= FAULT_FLAG_WRITE;\n\tif (*flags & FOLL_REMOTE)\n\t\tfault_flags |= FAULT_FLAG_REMOTE;\n\tif (nonblocking)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\tif (*flags & FOLL_NOWAIT)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;\n\tif (*flags & FOLL_TRIED) {\n\t\tVM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);\n\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t}\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, *flags);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (tsk) {\n\t\tif (ret & VM_FAULT_MAJOR)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tif (nonblocking && !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\t\t*nonblocking = 0;\n\t\treturn -EBUSY;\n\t}\n\n\t/*\n\t * The VM_FAULT_WRITE bit tells us that do_wp_page has broken COW when\n\t * necessary, even if maybe_mkwrite decided not to set pte_write. We\n\t * can thus safely do subsequent page lookups as if they were reads.\n\t * But only do so when looping for pte_write is futile: in some cases\n\t * userspace may also be wanting to write to the gotten user page,\n\t * which a read fault here might prevent (a readonly page might get\n\t * reCOWed by userspace write).\n\t */\n\tif ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))\n\t\t*flags |= FOLL_COW;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_391": {
        "vulnerable_code": {
            "Code": "fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,\n\t\t     unsigned long address, unsigned int fault_flags,\n\t\t     bool *unlocked){\n\tstruct vm_area_struct *vma;\n\tvm_fault_t ret, major = 0;\n\n\tif (unlocked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\nretry:\n\tvma = find_extend_vma(mm, address);\n\tif (!vma || address < vma->vm_start)\n\t\treturn -EFAULT;\n\n\tif (!vma_permits_fault(vma, fault_flags))\n\t\treturn -EFAULT;\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tmajor |= ret & VM_FAULT_MAJOR;\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, 0);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tdown_read(&mm->mmap_sem);\n\t\tif (!(fault_flags & FAULT_FLAG_TRIED)) {\n\t\t\t*unlocked = true;\n\t\t\tfault_flags &= ~FAULT_FLAG_ALLOW_RETRY;\n\t\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (tsk) {\n\t\tif (major)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,\n\t\t     unsigned long address, unsigned int fault_flags,\n\t\t     bool *unlocked){\n\tstruct vm_area_struct *vma;\n\tvm_fault_t ret, major = 0;\n\n\tif (unlocked)\n\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY;\n\nretry:\n\tvma = find_extend_vma(mm, address);\n\tif (!vma || address < vma->vm_start)\n\t\treturn -EFAULT;\n\n\tif (!vma_permits_fault(vma, fault_flags))\n\t\treturn -EFAULT;\n\n\tret = handle_mm_fault(vma, address, fault_flags);\n\tmajor |= ret & VM_FAULT_MAJOR;\n\tif (ret & VM_FAULT_ERROR) {\n\t\tint err = vm_fault_to_errno(ret, 0);\n\n\t\tif (err)\n\t\t\treturn err;\n\t\tBUG();\n\t}\n\n\tif (ret & VM_FAULT_RETRY) {\n\t\tdown_read(&mm->mmap_sem);\n\t\tif (!(fault_flags & FAULT_FLAG_TRIED)) {\n\t\t\t*unlocked = true;\n\t\t\tfault_flags &= ~FAULT_FLAG_ALLOW_RETRY;\n\t\t\tfault_flags |= FAULT_FLAG_TRIED;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (tsk) {\n\t\tif (major)\n\t\t\ttsk->maj_flt++;\n\t\telse\n\t\t\ttsk->min_flt++;\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_392": {
        "vulnerable_code": {
            "Code": "get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas){\n\treturn __get_user_pages_locked(current, current->mm, start, nr_pages,\n\t\t\t\t       pages, vmas, NULL,\n\t\t\t\t       gup_flags | FOLL_TOUCH);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas){\n\treturn __get_user_pages_locked(current, current->mm, start, nr_pages,\n\t\t\t\t       pages, vmas, NULL,\n\t\t\t\t       gup_flags | FOLL_TOUCH);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_393": {
        "vulnerable_code": {
            "Code": "get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked){\n\treturn __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,\n\t\t\t\t       locked,\n\t\t\t\t       gup_flags | FOLL_TOUCH | FOLL_REMOTE);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long nr_pages,\n\t\tunsigned int gup_flags, struct page **pages,\n\t\tstruct vm_area_struct **vmas, int *locked){\n\treturn __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,\n\t\t\t\t       locked,\n\t\t\t\t       gup_flags | FOLL_TOUCH | FOLL_REMOTE);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_394": {
        "vulnerable_code": {
            "Code": "__alloc_bootmem_huge_page(struct hstate *h){\n\tstruct huge_bootmem_page *m;\n\tint nr_nodes, node;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {\n\t\tvoid *addr;\n\n\t\taddr = memblock_alloc_try_nid_raw(\n\t\t\t\thuge_page_size(h), huge_page_size(h),\n\t\t\t\t0, MEMBLOCK_ALLOC_ACCESSIBLE, node);\n\t\tif (addr) {\n\t\t\t/*\n\t\t\t * Use the beginning of the huge page to store the\n\t\t\t * huge_bootmem_page struct (until gather_bootmem\n\t\t\t * puts them into the mem_map).\n\t\t\t */\n\t\t\tm = addr;\n\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\tBUG_ON(!IS_ALIGNED(virt_to_phys(m), huge_page_size(h)));\n\t/* Put them into a private list first because mem_map is not up yet */\n\tINIT_LIST_HEAD(&m->list);\n\tlist_add(&m->list, &huge_boot_pages);\n\tm->hstate = h;\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__alloc_bootmem_huge_page(struct hstate *h){\n\tstruct huge_bootmem_page *m;\n\tint nr_nodes, node;\n\n\tfor_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {\n\t\tvoid *addr;\n\n\t\taddr = memblock_alloc_try_nid_raw(\n\t\t\t\thuge_page_size(h), huge_page_size(h),\n\t\t\t\t0, MEMBLOCK_ALLOC_ACCESSIBLE, node);\n\t\tif (addr) {\n\t\t\t/*\n\t\t\t * Use the beginning of the huge page to store the\n\t\t\t * huge_bootmem_page struct (until gather_bootmem\n\t\t\t * puts them into the mem_map).\n\t\t\t */\n\t\t\tm = addr;\n\t\t\tgoto found;\n\t\t}\n\t}\n\treturn 0;\n\nfound:\n\tBUG_ON(!IS_ALIGNED(virt_to_phys(m), huge_page_size(h)));\n\t/* Put them into a private list first because mem_map is not up yet */\n\tINIT_LIST_HEAD(&m->list);\n\tlist_add(&m->list, &huge_boot_pages);\n\tm->hstate = h;\n\treturn 1;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_395": {
        "vulnerable_code": {
            "Code": "__vma_reservation_common(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tenum vma_resv_mode mode){\n\tstruct resv_map *resv;\n\tpgoff_t idx;\n\tlong ret;\n\n\tresv = vma_resv_map(vma);\n\tif (!resv)\n\t\treturn 1;\n\n\tidx = vma_hugecache_offset(h, vma, addr);\n\tswitch (mode) {\n\tcase VMA_NEEDS_RESV:\n\t\tret = region_chg(resv, idx, idx + 1);\n\t\tbreak;\n\tcase VMA_COMMIT_RESV:\n\t\tret = region_add(resv, idx, idx + 1);\n\t\tbreak;\n\tcase VMA_END_RESV:\n\t\tregion_abort(resv, idx, idx + 1);\n\t\tret = 0;\n\t\tbreak;\n\tcase VMA_ADD_RESV:\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tret = region_add(resv, idx, idx + 1);\n\t\telse {\n\t\t\tregion_abort(resv, idx, idx + 1);\n\t\t\tret = region_del(resv, idx, idx + 1);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn ret;\n\telse if (is_vma_resv_set(vma, HPAGE_RESV_OWNER) && ret >= 0) {\n\t\t/*\n\t\t * In most cases, reserves always exist for private mappings.\n\t\t * However, a file associated with mapping could have been\n\t\t * hole punched or truncated after reserves were consumed.\n\t\t * As subsequent fault on such a range will not use reserves.\n\t\t * Subtle - The reserve map for private mappings has the\n\t\t * opposite meaning than that of shared mappings.  If NO\n\t\t * entry is in the reserve map, it means a reservation exists.\n\t\t * If an entry exists in the reserve map, it means the\n\t\t * reservation has already been consumed.  As a result, the\n\t\t * return value of this routine is the opposite of the\n\t\t * value returned from reserve map manipulation routines above.\n\t\t */\n\t\tif (ret)\n\t\t\treturn 0;\n\t\telse\n\t\t\treturn 1;\n\t}\n\telse\n\t\treturn ret < 0 ? ret : 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__vma_reservation_common(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tenum vma_resv_mode mode){\n\tstruct resv_map *resv;\n\tpgoff_t idx;\n\tlong ret;\n\n\tresv = vma_resv_map(vma);\n\tif (!resv)\n\t\treturn 1;\n\n\tidx = vma_hugecache_offset(h, vma, addr);\n\tswitch (mode) {\n\tcase VMA_NEEDS_RESV:\n\t\tret = region_chg(resv, idx, idx + 1);\n\t\tbreak;\n\tcase VMA_COMMIT_RESV:\n\t\tret = region_add(resv, idx, idx + 1);\n\t\tbreak;\n\tcase VMA_END_RESV:\n\t\tregion_abort(resv, idx, idx + 1);\n\t\tret = 0;\n\t\tbreak;\n\tcase VMA_ADD_RESV:\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tret = region_add(resv, idx, idx + 1);\n\t\telse {\n\t\t\tregion_abort(resv, idx, idx + 1);\n\t\t\tret = region_del(resv, idx, idx + 1);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn ret;\n\telse if (is_vma_resv_set(vma, HPAGE_RESV_OWNER) && ret >= 0) {\n\t\t/*\n\t\t * In most cases, reserves always exist for private mappings.\n\t\t * However, a file associated with mapping could have been\n\t\t * hole punched or truncated after reserves were consumed.\n\t\t * As subsequent fault on such a range will not use reserves.\n\t\t * Subtle - The reserve map for private mappings has the\n\t\t * opposite meaning than that of shared mappings.  If NO\n\t\t * entry is in the reserve map, it means a reservation exists.\n\t\t * If an entry exists in the reserve map, it means the\n\t\t * reservation has already been consumed.  As a result, the\n\t\t * return value of this routine is the opposite of the\n\t\t * value returned from reserve map manipulation routines above.\n\t\t */\n\t\tif (ret)\n\t\t\treturn 0;\n\t\telse\n\t\t\treturn 1;\n\t}\n\telse\n\t\treturn ret < 0 ? ret : 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_396": {
        "vulnerable_code": {
            "Code": "huge_add_to_page_cache(struct page *page, struct address_space *mapping,\n\t\t\t   pgoff_t idx){\n\tstruct inode *inode = mapping->host;\n\tstruct hstate *h = hstate_inode(inode);\n\tint err = add_to_page_cache(page, mapping, idx, GFP_KERNEL);\n\n\tif (err)\n\t\treturn err;\n\tClearPagePrivate(page);\n\n\t/*\n\t * set page dirty so that it will not be removed from cache/file\n\t * by non-hugetlbfs specific code paths.\n\t */\n\tset_page_dirty(page);\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks += blocks_per_huge_page(h);\n\tspin_unlock(&inode->i_lock);\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "huge_add_to_page_cache(struct page *page, struct address_space *mapping,\n\t\t\t   pgoff_t idx){\n\tstruct inode *inode = mapping->host;\n\tstruct hstate *h = hstate_inode(inode);\n\tint err = add_to_page_cache(page, mapping, idx, GFP_KERNEL);\n\n\tif (err)\n\t\treturn err;\n\tClearPagePrivate(page);\n\n\t/*\n\t * set page dirty so that it will not be removed from cache/file\n\t * by non-hugetlbfs specific code paths.\n\t */\n\tset_page_dirty(page);\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks += blocks_per_huge_page(h);\n\tspin_unlock(&inode->i_lock);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_397": {
        "vulnerable_code": {
            "Code": "hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,\n\t\t\t    struct vm_area_struct *vma,\n\t\t\t    struct address_space *mapping,\n\t\t\t    pgoff_t idx, unsigned long address){\n\tunsigned long key[2];\n\tu32 hash;\n\n\tif (vma->vm_flags & VM_SHARED) {\n\t\tkey[0] = (unsigned long) mapping;\n\t\tkey[1] = idx;\n\t} else {\n\t\tkey[0] = (unsigned long) mm;\n\t\tkey[1] = address >> huge_page_shift(h);\n\t}\n\n\thash = jhash2((u32 *)&key, sizeof(key)/sizeof(u32), 0);\n\n\treturn hash & (num_fault_mutexes - 1);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,\n\t\t\t    struct vm_area_struct *vma,\n\t\t\t    struct address_space *mapping,\n\t\t\t    pgoff_t idx, unsigned long address){\n\tunsigned long key[2];\n\tu32 hash;\n\n\tif (vma->vm_flags & VM_SHARED) {\n\t\tkey[0] = (unsigned long) mapping;\n\t\tkey[1] = idx;\n\t} else {\n\t\tkey[0] = (unsigned long) mm;\n\t\tkey[1] = address >> huge_page_shift(h);\n\t}\n\n\thash = jhash2((u32 *)&key, sizeof(key)/sizeof(u32), 0);\n\n\treturn hash & (num_fault_mutexes - 1);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_398": {
        "vulnerable_code": {
            "Code": "hugetlb_report_meminfo(struct seq_file *m){\n\tstruct hstate *h;\n\tunsigned long total = 0;\n\n\tif (!hugepages_supported())\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\tunsigned long count = h->nr_huge_pages;\n\n\t\ttotal += (PAGE_SIZE << huge_page_order(h)) * count;\n\n\t\tif (h == &default_hstate)\n\t\t\tseq_printf(m,\n\t\t\t\t   \"HugePages_Total:   %5lu\\n\"\n\t\t\t\t   \"HugePages_Free:    %5lu\\n\"\n\t\t\t\t   \"HugePages_Rsvd:    %5lu\\n\"\n\t\t\t\t   \"HugePages_Surp:    %5lu\\n\"\n\t\t\t\t   \"Hugepagesize:   %8lu kB\\n\",\n\t\t\t\t   count,\n\t\t\t\t   h->free_huge_pages,\n\t\t\t\t   h->resv_huge_pages,\n\t\t\t\t   h->surplus_huge_pages,\n\t\t\t\t   (PAGE_SIZE << huge_page_order(h)) / 1024);\n\t}\n\n\tseq_printf(m, \"Hugetlb:        %8lu kB\\n\", total / 1024);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "hugetlb_report_meminfo(struct seq_file *m){\n\tstruct hstate *h;\n\tunsigned long total = 0;\n\n\tif (!hugepages_supported())\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\tunsigned long count = h->nr_huge_pages;\n\n\t\ttotal += (PAGE_SIZE << huge_page_order(h)) * count;\n\n\t\tif (h == &default_hstate)\n\t\t\tseq_printf(m,\n\t\t\t\t   \"HugePages_Total:   %5lu\\n\"\n\t\t\t\t   \"HugePages_Free:    %5lu\\n\"\n\t\t\t\t   \"HugePages_Rsvd:    %5lu\\n\"\n\t\t\t\t   \"HugePages_Surp:    %5lu\\n\"\n\t\t\t\t   \"Hugepagesize:   %8lu kB\\n\",\n\t\t\t\t   count,\n\t\t\t\t   h->free_huge_pages,\n\t\t\t\t   h->resv_huge_pages,\n\t\t\t\t   h->surplus_huge_pages,\n\t\t\t\t   (PAGE_SIZE << huge_page_order(h)) / 1024);\n\t}\n\n\tseq_printf(m, \"Hugetlb:        %8lu kB\\n\", total / 1024);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_399": {
        "vulnerable_code": {
            "Code": "is_hugetlb_entry_hwpoisoned(pte_t pte){\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn 0;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_hwpoison_entry(swp))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "is_hugetlb_entry_hwpoisoned(pte_t pte){\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn 0;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_hwpoison_entry(swp))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_400": {
        "vulnerable_code": {
            "Code": "return_unused_surplus_pages(struct hstate *h,\n\t\t\t\t\tunsigned long unused_resv_pages){\n\tunsigned long nr_pages;\n\n\t/* Cannot return gigantic pages currently */\n\tif (hstate_is_gigantic(h))\n\t\tgoto out;\n\n\t/*\n\t * Part (or even all) of the reservation could have been backed\n\t * by pre-allocated pages. Only free surplus pages.\n\t */\n\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);\n\n\t/*\n\t * We want to release as many surplus pages as possible, spread\n\t * evenly across all nodes with memory. Iterate across these nodes\n\t * until we can no longer free unreserved surplus pages. This occurs\n\t * when the nodes with surplus pages have no free pages.\n\t * free_pool_huge_page() will balance the the freed pages across the\n\t * on-line nodes with memory and will handle the hstate accounting.\n\t *\n\t * Note that we decrement resv_huge_pages as we free the pages.  If\n\t * we drop the lock, resv_huge_pages will still be sufficiently large\n\t * to cover subsequent pages we may free.\n\t */\n\twhile (nr_pages--) {\n\t\th->resv_huge_pages--;\n\t\tunused_resv_pages--;\n\t\tif (!free_pool_huge_page(h, &node_states[N_MEMORY], 1))\n\t\t\tgoto out;\n\t\tcond_resched_lock(&hugetlb_lock);\n\t}\n\nout:\n\t/* Fully uncommit the reservation */\n\th->resv_huge_pages -= unused_resv_pages;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "return_unused_surplus_pages(struct hstate *h,\n\t\t\t\t\tunsigned long unused_resv_pages){\n\tunsigned long nr_pages;\n\n\t/* Cannot return gigantic pages currently */\n\tif (hstate_is_gigantic(h))\n\t\tgoto out;\n\n\t/*\n\t * Part (or even all) of the reservation could have been backed\n\t * by pre-allocated pages. Only free surplus pages.\n\t */\n\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);\n\n\t/*\n\t * We want to release as many surplus pages as possible, spread\n\t * evenly across all nodes with memory. Iterate across these nodes\n\t * until we can no longer free unreserved surplus pages. This occurs\n\t * when the nodes with surplus pages have no free pages.\n\t * free_pool_huge_page() will balance the the freed pages across the\n\t * on-line nodes with memory and will handle the hstate accounting.\n\t *\n\t * Note that we decrement resv_huge_pages as we free the pages.  If\n\t * we drop the lock, resv_huge_pages will still be sufficiently large\n\t * to cover subsequent pages we may free.\n\t */\n\twhile (nr_pages--) {\n\t\th->resv_huge_pages--;\n\t\tunused_resv_pages--;\n\t\tif (!free_pool_huge_page(h, &node_states[N_MEMORY], 1))\n\t\t\tgoto out;\n\t\tcond_resched_lock(&hugetlb_lock);\n\t}\n\nout:\n\t/* Fully uncommit the reservation */\n\th->resv_huge_pages -= unused_resv_pages;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_401": {
        "vulnerable_code": {
            "Code": "set_huge_ptep_writable(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, pte_t *ptep){\n\tpte_t entry;\n\n\tentry = huge_pte_mkwrite(huge_pte_mkdirty(huge_ptep_get(ptep)));\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))\n\t\tupdate_mmu_cache(vma, address, ptep);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "set_huge_ptep_writable(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, pte_t *ptep){\n\tpte_t entry;\n\n\tentry = huge_pte_mkwrite(huge_pte_mkdirty(huge_ptep_get(ptep)));\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))\n\t\tupdate_mmu_cache(vma, address, ptep);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_402": {
        "vulnerable_code": {
            "Code": "subpool_inode(struct inode *inode){\n\treturn HUGETLBFS_SB(inode->i_sb)->spool;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "subpool_inode(struct inode *inode){\n\treturn HUGETLBFS_SB(inode->i_sb)->spool;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_403": {
        "vulnerable_code": {
            "Code": "unlock_or_release_subpool(struct hugepage_subpool *spool){\n\tbool free = (spool->count == 0) && (spool->used_hpages == 0);\n\n\tspin_unlock(&spool->lock);\n\n\t/* If no pages are used, and no other handles to the subpool\n\t * remain, give up any reservations mased on minimum size and\n\t * free the subpool */\n\tif (free) {\n\t\tif (spool->min_hpages != -1)\n\t\t\thugetlb_acct_memory(spool->hstate,\n\t\t\t\t\t\t-spool->min_hpages);\n\t\tkfree(spool);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "unlock_or_release_subpool(struct hugepage_subpool *spool){\n\tbool free = (spool->count == 0) && (spool->used_hpages == 0);\n\n\tspin_unlock(&spool->lock);\n\n\t/* If no pages are used, and no other handles to the subpool\n\t * remain, give up any reservations mased on minimum size and\n\t * free the subpool */\n\tif (free) {\n\t\tif (spool->min_hpages != -1)\n\t\t\thugetlb_acct_memory(spool->hstate,\n\t\t\t\t\t\t-spool->min_hpages);\n\t\tkfree(spool);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_404": {
        "vulnerable_code": {
            "Code": "SMB2_QFS_attr(const unsigned int xid, struct cifs_tcon *tcon,\n\t      u64 persistent_fid, u64 volatile_fid, int level){\n\tstruct smb_rqst rqst;\n\tstruct smb2_query_info_rsp *rsp = NULL;\n\tstruct kvec iov;\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype, max_len, min_len;\n\tstruct cifs_ses *ses = tcon->ses;\n\tunsigned int rsp_len, offset;\n\tint flags = 0;\n\n\tif (level == FS_DEVICE_INFORMATION) {\n\t\tmax_len = sizeof(FILE_SYSTEM_DEVICE_INFO);\n\t\tmin_len = sizeof(FILE_SYSTEM_DEVICE_INFO);\n\t} else if (level == FS_ATTRIBUTE_INFORMATION) {\n\t\tmax_len = sizeof(FILE_SYSTEM_ATTRIBUTE_INFO);\n\t\tmin_len = MIN_FS_ATTR_INFO_SIZE;\n\t} else if (level == FS_SECTOR_SIZE_INFORMATION) {\n\t\tmax_len = sizeof(struct smb3_fs_ss_info);\n\t\tmin_len = sizeof(struct smb3_fs_ss_info);\n\t} else if (level == FS_VOLUME_INFORMATION) {\n\t\tmax_len = sizeof(struct smb3_fs_vol_info) + MAX_VOL_LABEL_LEN;\n\t\tmin_len = sizeof(struct smb3_fs_vol_info);\n\t} else {\n\t\tcifs_dbg(FYI, \"Invalid qfsinfo level %d\\n\", level);\n\t\treturn -EINVAL;\n\t}\n\n\trc = build_qfs_info_req(&iov, tcon, level, max_len,\n\t\t\t\tpersistent_fid, volatile_fid);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = &iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(iov.iov_base);\n\tif (rc) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_QUERY_INFO_HE);\n\t\tgoto qfsattr_exit;\n\t}\n\trsp = (struct smb2_query_info_rsp *)rsp_iov.iov_base;\n\n\trsp_len = le32_to_cpu(rsp->OutputBufferLength);\n\toffset = le16_to_cpu(rsp->OutputBufferOffset);\n\trc = smb2_validate_iov(offset, rsp_len, &rsp_iov, min_len);\n\tif (rc)\n\t\tgoto qfsattr_exit;\n\n\tif (level == FS_ATTRIBUTE_INFORMATION)\n\t\tmemcpy(&tcon->fsAttrInfo, offset\n\t\t\t+ (char *)rsp, min_t(unsigned int,\n\t\t\trsp_len, max_len));\n\telse if (level == FS_DEVICE_INFORMATION)\n\t\tmemcpy(&tcon->fsDevInfo, offset\n\t\t\t+ (char *)rsp, sizeof(FILE_SYSTEM_DEVICE_INFO));\n\telse if (level == FS_SECTOR_SIZE_INFORMATION) {\n\t\tstruct smb3_fs_ss_info *ss_info = (struct smb3_fs_ss_info *)\n\t\t\t(offset + (char *)rsp);\n\t\ttcon->ss_flags = le32_to_cpu(ss_info->Flags);\n\t\ttcon->perf_sector_size =\n\t\t\tle32_to_cpu(ss_info->PhysicalBytesPerSectorForPerf);\n\t} else if (level == FS_VOLUME_INFORMATION) {\n\t\tstruct smb3_fs_vol_info *vol_info = (struct smb3_fs_vol_info *)\n\t\t\t(offset + (char *)rsp);\n\t\ttcon->vol_serial_number = vol_info->VolumeSerialNumber;\n\t\ttcon->vol_create_time = vol_info->VolumeCreationTime;\n\t}\n\nqfsattr_exit:\n\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_QFS_attr(const unsigned int xid, struct cifs_tcon *tcon,\n\t      u64 persistent_fid, u64 volatile_fid, int level){\n\tstruct smb_rqst rqst;\n\tstruct smb2_query_info_rsp *rsp = NULL;\n\tstruct kvec iov;\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype, max_len, min_len;\n\tstruct cifs_ses *ses = tcon->ses;\n\tunsigned int rsp_len, offset;\n\tint flags = 0;\n\n\tif (level == FS_DEVICE_INFORMATION) {\n\t\tmax_len = sizeof(FILE_SYSTEM_DEVICE_INFO);\n\t\tmin_len = sizeof(FILE_SYSTEM_DEVICE_INFO);\n\t} else if (level == FS_ATTRIBUTE_INFORMATION) {\n\t\tmax_len = sizeof(FILE_SYSTEM_ATTRIBUTE_INFO);\n\t\tmin_len = MIN_FS_ATTR_INFO_SIZE;\n\t} else if (level == FS_SECTOR_SIZE_INFORMATION) {\n\t\tmax_len = sizeof(struct smb3_fs_ss_info);\n\t\tmin_len = sizeof(struct smb3_fs_ss_info);\n\t} else if (level == FS_VOLUME_INFORMATION) {\n\t\tmax_len = sizeof(struct smb3_fs_vol_info) + MAX_VOL_LABEL_LEN;\n\t\tmin_len = sizeof(struct smb3_fs_vol_info);\n\t} else {\n\t\tcifs_dbg(FYI, \"Invalid qfsinfo level %d\\n\", level);\n\t\treturn -EINVAL;\n\t}\n\n\trc = build_qfs_info_req(&iov, tcon, level, max_len,\n\t\t\t\tpersistent_fid, volatile_fid);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = &iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(iov.iov_base);\n\tif (rc) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_QUERY_INFO_HE);\n\t\tgoto qfsattr_exit;\n\t}\n\trsp = (struct smb2_query_info_rsp *)rsp_iov.iov_base;\n\n\trsp_len = le32_to_cpu(rsp->OutputBufferLength);\n\toffset = le16_to_cpu(rsp->OutputBufferOffset);\n\trc = smb2_validate_iov(offset, rsp_len, &rsp_iov, min_len);\n\tif (rc)\n\t\tgoto qfsattr_exit;\n\n\tif (level == FS_ATTRIBUTE_INFORMATION)\n\t\tmemcpy(&tcon->fsAttrInfo, offset\n\t\t\t+ (char *)rsp, min_t(unsigned int,\n\t\t\trsp_len, max_len));\n\telse if (level == FS_DEVICE_INFORMATION)\n\t\tmemcpy(&tcon->fsDevInfo, offset\n\t\t\t+ (char *)rsp, sizeof(FILE_SYSTEM_DEVICE_INFO));\n\telse if (level == FS_SECTOR_SIZE_INFORMATION) {\n\t\tstruct smb3_fs_ss_info *ss_info = (struct smb3_fs_ss_info *)\n\t\t\t(offset + (char *)rsp);\n\t\ttcon->ss_flags = le32_to_cpu(ss_info->Flags);\n\t\ttcon->perf_sector_size =\n\t\t\tle32_to_cpu(ss_info->PhysicalBytesPerSectorForPerf);\n\t} else if (level == FS_VOLUME_INFORMATION) {\n\t\tstruct smb3_fs_vol_info *vol_info = (struct smb3_fs_vol_info *)\n\t\t\t(offset + (char *)rsp);\n\t\ttcon->vol_serial_number = vol_info->VolumeSerialNumber;\n\t\ttcon->vol_create_time = vol_info->VolumeCreationTime;\n\t}\n\nqfsattr_exit:\n\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_405": {
        "vulnerable_code": {
            "Code": "SMB2_auth_kerberos(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct cifs_ses *ses = sess_data->ses;\n\tstruct cifs_spnego_msg *msg;\n\tstruct key *spnego_key = NULL;\n\tstruct smb2_sess_setup_rsp *rsp = NULL;\n\n\trc = SMB2_sess_alloc_buffer(sess_data);\n\tif (rc)\n\t\tgoto out;\n\n\tspnego_key = cifs_get_spnego_key(ses);\n\tif (IS_ERR(spnego_key)) {\n\t\trc = PTR_ERR(spnego_key);\n\t\tspnego_key = NULL;\n\t\tgoto out;\n\t}\n\n\tmsg = spnego_key->payload.data[0];\n\t/*\n\t * check version field to make sure that cifs.upcall is\n\t * sending us a response in an expected form\n\t */\n\tif (msg->version != CIFS_SPNEGO_UPCALL_VERSION) {\n\t\tcifs_dbg(VFS,\n\t\t\t  \"bad cifs.upcall version. Expected %d got %d\",\n\t\t\t  CIFS_SPNEGO_UPCALL_VERSION, msg->version);\n\t\trc = -EKEYREJECTED;\n\t\tgoto out_put_spnego_key;\n\t}\n\n\tses->auth_key.response = kmemdup(msg->data, msg->sesskey_len,\n\t\t\t\t\t GFP_KERNEL);\n\tif (!ses->auth_key.response) {\n\t\tcifs_dbg(VFS,\n\t\t\t\"Kerberos can't allocate (%u bytes) memory\",\n\t\t\tmsg->sesskey_len);\n\t\trc = -ENOMEM;\n\t\tgoto out_put_spnego_key;\n\t}\n\tses->auth_key.len = msg->sesskey_len;\n\n\tsess_data->iov[1].iov_base = msg->data + msg->sesskey_len;\n\tsess_data->iov[1].iov_len = msg->secblob_len;\n\n\trc = SMB2_sess_sendreceive(sess_data);\n\tif (rc)\n\t\tgoto out_put_spnego_key;\n\n\trsp = (struct smb2_sess_setup_rsp *)sess_data->iov[0].iov_base;\n\tses->Suid = rsp->sync_hdr.SessionId;\n\n\tses->session_flags = le16_to_cpu(rsp->SessionFlags);\n\n\trc = SMB2_sess_establish_session(sess_data);\nout_put_spnego_key:\n\tkey_invalidate(spnego_key);\n\tkey_put(spnego_key);\nout:\n\tsess_data->result = rc;\n\tsess_data->func = NULL;\n\tSMB2_sess_free_buffer(sess_data);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_auth_kerberos(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct cifs_ses *ses = sess_data->ses;\n\tstruct cifs_spnego_msg *msg;\n\tstruct key *spnego_key = NULL;\n\tstruct smb2_sess_setup_rsp *rsp = NULL;\n\n\trc = SMB2_sess_alloc_buffer(sess_data);\n\tif (rc)\n\t\tgoto out;\n\n\tspnego_key = cifs_get_spnego_key(ses);\n\tif (IS_ERR(spnego_key)) {\n\t\trc = PTR_ERR(spnego_key);\n\t\tspnego_key = NULL;\n\t\tgoto out;\n\t}\n\n\tmsg = spnego_key->payload.data[0];\n\t/*\n\t * check version field to make sure that cifs.upcall is\n\t * sending us a response in an expected form\n\t */\n\tif (msg->version != CIFS_SPNEGO_UPCALL_VERSION) {\n\t\tcifs_dbg(VFS,\n\t\t\t  \"bad cifs.upcall version. Expected %d got %d\",\n\t\t\t  CIFS_SPNEGO_UPCALL_VERSION, msg->version);\n\t\trc = -EKEYREJECTED;\n\t\tgoto out_put_spnego_key;\n\t}\n\n\tses->auth_key.response = kmemdup(msg->data, msg->sesskey_len,\n\t\t\t\t\t GFP_KERNEL);\n\tif (!ses->auth_key.response) {\n\t\tcifs_dbg(VFS,\n\t\t\t\"Kerberos can't allocate (%u bytes) memory\",\n\t\t\tmsg->sesskey_len);\n\t\trc = -ENOMEM;\n\t\tgoto out_put_spnego_key;\n\t}\n\tses->auth_key.len = msg->sesskey_len;\n\n\tsess_data->iov[1].iov_base = msg->data + msg->sesskey_len;\n\tsess_data->iov[1].iov_len = msg->secblob_len;\n\n\trc = SMB2_sess_sendreceive(sess_data);\n\tif (rc)\n\t\tgoto out_put_spnego_key;\n\n\trsp = (struct smb2_sess_setup_rsp *)sess_data->iov[0].iov_base;\n\tses->Suid = rsp->sync_hdr.SessionId;\n\n\tses->session_flags = le16_to_cpu(rsp->SessionFlags);\n\n\trc = SMB2_sess_establish_session(sess_data);\nout_put_spnego_key:\n\tkey_invalidate(spnego_key);\n\tkey_put(spnego_key);\nout:\n\tsess_data->result = rc;\n\tsess_data->func = NULL;\n\tSMB2_sess_free_buffer(sess_data);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_406": {
        "vulnerable_code": {
            "Code": "SMB2_close_init(struct cifs_tcon *tcon, struct smb_rqst *rqst,\n\t\tu64 persistent_fid, u64 volatile_fid){\n\tstruct smb2_close_req *req;\n\tstruct kvec *iov = rqst->rq_iov;\n\tunsigned int total_len;\n\tint rc;\n\n\trc = smb2_plain_req_init(SMB2_CLOSE, tcon, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_close_init(struct cifs_tcon *tcon, struct smb_rqst *rqst,\n\t\tu64 persistent_fid, u64 volatile_fid){\n\tstruct smb2_close_req *req;\n\tstruct kvec *iov = rqst->rq_iov;\n\tunsigned int total_len;\n\tint rc;\n\n\trc = smb2_plain_req_init(SMB2_CLOSE, tcon, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_407": {
        "vulnerable_code": {
            "Code": "SMB2_get_srv_num(const unsigned int xid, struct cifs_tcon *tcon,\n\t\t u64 persistent_fid, u64 volatile_fid, __le64 *uniqueid){\n\treturn query_info(xid, tcon, persistent_fid, volatile_fid,\n\t\t\t  FILE_INTERNAL_INFORMATION, SMB2_O_INFO_FILE, 0,\n\t\t\t  sizeof(struct smb2_file_internal_info),\n\t\t\t  sizeof(struct smb2_file_internal_info),\n\t\t\t  (void **)&uniqueid, NULL);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_get_srv_num(const unsigned int xid, struct cifs_tcon *tcon,\n\t\t u64 persistent_fid, u64 volatile_fid, __le64 *uniqueid){\n\treturn query_info(xid, tcon, persistent_fid, volatile_fid,\n\t\t\t  FILE_INTERNAL_INFORMATION, SMB2_O_INFO_FILE, 0,\n\t\t\t  sizeof(struct smb2_file_internal_info),\n\t\t\t  sizeof(struct smb2_file_internal_info),\n\t\t\t  (void **)&uniqueid, NULL);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_408": {
        "vulnerable_code": {
            "Code": "SMB2_sess_sendreceive(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct smb_rqst rqst;\n\tstruct smb2_sess_setup_req *req = sess_data->iov[0].iov_base;\n\tstruct kvec rsp_iov = { NULL, 0 };\n\n\t/* Testing shows that buffer offset must be at location of Buffer[0] */\n\treq->SecurityBufferOffset =\n\t\tcpu_to_le16(sizeof(struct smb2_sess_setup_req) - 1 /* pad */);\n\treq->SecurityBufferLength = cpu_to_le16(sess_data->iov[1].iov_len);\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = sess_data->iov;\n\trqst.rq_nvec = 2;\n\n\t/* BB add code to build os and lm fields */\n\trc = cifs_send_recv(sess_data->xid, sess_data->ses,\n\t\t\t    &rqst,\n\t\t\t    &sess_data->buf0_type,\n\t\t\t    CIFS_LOG_ERROR | CIFS_NEG_OP, &rsp_iov);\n\tcifs_small_buf_release(sess_data->iov[0].iov_base);\n\tmemcpy(&sess_data->iov[0], &rsp_iov, sizeof(struct kvec));\n\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_sess_sendreceive(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct smb_rqst rqst;\n\tstruct smb2_sess_setup_req *req = sess_data->iov[0].iov_base;\n\tstruct kvec rsp_iov = { NULL, 0 };\n\n\t/* Testing shows that buffer offset must be at location of Buffer[0] */\n\treq->SecurityBufferOffset =\n\t\tcpu_to_le16(sizeof(struct smb2_sess_setup_req) - 1 /* pad */);\n\treq->SecurityBufferLength = cpu_to_le16(sess_data->iov[1].iov_len);\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = sess_data->iov;\n\trqst.rq_nvec = 2;\n\n\t/* BB add code to build os and lm fields */\n\trc = cifs_send_recv(sess_data->xid, sess_data->ses,\n\t\t\t    &rqst,\n\t\t\t    &sess_data->buf0_type,\n\t\t\t    CIFS_LOG_ERROR | CIFS_NEG_OP, &rsp_iov);\n\tcifs_small_buf_release(sess_data->iov[0].iov_base);\n\tmemcpy(&sess_data->iov[0], &rsp_iov, sizeof(struct kvec));\n\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_409": {
        "vulnerable_code": {
            "Code": "SMB2_set_info_init(struct cifs_tcon *tcon, struct smb_rqst *rqst,\n\t       u64 persistent_fid, u64 volatile_fid, u32 pid, u8 info_class,\n\t       u8 info_type, u32 additional_info,\n\t\tvoid **data, unsigned int *size){\n\tstruct smb2_set_info_req *req;\n\tstruct kvec *iov = rqst->rq_iov;\n\tunsigned int i, total_len;\n\tint rc;\n\n\trc = smb2_plain_req_init(SMB2_SET_INFO, tcon, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(pid);\n\treq->InfoType = info_type;\n\treq->FileInfoClass = info_class;\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\treq->AdditionalInformation = cpu_to_le32(additional_info);\n\n\treq->BufferOffset =\n\t\t\tcpu_to_le16(sizeof(struct smb2_set_info_req) - 1);\n\treq->BufferLength = cpu_to_le32(*size);\n\n\tmemcpy(req->Buffer, *data, *size);\n\ttotal_len += *size;\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tfor (i = 1; i < rqst->rq_nvec; i++) {\n\t\tle32_add_cpu(&req->BufferLength, size[i]);\n\t\tiov[i].iov_base = (char *)data[i];\n\t\tiov[i].iov_len = size[i];\n\t}\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_set_info_init(struct cifs_tcon *tcon, struct smb_rqst *rqst,\n\t       u64 persistent_fid, u64 volatile_fid, u32 pid, u8 info_class,\n\t       u8 info_type, u32 additional_info,\n\t\tvoid **data, unsigned int *size){\n\tstruct smb2_set_info_req *req;\n\tstruct kvec *iov = rqst->rq_iov;\n\tunsigned int i, total_len;\n\tint rc;\n\n\trc = smb2_plain_req_init(SMB2_SET_INFO, tcon, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(pid);\n\treq->InfoType = info_type;\n\treq->FileInfoClass = info_class;\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\treq->AdditionalInformation = cpu_to_le32(additional_info);\n\n\treq->BufferOffset =\n\t\t\tcpu_to_le16(sizeof(struct smb2_set_info_req) - 1);\n\treq->BufferLength = cpu_to_le32(*size);\n\n\tmemcpy(req->Buffer, *data, *size);\n\ttotal_len += *size;\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tfor (i = 1; i < rqst->rq_nvec; i++) {\n\t\tle32_add_cpu(&req->BufferLength, size[i]);\n\t\tiov[i].iov_base = (char *)data[i];\n\t\tiov[i].iov_len = size[i];\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_410": {
        "vulnerable_code": {
            "Code": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec){\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec){\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tcifs_small_buf_release(req);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_411": {
        "vulnerable_code": {
            "Code": "SMB311_posix_qfs_info(const unsigned int xid, struct cifs_tcon *tcon,\n\t      u64 persistent_fid, u64 volatile_fid, struct kstatfs *fsdata){\n\tstruct smb_rqst rqst;\n\tstruct smb2_query_info_rsp *rsp = NULL;\n\tstruct kvec iov;\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype;\n\tstruct cifs_ses *ses = tcon->ses;\n\tFILE_SYSTEM_POSIX_INFO *info = NULL;\n\tint flags = 0;\n\n\trc = build_qfs_info_req(&iov, tcon, FS_POSIX_INFORMATION,\n\t\t\t\tsizeof(FILE_SYSTEM_POSIX_INFO),\n\t\t\t\tpersistent_fid, volatile_fid);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = &iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(iov.iov_base);\n\tif (rc) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_QUERY_INFO_HE);\n\t\tgoto posix_qfsinf_exit;\n\t}\n\trsp = (struct smb2_query_info_rsp *)rsp_iov.iov_base;\n\n\tinfo = (FILE_SYSTEM_POSIX_INFO *)(\n\t\tle16_to_cpu(rsp->OutputBufferOffset) + (char *)rsp);\n\trc = smb2_validate_iov(le16_to_cpu(rsp->OutputBufferOffset),\n\t\t\t       le32_to_cpu(rsp->OutputBufferLength), &rsp_iov,\n\t\t\t       sizeof(FILE_SYSTEM_POSIX_INFO));\n\tif (!rc)\n\t\tcopy_posix_fs_info_to_kstatfs(info, fsdata);\n\nposix_qfsinf_exit:\n\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB311_posix_qfs_info(const unsigned int xid, struct cifs_tcon *tcon,\n\t      u64 persistent_fid, u64 volatile_fid, struct kstatfs *fsdata){\n\tstruct smb_rqst rqst;\n\tstruct smb2_query_info_rsp *rsp = NULL;\n\tstruct kvec iov;\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype;\n\tstruct cifs_ses *ses = tcon->ses;\n\tFILE_SYSTEM_POSIX_INFO *info = NULL;\n\tint flags = 0;\n\n\trc = build_qfs_info_req(&iov, tcon, FS_POSIX_INFORMATION,\n\t\t\t\tsizeof(FILE_SYSTEM_POSIX_INFO),\n\t\t\t\tpersistent_fid, volatile_fid);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = &iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(iov.iov_base);\n\tif (rc) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_QUERY_INFO_HE);\n\t\tgoto posix_qfsinf_exit;\n\t}\n\trsp = (struct smb2_query_info_rsp *)rsp_iov.iov_base;\n\n\tinfo = (FILE_SYSTEM_POSIX_INFO *)(\n\t\tle16_to_cpu(rsp->OutputBufferOffset) + (char *)rsp);\n\trc = smb2_validate_iov(le16_to_cpu(rsp->OutputBufferOffset),\n\t\t\t       le32_to_cpu(rsp->OutputBufferLength), &rsp_iov,\n\t\t\t       sizeof(FILE_SYSTEM_POSIX_INFO));\n\tif (!rc)\n\t\tcopy_posix_fs_info_to_kstatfs(info, fsdata);\n\nposix_qfsinf_exit:\n\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_412": {
        "vulnerable_code": {
            "Code": "build_encrypt_ctxt(struct smb2_encryption_neg_context *pneg_ctxt){\n\tpneg_ctxt->ContextType = SMB2_ENCRYPTION_CAPABILITIES;\n\tpneg_ctxt->DataLength = cpu_to_le16(4); /* Cipher Count + le16 cipher */\n\tpneg_ctxt->CipherCount = cpu_to_le16(1);\n/* pneg_ctxt->Ciphers[0] = SMB2_ENCRYPTION_AES128_GCM;*/ /* not supported yet */\n\tpneg_ctxt->Ciphers[0] = SMB2_ENCRYPTION_AES128_CCM;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "build_encrypt_ctxt(struct smb2_encryption_neg_context *pneg_ctxt){\n\tpneg_ctxt->ContextType = SMB2_ENCRYPTION_CAPABILITIES;\n\tpneg_ctxt->DataLength = cpu_to_le16(4); /* Cipher Count + le16 cipher */\n\tpneg_ctxt->CipherCount = cpu_to_le16(1);\n/* pneg_ctxt->Ciphers[0] = SMB2_ENCRYPTION_AES128_GCM;*/ /* not supported yet */\n\tpneg_ctxt->Ciphers[0] = SMB2_ENCRYPTION_AES128_CCM;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_413": {
        "vulnerable_code": {
            "Code": "send_set_info(const unsigned int xid, struct cifs_tcon *tcon,\n\t       u64 persistent_fid, u64 volatile_fid, u32 pid, u8 info_class,\n\t       u8 info_type, u32 additional_info, unsigned int num,\n\t\tvoid **data, unsigned int *size){\n\tstruct smb_rqst rqst;\n\tstruct smb2_set_info_rsp *rsp = NULL;\n\tstruct kvec *iov;\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype;\n\tstruct cifs_ses *ses = tcon->ses;\n\tint flags = 0;\n\n\tif (!ses || !(ses->server))\n\t\treturn -EIO;\n\n\tif (!num)\n\t\treturn -EINVAL;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov = kmalloc_array(num, sizeof(struct kvec), GFP_KERNEL);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = num;\n\n\trc = SMB2_set_info_init(tcon, &rqst, persistent_fid, volatile_fid, pid,\n\t\t\t\tinfo_class, info_type, additional_info,\n\t\t\t\tdata, size);\n\tif (rc) {\n\t\tkfree(iov);\n\t\treturn rc;\n\t}\n\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags,\n\t\t\t    &rsp_iov);\n\tSMB2_set_info_free(&rqst);\n\trsp = (struct smb2_set_info_rsp *)rsp_iov.iov_base;\n\n\tif (rc != 0) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_SET_INFO_HE);\n\t\ttrace_smb3_set_info_err(xid, persistent_fid, tcon->tid,\n\t\t\t\tses->Suid, info_class, (__u32)info_type, rc);\n\t}\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\tkfree(iov);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "send_set_info(const unsigned int xid, struct cifs_tcon *tcon,\n\t       u64 persistent_fid, u64 volatile_fid, u32 pid, u8 info_class,\n\t       u8 info_type, u32 additional_info, unsigned int num,\n\t\tvoid **data, unsigned int *size){\n\tstruct smb_rqst rqst;\n\tstruct smb2_set_info_rsp *rsp = NULL;\n\tstruct kvec *iov;\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype;\n\tstruct cifs_ses *ses = tcon->ses;\n\tint flags = 0;\n\n\tif (!ses || !(ses->server))\n\t\treturn -EIO;\n\n\tif (!num)\n\t\treturn -EINVAL;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov = kmalloc_array(num, sizeof(struct kvec), GFP_KERNEL);\n\tif (!iov)\n\t\treturn -ENOMEM;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = num;\n\n\trc = SMB2_set_info_init(tcon, &rqst, persistent_fid, volatile_fid, pid,\n\t\t\t\tinfo_class, info_type, additional_info,\n\t\t\t\tdata, size);\n\tif (rc) {\n\t\tkfree(iov);\n\t\treturn rc;\n\t}\n\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags,\n\t\t\t    &rsp_iov);\n\tSMB2_set_info_free(&rqst);\n\trsp = (struct smb2_set_info_rsp *)rsp_iov.iov_base;\n\n\tif (rc != 0) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_SET_INFO_HE);\n\t\ttrace_smb3_set_info_err(xid, persistent_fid, tcon->tid,\n\t\t\t\tses->Suid, info_class, (__u32)info_type, rc);\n\t}\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\tkfree(iov);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_414": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_415": {
        "vulnerable_code": {
            "Code": "smb2_new_read_req(void **buf, unsigned int *total_len,\n\tstruct cifs_io_parms *io_parms, struct cifs_readdata *rdata,\n\tunsigned int remaining_bytes, int request_type){\n\tint rc = -EACCES;\n\tstruct smb2_read_plain_req *req = NULL;\n\tstruct smb2_sync_hdr *shdr;\n\tstruct TCP_Server_Info *server;\n\n\trc = smb2_plain_req_init(SMB2_READ, io_parms->tcon, (void **) &req,\n\t\t\t\t total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tserver = io_parms->tcon->ses->server;\n\tif (server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tshdr = &req->sync_hdr;\n\tshdr->ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->ReadChannelInfoOffset = 0; /* reserved */\n\treq->ReadChannelInfoLength = 0; /* reserved */\n\treq->Channel = 0; /* reserved */\n\treq->MinimumCount = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\n\ttrace_smb3_read_enter(0 /* xid */,\n\t\t\tio_parms->persistent_fid,\n\t\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\t\tio_parms->offset, io_parms->length);\n#ifdef CONFIG_CIFS_SMB_DIRECT\n\t/*\n\t * If we want to do a RDMA write, fill in and append\n\t * smbd_buffer_descriptor_v1 to the end of read request\n\t */\n\tif (server->rdma && rdata && !server->sign &&\n\t\trdata->bytes >= server->smbd_conn->rdma_readwrite_threshold) {\n\n\t\tstruct smbd_buffer_descriptor_v1 *v1;\n\t\tbool need_invalidate =\n\t\t\tio_parms->tcon->ses->server->dialect == SMB30_PROT_ID;\n\n\t\trdata->mr = smbd_register_mr(\n\t\t\t\tserver->smbd_conn, rdata->pages,\n\t\t\t\trdata->nr_pages, rdata->page_offset,\n\t\t\t\trdata->tailsz, true, need_invalidate);\n\t\tif (!rdata->mr)\n\t\t\treturn -ENOBUFS;\n\n\t\treq->Channel = SMB2_CHANNEL_RDMA_V1_INVALIDATE;\n\t\tif (need_invalidate)\n\t\t\treq->Channel = SMB2_CHANNEL_RDMA_V1;\n\t\treq->ReadChannelInfoOffset =\n\t\t\tcpu_to_le16(offsetof(struct smb2_read_plain_req, Buffer));\n\t\treq->ReadChannelInfoLength =\n\t\t\tcpu_to_le16(sizeof(struct smbd_buffer_descriptor_v1));\n\t\tv1 = (struct smbd_buffer_descriptor_v1 *) &req->Buffer[0];\n\t\tv1->offset = cpu_to_le64(rdata->mr->mr->iova);\n\t\tv1->token = cpu_to_le32(rdata->mr->mr->rkey);\n\t\tv1->length = cpu_to_le32(rdata->mr->mr->length);\n\n\t\t*total_len += sizeof(*v1) - 1;\n\t}\n#endif\n\tif (request_type & CHAINED_REQUEST) {\n\t\tif (!(request_type & END_OF_CHAIN)) {\n\t\t\t/* next 8-byte aligned request */\n\t\t\t*total_len = DIV_ROUND_UP(*total_len, 8) * 8;\n\t\t\tshdr->NextCommand = cpu_to_le32(*total_len);\n\t\t} else /* END_OF_CHAIN */\n\t\t\tshdr->NextCommand = 0;\n\t\tif (request_type & RELATED_REQUEST) {\n\t\t\tshdr->Flags |= SMB2_FLAGS_RELATED_OPERATIONS;\n\t\t\t/*\n\t\t\t * Related requests use info from previous read request\n\t\t\t * in chain.\n\t\t\t */\n\t\t\tshdr->SessionId = 0xFFFFFFFF;\n\t\t\tshdr->TreeId = 0xFFFFFFFF;\n\t\t\treq->PersistentFileId = 0xFFFFFFFF;\n\t\t\treq->VolatileFileId = 0xFFFFFFFF;\n\t\t}\n\t}\n\tif (remaining_bytes > io_parms->length)\n\t\treq->RemainingBytes = cpu_to_le32(remaining_bytes);\n\telse\n\t\treq->RemainingBytes = 0;\n\n\t*buf = req;\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smb2_new_read_req(void **buf, unsigned int *total_len,\n\tstruct cifs_io_parms *io_parms, struct cifs_readdata *rdata,\n\tunsigned int remaining_bytes, int request_type){\n\tint rc = -EACCES;\n\tstruct smb2_read_plain_req *req = NULL;\n\tstruct smb2_sync_hdr *shdr;\n\tstruct TCP_Server_Info *server;\n\n\trc = smb2_plain_req_init(SMB2_READ, io_parms->tcon, (void **) &req,\n\t\t\t\t total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tserver = io_parms->tcon->ses->server;\n\tif (server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tshdr = &req->sync_hdr;\n\tshdr->ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->ReadChannelInfoOffset = 0; /* reserved */\n\treq->ReadChannelInfoLength = 0; /* reserved */\n\treq->Channel = 0; /* reserved */\n\treq->MinimumCount = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\n\ttrace_smb3_read_enter(0 /* xid */,\n\t\t\tio_parms->persistent_fid,\n\t\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\t\tio_parms->offset, io_parms->length);\n#ifdef CONFIG_CIFS_SMB_DIRECT\n\t/*\n\t * If we want to do a RDMA write, fill in and append\n\t * smbd_buffer_descriptor_v1 to the end of read request\n\t */\n\tif (server->rdma && rdata && !server->sign &&\n\t\trdata->bytes >= server->smbd_conn->rdma_readwrite_threshold) {\n\n\t\tstruct smbd_buffer_descriptor_v1 *v1;\n\t\tbool need_invalidate =\n\t\t\tio_parms->tcon->ses->server->dialect == SMB30_PROT_ID;\n\n\t\trdata->mr = smbd_register_mr(\n\t\t\t\tserver->smbd_conn, rdata->pages,\n\t\t\t\trdata->nr_pages, rdata->page_offset,\n\t\t\t\trdata->tailsz, true, need_invalidate);\n\t\tif (!rdata->mr)\n\t\t\treturn -ENOBUFS;\n\n\t\treq->Channel = SMB2_CHANNEL_RDMA_V1_INVALIDATE;\n\t\tif (need_invalidate)\n\t\t\treq->Channel = SMB2_CHANNEL_RDMA_V1;\n\t\treq->ReadChannelInfoOffset =\n\t\t\tcpu_to_le16(offsetof(struct smb2_read_plain_req, Buffer));\n\t\treq->ReadChannelInfoLength =\n\t\t\tcpu_to_le16(sizeof(struct smbd_buffer_descriptor_v1));\n\t\tv1 = (struct smbd_buffer_descriptor_v1 *) &req->Buffer[0];\n\t\tv1->offset = cpu_to_le64(rdata->mr->mr->iova);\n\t\tv1->token = cpu_to_le32(rdata->mr->mr->rkey);\n\t\tv1->length = cpu_to_le32(rdata->mr->mr->length);\n\n\t\t*total_len += sizeof(*v1) - 1;\n\t}\n#endif\n\tif (request_type & CHAINED_REQUEST) {\n\t\tif (!(request_type & END_OF_CHAIN)) {\n\t\t\t/* next 8-byte aligned request */\n\t\t\t*total_len = DIV_ROUND_UP(*total_len, 8) * 8;\n\t\t\tshdr->NextCommand = cpu_to_le32(*total_len);\n\t\t} else /* END_OF_CHAIN */\n\t\t\tshdr->NextCommand = 0;\n\t\tif (request_type & RELATED_REQUEST) {\n\t\t\tshdr->Flags |= SMB2_FLAGS_RELATED_OPERATIONS;\n\t\t\t/*\n\t\t\t * Related requests use info from previous read request\n\t\t\t * in chain.\n\t\t\t */\n\t\t\tshdr->SessionId = 0xFFFFFFFF;\n\t\t\tshdr->TreeId = 0xFFFFFFFF;\n\t\t\treq->PersistentFileId = 0xFFFFFFFF;\n\t\t\treq->VolatileFileId = 0xFFFFFFFF;\n\t\t}\n\t}\n\tif (remaining_bytes > io_parms->length)\n\t\treq->RemainingBytes = cpu_to_le32(remaining_bytes);\n\telse\n\t\treq->RemainingBytes = 0;\n\n\t*buf = req;\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_416": {
        "vulnerable_code": {
            "Code": "SMB2_QFS_attr(const unsigned int xid, struct cifs_tcon *tcon,\n\t      u64 persistent_fid, u64 volatile_fid, int level){\n\tstruct smb_rqst rqst;\n\tstruct smb2_query_info_rsp *rsp = NULL;\n\tstruct kvec iov;\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype, max_len, min_len;\n\tstruct cifs_ses *ses = tcon->ses;\n\tunsigned int rsp_len, offset;\n\tint flags = 0;\n\n\tif (level == FS_DEVICE_INFORMATION) {\n\t\tmax_len = sizeof(FILE_SYSTEM_DEVICE_INFO);\n\t\tmin_len = sizeof(FILE_SYSTEM_DEVICE_INFO);\n\t} else if (level == FS_ATTRIBUTE_INFORMATION) {\n\t\tmax_len = sizeof(FILE_SYSTEM_ATTRIBUTE_INFO);\n\t\tmin_len = MIN_FS_ATTR_INFO_SIZE;\n\t} else if (level == FS_SECTOR_SIZE_INFORMATION) {\n\t\tmax_len = sizeof(struct smb3_fs_ss_info);\n\t\tmin_len = sizeof(struct smb3_fs_ss_info);\n\t} else if (level == FS_VOLUME_INFORMATION) {\n\t\tmax_len = sizeof(struct smb3_fs_vol_info) + MAX_VOL_LABEL_LEN;\n\t\tmin_len = sizeof(struct smb3_fs_vol_info);\n\t} else {\n\t\tcifs_dbg(FYI, \"Invalid qfsinfo level %d\\n\", level);\n\t\treturn -EINVAL;\n\t}\n\n\trc = build_qfs_info_req(&iov, tcon, level, max_len,\n\t\t\t\tpersistent_fid, volatile_fid);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = &iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(iov.iov_base);\n\tif (rc) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_QUERY_INFO_HE);\n\t\tgoto qfsattr_exit;\n\t}\n\trsp = (struct smb2_query_info_rsp *)rsp_iov.iov_base;\n\n\trsp_len = le32_to_cpu(rsp->OutputBufferLength);\n\toffset = le16_to_cpu(rsp->OutputBufferOffset);\n\trc = smb2_validate_iov(offset, rsp_len, &rsp_iov, min_len);\n\tif (rc)\n\t\tgoto qfsattr_exit;\n\n\tif (level == FS_ATTRIBUTE_INFORMATION)\n\t\tmemcpy(&tcon->fsAttrInfo, offset\n\t\t\t+ (char *)rsp, min_t(unsigned int,\n\t\t\trsp_len, max_len));\n\telse if (level == FS_DEVICE_INFORMATION)\n\t\tmemcpy(&tcon->fsDevInfo, offset\n\t\t\t+ (char *)rsp, sizeof(FILE_SYSTEM_DEVICE_INFO));\n\telse if (level == FS_SECTOR_SIZE_INFORMATION) {\n\t\tstruct smb3_fs_ss_info *ss_info = (struct smb3_fs_ss_info *)\n\t\t\t(offset + (char *)rsp);\n\t\ttcon->ss_flags = le32_to_cpu(ss_info->Flags);\n\t\ttcon->perf_sector_size =\n\t\t\tle32_to_cpu(ss_info->PhysicalBytesPerSectorForPerf);\n\t} else if (level == FS_VOLUME_INFORMATION) {\n\t\tstruct smb3_fs_vol_info *vol_info = (struct smb3_fs_vol_info *)\n\t\t\t(offset + (char *)rsp);\n\t\ttcon->vol_serial_number = vol_info->VolumeSerialNumber;\n\t\ttcon->vol_create_time = vol_info->VolumeCreationTime;\n\t}\n\nqfsattr_exit:\n\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_QFS_attr(const unsigned int xid, struct cifs_tcon *tcon,\n\t      u64 persistent_fid, u64 volatile_fid, int level){\n\tstruct smb_rqst rqst;\n\tstruct smb2_query_info_rsp *rsp = NULL;\n\tstruct kvec iov;\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype, max_len, min_len;\n\tstruct cifs_ses *ses = tcon->ses;\n\tunsigned int rsp_len, offset;\n\tint flags = 0;\n\n\tif (level == FS_DEVICE_INFORMATION) {\n\t\tmax_len = sizeof(FILE_SYSTEM_DEVICE_INFO);\n\t\tmin_len = sizeof(FILE_SYSTEM_DEVICE_INFO);\n\t} else if (level == FS_ATTRIBUTE_INFORMATION) {\n\t\tmax_len = sizeof(FILE_SYSTEM_ATTRIBUTE_INFO);\n\t\tmin_len = MIN_FS_ATTR_INFO_SIZE;\n\t} else if (level == FS_SECTOR_SIZE_INFORMATION) {\n\t\tmax_len = sizeof(struct smb3_fs_ss_info);\n\t\tmin_len = sizeof(struct smb3_fs_ss_info);\n\t} else if (level == FS_VOLUME_INFORMATION) {\n\t\tmax_len = sizeof(struct smb3_fs_vol_info) + MAX_VOL_LABEL_LEN;\n\t\tmin_len = sizeof(struct smb3_fs_vol_info);\n\t} else {\n\t\tcifs_dbg(FYI, \"Invalid qfsinfo level %d\\n\", level);\n\t\treturn -EINVAL;\n\t}\n\n\trc = build_qfs_info_req(&iov, tcon, level, max_len,\n\t\t\t\tpersistent_fid, volatile_fid);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = &iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(iov.iov_base);\n\tif (rc) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_QUERY_INFO_HE);\n\t\tgoto qfsattr_exit;\n\t}\n\trsp = (struct smb2_query_info_rsp *)rsp_iov.iov_base;\n\n\trsp_len = le32_to_cpu(rsp->OutputBufferLength);\n\toffset = le16_to_cpu(rsp->OutputBufferOffset);\n\trc = smb2_validate_iov(offset, rsp_len, &rsp_iov, min_len);\n\tif (rc)\n\t\tgoto qfsattr_exit;\n\n\tif (level == FS_ATTRIBUTE_INFORMATION)\n\t\tmemcpy(&tcon->fsAttrInfo, offset\n\t\t\t+ (char *)rsp, min_t(unsigned int,\n\t\t\trsp_len, max_len));\n\telse if (level == FS_DEVICE_INFORMATION)\n\t\tmemcpy(&tcon->fsDevInfo, offset\n\t\t\t+ (char *)rsp, sizeof(FILE_SYSTEM_DEVICE_INFO));\n\telse if (level == FS_SECTOR_SIZE_INFORMATION) {\n\t\tstruct smb3_fs_ss_info *ss_info = (struct smb3_fs_ss_info *)\n\t\t\t(offset + (char *)rsp);\n\t\ttcon->ss_flags = le32_to_cpu(ss_info->Flags);\n\t\ttcon->perf_sector_size =\n\t\t\tle32_to_cpu(ss_info->PhysicalBytesPerSectorForPerf);\n\t} else if (level == FS_VOLUME_INFORMATION) {\n\t\tstruct smb3_fs_vol_info *vol_info = (struct smb3_fs_vol_info *)\n\t\t\t(offset + (char *)rsp);\n\t\ttcon->vol_serial_number = vol_info->VolumeSerialNumber;\n\t\ttcon->vol_create_time = vol_info->VolumeCreationTime;\n\t}\n\nqfsattr_exit:\n\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_417": {
        "vulnerable_code": {
            "Code": "SMB2_negotiate(const unsigned int xid, struct cifs_ses *ses){\n\tstruct smb_rqst rqst;\n\tstruct smb2_negotiate_req *req;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype;\n\tstruct TCP_Server_Info *server = ses->server;\n\tint blob_offset, blob_length;\n\tchar *security_blob;\n\tint flags = CIFS_NEG_OP;\n\tunsigned int total_len;\n\n\tcifs_dbg(FYI, \"Negotiate protocol\\n\");\n\n\tif (!server) {\n\t\tWARN(1, \"%s: server is NULL!\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\trc = smb2_plain_req_init(SMB2_NEGOTIATE, NULL, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\treq->sync_hdr.SessionId = 0;\n\n\tmemset(server->preauth_sha_hash, 0, SMB2_PREAUTH_HASH_SIZE);\n\tmemset(ses->preauth_sha_hash, 0, SMB2_PREAUTH_HASH_SIZE);\n\n\tif (strcmp(ses->server->vals->version_string,\n\t\t   SMB3ANY_VERSION_STRING) == 0) {\n\t\treq->Dialects[0] = cpu_to_le16(SMB30_PROT_ID);\n\t\treq->Dialects[1] = cpu_to_le16(SMB302_PROT_ID);\n\t\treq->DialectCount = cpu_to_le16(2);\n\t\ttotal_len += 4;\n\t} else if (strcmp(ses->server->vals->version_string,\n\t\t   SMBDEFAULT_VERSION_STRING) == 0) {\n\t\treq->Dialects[0] = cpu_to_le16(SMB21_PROT_ID);\n\t\treq->Dialects[1] = cpu_to_le16(SMB30_PROT_ID);\n\t\treq->Dialects[2] = cpu_to_le16(SMB302_PROT_ID);\n\t\treq->Dialects[3] = cpu_to_le16(SMB311_PROT_ID);\n\t\treq->DialectCount = cpu_to_le16(4);\n\t\ttotal_len += 8;\n\t} else {\n\t\t/* otherwise send specific dialect */\n\t\treq->Dialects[0] = cpu_to_le16(ses->server->vals->protocol_id);\n\t\treq->DialectCount = cpu_to_le16(1);\n\t\ttotal_len += 2;\n\t}\n\n\t/* only one of SMB2 signing flags may be set in SMB2 request */\n\tif (ses->sign)\n\t\treq->SecurityMode = cpu_to_le16(SMB2_NEGOTIATE_SIGNING_REQUIRED);\n\telse if (global_secflags & CIFSSEC_MAY_SIGN)\n\t\treq->SecurityMode = cpu_to_le16(SMB2_NEGOTIATE_SIGNING_ENABLED);\n\telse\n\t\treq->SecurityMode = 0;\n\n\treq->Capabilities = cpu_to_le32(ses->server->vals->req_capabilities);\n\n\t/* ClientGUID must be zero for SMB2.02 dialect */\n\tif (ses->server->vals->protocol_id == SMB20_PROT_ID)\n\t\tmemset(req->ClientGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\telse {\n\t\tmemcpy(req->ClientGUID, server->client_guid,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\t\tif ((ses->server->vals->protocol_id == SMB311_PROT_ID) ||\n\t\t    (strcmp(ses->server->vals->version_string,\n\t\t     SMBDEFAULT_VERSION_STRING) == 0))\n\t\t\tassemble_neg_contexts(req, &total_len);\n\t}\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_negotiate_rsp *)rsp_iov.iov_base;\n\t/*\n\t * No tcon so can't do\n\t * cifs_stats_inc(&tcon->stats.smb2_stats.smb2_com_fail[SMB2...]);\n\t */\n\tif (rc == -EOPNOTSUPP) {\n\t\tcifs_dbg(VFS, \"Dialect not supported by server. Consider \"\n\t\t\t\"specifying vers=1.0 or vers=2.0 on mount for accessing\"\n\t\t\t\" older servers\\n\");\n\t\tgoto neg_exit;\n\t} else if (rc != 0)\n\t\tgoto neg_exit;\n\n\tif (strcmp(ses->server->vals->version_string,\n\t\t   SMB3ANY_VERSION_STRING) == 0) {\n\t\tif (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) {\n\t\t\tcifs_dbg(VFS,\n\t\t\t\t\"SMB2 dialect returned but not requested\\n\");\n\t\t\treturn -EIO;\n\t\t} else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) {\n\t\t\tcifs_dbg(VFS,\n\t\t\t\t\"SMB2.1 dialect returned but not requested\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\t} else if (strcmp(ses->server->vals->version_string,\n\t\t   SMBDEFAULT_VERSION_STRING) == 0) {\n\t\tif (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) {\n\t\t\tcifs_dbg(VFS,\n\t\t\t\t\"SMB2 dialect returned but not requested\\n\");\n\t\t\treturn -EIO;\n\t\t} else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) {\n\t\t\t/* ops set to 3.0 by default for default so update */\n\t\t\tses->server->ops = &smb21_operations;\n\t\t} else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID))\n\t\t\tses->server->ops = &smb311_operations;\n\t} else if (le16_to_cpu(rsp->DialectRevision) !=\n\t\t\t\tses->server->vals->protocol_id) {\n\t\t/* if requested single dialect ensure returned dialect matched */\n\t\tcifs_dbg(VFS, \"Illegal 0x%x dialect returned: not requested\\n\",\n\t\t\tle16_to_cpu(rsp->DialectRevision));\n\t\treturn -EIO;\n\t}\n\n\tcifs_dbg(FYI, \"mode 0x%x\\n\", rsp->SecurityMode);\n\n\tif (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb2.0 dialect\\n\");\n\telse if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb2.1 dialect\\n\");\n\telse if (rsp->DialectRevision == cpu_to_le16(SMB30_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb3.0 dialect\\n\");\n\telse if (rsp->DialectRevision == cpu_to_le16(SMB302_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb3.02 dialect\\n\");\n\telse if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb3.1.1 dialect\\n\");\n\telse {\n\t\tcifs_dbg(VFS, \"Illegal dialect returned by server 0x%x\\n\",\n\t\t\t le16_to_cpu(rsp->DialectRevision));\n\t\trc = -EIO;\n\t\tgoto neg_exit;\n\t}\n\tserver->dialect = le16_to_cpu(rsp->DialectRevision);\n\n\t/*\n\t * Keep a copy of the hash after negprot. This hash will be\n\t * the starting hash value for all sessions made from this\n\t * server.\n\t */\n\tmemcpy(server->preauth_sha_hash, ses->preauth_sha_hash,\n\t       SMB2_PREAUTH_HASH_SIZE);\n\n\t/* SMB2 only has an extended negflavor */\n\tserver->negflavor = CIFS_NEGFLAVOR_EXTENDED;\n\t/* set it to the maximum buffer size value we can send with 1 credit */\n\tserver->maxBuf = min_t(unsigned int, le32_to_cpu(rsp->MaxTransactSize),\n\t\t\t       SMB2_MAX_BUFFER_SIZE);\n\tserver->max_read = le32_to_cpu(rsp->MaxReadSize);\n\tserver->max_write = le32_to_cpu(rsp->MaxWriteSize);\n\tserver->sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tif ((server->sec_mode & SMB2_SEC_MODE_FLAGS_ALL) != server->sec_mode)\n\t\tcifs_dbg(FYI, \"Server returned unexpected security mode 0x%x\\n\",\n\t\t\t\tserver->sec_mode);\n\tserver->capabilities = le32_to_cpu(rsp->Capabilities);\n\t/* Internal types */\n\tserver->capabilities |= SMB2_NT_FIND | SMB2_LARGE_FILES;\n\n\tsecurity_blob = smb2_get_data_area_len(&blob_offset, &blob_length,\n\t\t\t\t\t       (struct smb2_sync_hdr *)rsp);\n\t/*\n\t * See MS-SMB2 section 2.2.4: if no blob, client picks default which\n\t * for us will be\n\t *\tses->sectype = RawNTLMSSP;\n\t * but for time being this is our only auth choice so doesn't matter.\n\t * We just found a server which sets blob length to zero expecting raw.\n\t */\n\tif (blob_length == 0) {\n\t\tcifs_dbg(FYI, \"missing security blob on negprot\\n\");\n\t\tserver->sec_ntlmssp = true;\n\t}\n\n\trc = cifs_enable_signing(server, ses->sign);\n\tif (rc)\n\t\tgoto neg_exit;\n\tif (blob_length) {\n\t\trc = decode_negTokenInit(security_blob, blob_length, server);\n\t\tif (rc == 1)\n\t\t\trc = 0;\n\t\telse if (rc == 0)\n\t\t\trc = -EIO;\n\t}\n\n\tif (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) {\n\t\tif (rsp->NegotiateContextCount)\n\t\t\trc = smb311_decode_neg_context(rsp, server,\n\t\t\t\t\t\t       rsp_iov.iov_len);\n\t\telse\n\t\t\tcifs_dbg(VFS, \"Missing expected negotiate contexts\\n\");\n\t}\nneg_exit:\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
            "Size": 0,
            "Code Complexity": 0,
            "Memory Management": 0,
            "Code Clarity": 0,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_negotiate(const unsigned int xid, struct cifs_ses *ses){\n\tstruct smb_rqst rqst;\n\tstruct smb2_negotiate_req *req;\n\tstruct smb2_negotiate_rsp *rsp;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tint rc = 0;\n\tint resp_buftype;\n\tstruct TCP_Server_Info *server = ses->server;\n\tint blob_offset, blob_length;\n\tchar *security_blob;\n\tint flags = CIFS_NEG_OP;\n\tunsigned int total_len;\n\n\tcifs_dbg(FYI, \"Negotiate protocol\\n\");\n\n\tif (!server) {\n\t\tWARN(1, \"%s: server is NULL!\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\trc = smb2_plain_req_init(SMB2_NEGOTIATE, NULL, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\treq->sync_hdr.SessionId = 0;\n\n\tmemset(server->preauth_sha_hash, 0, SMB2_PREAUTH_HASH_SIZE);\n\tmemset(ses->preauth_sha_hash, 0, SMB2_PREAUTH_HASH_SIZE);\n\n\tif (strcmp(ses->server->vals->version_string,\n\t\t   SMB3ANY_VERSION_STRING) == 0) {\n\t\treq->Dialects[0] = cpu_to_le16(SMB30_PROT_ID);\n\t\treq->Dialects[1] = cpu_to_le16(SMB302_PROT_ID);\n\t\treq->DialectCount = cpu_to_le16(2);\n\t\ttotal_len += 4;\n\t} else if (strcmp(ses->server->vals->version_string,\n\t\t   SMBDEFAULT_VERSION_STRING) == 0) {\n\t\treq->Dialects[0] = cpu_to_le16(SMB21_PROT_ID);\n\t\treq->Dialects[1] = cpu_to_le16(SMB30_PROT_ID);\n\t\treq->Dialects[2] = cpu_to_le16(SMB302_PROT_ID);\n\t\treq->Dialects[3] = cpu_to_le16(SMB311_PROT_ID);\n\t\treq->DialectCount = cpu_to_le16(4);\n\t\ttotal_len += 8;\n\t} else {\n\t\t/* otherwise send specific dialect */\n\t\treq->Dialects[0] = cpu_to_le16(ses->server->vals->protocol_id);\n\t\treq->DialectCount = cpu_to_le16(1);\n\t\ttotal_len += 2;\n\t}\n\n\t/* only one of SMB2 signing flags may be set in SMB2 request */\n\tif (ses->sign)\n\t\treq->SecurityMode = cpu_to_le16(SMB2_NEGOTIATE_SIGNING_REQUIRED);\n\telse if (global_secflags & CIFSSEC_MAY_SIGN)\n\t\treq->SecurityMode = cpu_to_le16(SMB2_NEGOTIATE_SIGNING_ENABLED);\n\telse\n\t\treq->SecurityMode = 0;\n\n\treq->Capabilities = cpu_to_le32(ses->server->vals->req_capabilities);\n\n\t/* ClientGUID must be zero for SMB2.02 dialect */\n\tif (ses->server->vals->protocol_id == SMB20_PROT_ID)\n\t\tmemset(req->ClientGUID, 0, SMB2_CLIENT_GUID_SIZE);\n\telse {\n\t\tmemcpy(req->ClientGUID, server->client_guid,\n\t\t\tSMB2_CLIENT_GUID_SIZE);\n\t\tif ((ses->server->vals->protocol_id == SMB311_PROT_ID) ||\n\t\t    (strcmp(ses->server->vals->version_string,\n\t\t     SMBDEFAULT_VERSION_STRING) == 0))\n\t\t\tassemble_neg_contexts(req, &total_len);\n\t}\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_negotiate_rsp *)rsp_iov.iov_base;\n\t/*\n\t * No tcon so can't do\n\t * cifs_stats_inc(&tcon->stats.smb2_stats.smb2_com_fail[SMB2...]);\n\t */\n\tif (rc == -EOPNOTSUPP) {\n\t\tcifs_dbg(VFS, \"Dialect not supported by server. Consider \"\n\t\t\t\"specifying vers=1.0 or vers=2.0 on mount for accessing\"\n\t\t\t\" older servers\\n\");\n\t\tgoto neg_exit;\n\t} else if (rc != 0)\n\t\tgoto neg_exit;\n\n\tif (strcmp(ses->server->vals->version_string,\n\t\t   SMB3ANY_VERSION_STRING) == 0) {\n\t\tif (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) {\n\t\t\tcifs_dbg(VFS,\n\t\t\t\t\"SMB2 dialect returned but not requested\\n\");\n\t\t\treturn -EIO;\n\t\t} else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) {\n\t\t\tcifs_dbg(VFS,\n\t\t\t\t\"SMB2.1 dialect returned but not requested\\n\");\n\t\t\treturn -EIO;\n\t\t}\n\t} else if (strcmp(ses->server->vals->version_string,\n\t\t   SMBDEFAULT_VERSION_STRING) == 0) {\n\t\tif (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID)) {\n\t\t\tcifs_dbg(VFS,\n\t\t\t\t\"SMB2 dialect returned but not requested\\n\");\n\t\t\treturn -EIO;\n\t\t} else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) {\n\t\t\t/* ops set to 3.0 by default for default so update */\n\t\t\tses->server->ops = &smb21_operations;\n\t\t} else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID))\n\t\t\tses->server->ops = &smb311_operations;\n\t} else if (le16_to_cpu(rsp->DialectRevision) !=\n\t\t\t\tses->server->vals->protocol_id) {\n\t\t/* if requested single dialect ensure returned dialect matched */\n\t\tcifs_dbg(VFS, \"Illegal 0x%x dialect returned: not requested\\n\",\n\t\t\tle16_to_cpu(rsp->DialectRevision));\n\t\treturn -EIO;\n\t}\n\n\tcifs_dbg(FYI, \"mode 0x%x\\n\", rsp->SecurityMode);\n\n\tif (rsp->DialectRevision == cpu_to_le16(SMB20_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb2.0 dialect\\n\");\n\telse if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb2.1 dialect\\n\");\n\telse if (rsp->DialectRevision == cpu_to_le16(SMB30_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb3.0 dialect\\n\");\n\telse if (rsp->DialectRevision == cpu_to_le16(SMB302_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb3.02 dialect\\n\");\n\telse if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID))\n\t\tcifs_dbg(FYI, \"negotiated smb3.1.1 dialect\\n\");\n\telse {\n\t\tcifs_dbg(VFS, \"Illegal dialect returned by server 0x%x\\n\",\n\t\t\t le16_to_cpu(rsp->DialectRevision));\n\t\trc = -EIO;\n\t\tgoto neg_exit;\n\t}\n\tserver->dialect = le16_to_cpu(rsp->DialectRevision);\n\n\t/*\n\t * Keep a copy of the hash after negprot. This hash will be\n\t * the starting hash value for all sessions made from this\n\t * server.\n\t */\n\tmemcpy(server->preauth_sha_hash, ses->preauth_sha_hash,\n\t       SMB2_PREAUTH_HASH_SIZE);\n\n\t/* SMB2 only has an extended negflavor */\n\tserver->negflavor = CIFS_NEGFLAVOR_EXTENDED;\n\t/* set it to the maximum buffer size value we can send with 1 credit */\n\tserver->maxBuf = min_t(unsigned int, le32_to_cpu(rsp->MaxTransactSize),\n\t\t\t       SMB2_MAX_BUFFER_SIZE);\n\tserver->max_read = le32_to_cpu(rsp->MaxReadSize);\n\tserver->max_write = le32_to_cpu(rsp->MaxWriteSize);\n\tserver->sec_mode = le16_to_cpu(rsp->SecurityMode);\n\tif ((server->sec_mode & SMB2_SEC_MODE_FLAGS_ALL) != server->sec_mode)\n\t\tcifs_dbg(FYI, \"Server returned unexpected security mode 0x%x\\n\",\n\t\t\t\tserver->sec_mode);\n\tserver->capabilities = le32_to_cpu(rsp->Capabilities);\n\t/* Internal types */\n\tserver->capabilities |= SMB2_NT_FIND | SMB2_LARGE_FILES;\n\n\tsecurity_blob = smb2_get_data_area_len(&blob_offset, &blob_length,\n\t\t\t\t\t       (struct smb2_sync_hdr *)rsp);\n\t/*\n\t * See MS-SMB2 section 2.2.4: if no blob, client picks default which\n\t * for us will be\n\t *\tses->sectype = RawNTLMSSP;\n\t * but for time being this is our only auth choice so doesn't matter.\n\t * We just found a server which sets blob length to zero expecting raw.\n\t */\n\tif (blob_length == 0) {\n\t\tcifs_dbg(FYI, \"missing security blob on negprot\\n\");\n\t\tserver->sec_ntlmssp = true;\n\t}\n\n\trc = cifs_enable_signing(server, ses->sign);\n\tif (rc)\n\t\tgoto neg_exit;\n\tif (blob_length) {\n\t\trc = decode_negTokenInit(security_blob, blob_length, server);\n\t\tif (rc == 1)\n\t\t\trc = 0;\n\t\telse if (rc == 0)\n\t\t\trc = -EIO;\n\t}\n\n\tif (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) {\n\t\tif (rsp->NegotiateContextCount)\n\t\t\trc = smb311_decode_neg_context(rsp, server,\n\t\t\t\t\t\t       rsp_iov.iov_len);\n\t\telse\n\t\t\tcifs_dbg(VFS, \"Missing expected negotiate contexts\\n\");\n\t}\nneg_exit:\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
            "Size": 0,
            "Code Complexity": 0,
            "Memory Management": 0,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_418": {
        "vulnerable_code": {
            "Code": "SMB2_oplock_break(const unsigned int xid, struct cifs_tcon *tcon,\n\t\t  const u64 persistent_fid, const u64 volatile_fid,\n\t\t  __u8 oplock_level){\n\tstruct smb_rqst rqst;\n\tint rc;\n\tstruct smb2_oplock_break *req = NULL;\n\tstruct cifs_ses *ses = tcon->ses;\n\tint flags = CIFS_OBREAK_OP;\n\tunsigned int total_len;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tint resp_buf_type;\n\n\tcifs_dbg(FYI, \"SMB2_oplock_break\\n\");\n\trc = smb2_plain_req_init(SMB2_OPLOCK_BREAK, tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->VolatileFid = volatile_fid;\n\treq->PersistentFid = persistent_fid;\n\treq->OplockLevel = oplock_level;\n\treq->sync_hdr.CreditRequest = cpu_to_le16(1);\n\n\tflags |= CIFS_NO_RESP;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buf_type, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\n\tif (rc) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_OPLOCK_BREAK_HE);\n\t\tcifs_dbg(FYI, \"Send error in Oplock Break = %d\\n\", rc);\n\t}\n\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_oplock_break(const unsigned int xid, struct cifs_tcon *tcon,\n\t\t  const u64 persistent_fid, const u64 volatile_fid,\n\t\t  __u8 oplock_level){\n\tstruct smb_rqst rqst;\n\tint rc;\n\tstruct smb2_oplock_break *req = NULL;\n\tstruct cifs_ses *ses = tcon->ses;\n\tint flags = CIFS_OBREAK_OP;\n\tunsigned int total_len;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tint resp_buf_type;\n\n\tcifs_dbg(FYI, \"SMB2_oplock_break\\n\");\n\trc = smb2_plain_req_init(SMB2_OPLOCK_BREAK, tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->VolatileFid = volatile_fid;\n\treq->PersistentFid = persistent_fid;\n\treq->OplockLevel = oplock_level;\n\treq->sync_hdr.CreditRequest = cpu_to_le16(1);\n\n\tflags |= CIFS_NO_RESP;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buf_type, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\n\tif (rc) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_OPLOCK_BREAK_HE);\n\t\tcifs_dbg(FYI, \"Send error in Oplock Break = %d\\n\", rc);\n\t}\n\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_419": {
        "vulnerable_code": {
            "Code": "SMB2_read(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t  unsigned int *nbytes, char **buf, int *buf_type){\n\tstruct smb_rqst rqst;\n\tint resp_buftype, rc = -EACCES;\n\tstruct smb2_read_plain_req *req = NULL;\n\tstruct smb2_read_rsp *rsp = NULL;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tunsigned int total_len;\n\tint flags = CIFS_LOG_ERROR;\n\tstruct cifs_ses *ses = io_parms->tcon->ses;\n\n\t*nbytes = 0;\n\trc = smb2_new_read_req((void **)&req, &total_len, io_parms, NULL, 0, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\n\trsp = (struct smb2_read_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\tif (rc != -ENODATA) {\n\t\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_READ_HE);\n\t\t\tcifs_dbg(VFS, \"Send error in read = %d\\n\", rc);\n\t\t\ttrace_smb3_read_err(xid, req->PersistentFileId,\n\t\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t\t    io_parms->offset, io_parms->length,\n\t\t\t\t\t    rc);\n\t\t} else\n\t\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, 0);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t\treturn rc == -ENODATA ? 0 : rc;\n\t} else\n\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, io_parms->length);\n\n\t*nbytes = le32_to_cpu(rsp->DataLength);\n\tif ((*nbytes > CIFS_MAX_MSGSIZE) ||\n\t    (*nbytes > io_parms->length)) {\n\t\tcifs_dbg(FYI, \"bad length %d for count %d\\n\",\n\t\t\t *nbytes, io_parms->length);\n\t\trc = -EIO;\n\t\t*nbytes = 0;\n\t}\n\n\tif (*buf) {\n\t\tmemcpy(*buf, (char *)rsp + rsp->DataOffset, *nbytes);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t} else if (resp_buftype != CIFS_NO_BUFFER) {\n\t\t*buf = rsp_iov.iov_base;\n\t\tif (resp_buftype == CIFS_SMALL_BUFFER)\n\t\t\t*buf_type = CIFS_SMALL_BUFFER;\n\t\telse if (resp_buftype == CIFS_LARGE_BUFFER)\n\t\t\t*buf_type = CIFS_LARGE_BUFFER;\n\t}\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_read(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t  unsigned int *nbytes, char **buf, int *buf_type){\n\tstruct smb_rqst rqst;\n\tint resp_buftype, rc = -EACCES;\n\tstruct smb2_read_plain_req *req = NULL;\n\tstruct smb2_read_rsp *rsp = NULL;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tunsigned int total_len;\n\tint flags = CIFS_LOG_ERROR;\n\tstruct cifs_ses *ses = io_parms->tcon->ses;\n\n\t*nbytes = 0;\n\trc = smb2_new_read_req((void **)&req, &total_len, io_parms, NULL, 0, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_read_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\tif (rc != -ENODATA) {\n\t\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_READ_HE);\n\t\t\tcifs_dbg(VFS, \"Send error in read = %d\\n\", rc);\n\t\t\ttrace_smb3_read_err(xid, req->PersistentFileId,\n\t\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t\t    io_parms->offset, io_parms->length,\n\t\t\t\t\t    rc);\n\t\t} else\n\t\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, 0);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t\treturn rc == -ENODATA ? 0 : rc;\n\t} else\n\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, io_parms->length);\n\n\tcifs_small_buf_release(req);\n\n\t*nbytes = le32_to_cpu(rsp->DataLength);\n\tif ((*nbytes > CIFS_MAX_MSGSIZE) ||\n\t    (*nbytes > io_parms->length)) {\n\t\tcifs_dbg(FYI, \"bad length %d for count %d\\n\",\n\t\t\t *nbytes, io_parms->length);\n\t\trc = -EIO;\n\t\t*nbytes = 0;\n\t}\n\n\tif (*buf) {\n\t\tmemcpy(*buf, (char *)rsp + rsp->DataOffset, *nbytes);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t} else if (resp_buftype != CIFS_NO_BUFFER) {\n\t\t*buf = rsp_iov.iov_base;\n\t\tif (resp_buftype == CIFS_SMALL_BUFFER)\n\t\t\t*buf_type = CIFS_SMALL_BUFFER;\n\t\telse if (resp_buftype == CIFS_LARGE_BUFFER)\n\t\t\t*buf_type = CIFS_LARGE_BUFFER;\n\t}\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_420": {
        "vulnerable_code": {
            "Code": "SMB2_read(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t  unsigned int *nbytes, char **buf, int *buf_type){\n\tstruct smb_rqst rqst;\n\tint resp_buftype, rc = -EACCES;\n\tstruct smb2_read_plain_req *req = NULL;\n\tstruct smb2_read_rsp *rsp = NULL;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tunsigned int total_len;\n\tint flags = CIFS_LOG_ERROR;\n\tstruct cifs_ses *ses = io_parms->tcon->ses;\n\n\t*nbytes = 0;\n\trc = smb2_new_read_req((void **)&req, &total_len, io_parms, NULL, 0, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\n\trsp = (struct smb2_read_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\tif (rc != -ENODATA) {\n\t\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_READ_HE);\n\t\t\tcifs_dbg(VFS, \"Send error in read = %d\\n\", rc);\n\t\t\ttrace_smb3_read_err(xid, req->PersistentFileId,\n\t\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t\t    io_parms->offset, io_parms->length,\n\t\t\t\t\t    rc);\n\t\t} else\n\t\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, 0);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t\treturn rc == -ENODATA ? 0 : rc;\n\t} else\n\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, io_parms->length);\n\n\t*nbytes = le32_to_cpu(rsp->DataLength);\n\tif ((*nbytes > CIFS_MAX_MSGSIZE) ||\n\t    (*nbytes > io_parms->length)) {\n\t\tcifs_dbg(FYI, \"bad length %d for count %d\\n\",\n\t\t\t *nbytes, io_parms->length);\n\t\trc = -EIO;\n\t\t*nbytes = 0;\n\t}\n\n\tif (*buf) {\n\t\tmemcpy(*buf, (char *)rsp + rsp->DataOffset, *nbytes);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t} else if (resp_buftype != CIFS_NO_BUFFER) {\n\t\t*buf = rsp_iov.iov_base;\n\t\tif (resp_buftype == CIFS_SMALL_BUFFER)\n\t\t\t*buf_type = CIFS_SMALL_BUFFER;\n\t\telse if (resp_buftype == CIFS_LARGE_BUFFER)\n\t\t\t*buf_type = CIFS_LARGE_BUFFER;\n\t}\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_read(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t  unsigned int *nbytes, char **buf, int *buf_type){\n\tstruct smb_rqst rqst;\n\tint resp_buftype, rc = -EACCES;\n\tstruct smb2_read_plain_req *req = NULL;\n\tstruct smb2_read_rsp *rsp = NULL;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tunsigned int total_len;\n\tint flags = CIFS_LOG_ERROR;\n\tstruct cifs_ses *ses = io_parms->tcon->ses;\n\n\t*nbytes = 0;\n\trc = smb2_new_read_req((void **)&req, &total_len, io_parms, NULL, 0, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_read_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\tif (rc != -ENODATA) {\n\t\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_READ_HE);\n\t\t\tcifs_dbg(VFS, \"Send error in read = %d\\n\", rc);\n\t\t\ttrace_smb3_read_err(xid, req->PersistentFileId,\n\t\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t\t    io_parms->offset, io_parms->length,\n\t\t\t\t\t    rc);\n\t\t} else\n\t\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, 0);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t\treturn rc == -ENODATA ? 0 : rc;\n\t} else\n\t\ttrace_smb3_read_done(xid, req->PersistentFileId,\n\t\t\t\t    io_parms->tcon->tid, ses->Suid,\n\t\t\t\t    io_parms->offset, io_parms->length);\n\n\tcifs_small_buf_release(req);\n\n\t*nbytes = le32_to_cpu(rsp->DataLength);\n\tif ((*nbytes > CIFS_MAX_MSGSIZE) ||\n\t    (*nbytes > io_parms->length)) {\n\t\tcifs_dbg(FYI, \"bad length %d for count %d\\n\",\n\t\t\t *nbytes, io_parms->length);\n\t\trc = -EIO;\n\t\t*nbytes = 0;\n\t}\n\n\tif (*buf) {\n\t\tmemcpy(*buf, (char *)rsp + rsp->DataOffset, *nbytes);\n\t\tfree_rsp_buf(resp_buftype, rsp_iov.iov_base);\n\t} else if (resp_buftype != CIFS_NO_BUFFER) {\n\t\t*buf = rsp_iov.iov_base;\n\t\tif (resp_buftype == CIFS_SMALL_BUFFER)\n\t\t\t*buf_type = CIFS_SMALL_BUFFER;\n\t\telse if (resp_buftype == CIFS_LARGE_BUFFER)\n\t\t\t*buf_type = CIFS_LARGE_BUFFER;\n\t}\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_421": {
        "vulnerable_code": {
            "Code": "SMB2_sess_alloc_buffer(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct cifs_ses *ses = sess_data->ses;\n\tstruct smb2_sess_setup_req *req;\n\tstruct TCP_Server_Info *server = ses->server;\n\tunsigned int total_len;\n\n\trc = smb2_plain_req_init(SMB2_SESSION_SETUP, NULL, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\t/* First session, not a reauthenticate */\n\treq->sync_hdr.SessionId = 0;\n\n\t/* if reconnect, we need to send previous sess id, otherwise it is 0 */\n\treq->PreviousSessionId = sess_data->previous_session;\n\n\treq->Flags = 0; /* MBZ */\n\n\t/* enough to enable echos and oplocks and one max size write */\n\treq->sync_hdr.CreditRequest = cpu_to_le16(130);\n\n\t/* only one of SMB2 signing flags may be set in SMB2 request */\n\tif (server->sign)\n\t\treq->SecurityMode = SMB2_NEGOTIATE_SIGNING_REQUIRED;\n\telse if (global_secflags & CIFSSEC_MAY_SIGN) /* one flag unlike MUST_ */\n\t\treq->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED;\n\telse\n\t\treq->SecurityMode = 0;\n\n\treq->Capabilities = 0;\n\treq->Channel = 0; /* MBZ */\n\n\tsess_data->iov[0].iov_base = (char *)req;\n\t/* 1 for pad */\n\tsess_data->iov[0].iov_len = total_len - 1;\n\t/*\n\t * This variable will be used to clear the buffer\n\t * allocated above in case of any error in the calling function.\n\t */\n\tsess_data->buf0_type = CIFS_SMALL_BUFFER;\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_sess_alloc_buffer(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct cifs_ses *ses = sess_data->ses;\n\tstruct smb2_sess_setup_req *req;\n\tstruct TCP_Server_Info *server = ses->server;\n\tunsigned int total_len;\n\n\trc = smb2_plain_req_init(SMB2_SESSION_SETUP, NULL, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\t/* First session, not a reauthenticate */\n\treq->sync_hdr.SessionId = 0;\n\n\t/* if reconnect, we need to send previous sess id, otherwise it is 0 */\n\treq->PreviousSessionId = sess_data->previous_session;\n\n\treq->Flags = 0; /* MBZ */\n\n\t/* enough to enable echos and oplocks and one max size write */\n\treq->sync_hdr.CreditRequest = cpu_to_le16(130);\n\n\t/* only one of SMB2 signing flags may be set in SMB2 request */\n\tif (server->sign)\n\t\treq->SecurityMode = SMB2_NEGOTIATE_SIGNING_REQUIRED;\n\telse if (global_secflags & CIFSSEC_MAY_SIGN) /* one flag unlike MUST_ */\n\t\treq->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED;\n\telse\n\t\treq->SecurityMode = 0;\n\n\treq->Capabilities = 0;\n\treq->Channel = 0; /* MBZ */\n\n\tsess_data->iov[0].iov_base = (char *)req;\n\t/* 1 for pad */\n\tsess_data->iov[0].iov_len = total_len - 1;\n\t/*\n\t * This variable will be used to clear the buffer\n\t * allocated above in case of any error in the calling function.\n\t */\n\tsess_data->buf0_type = CIFS_SMALL_BUFFER;\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_422": {
        "vulnerable_code": {
            "Code": "SMB2_sess_alloc_buffer(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct cifs_ses *ses = sess_data->ses;\n\tstruct smb2_sess_setup_req *req;\n\tstruct TCP_Server_Info *server = ses->server;\n\tunsigned int total_len;\n\n\trc = smb2_plain_req_init(SMB2_SESSION_SETUP, NULL, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\t/* First session, not a reauthenticate */\n\treq->sync_hdr.SessionId = 0;\n\n\t/* if reconnect, we need to send previous sess id, otherwise it is 0 */\n\treq->PreviousSessionId = sess_data->previous_session;\n\n\treq->Flags = 0; /* MBZ */\n\n\t/* enough to enable echos and oplocks and one max size write */\n\treq->sync_hdr.CreditRequest = cpu_to_le16(130);\n\n\t/* only one of SMB2 signing flags may be set in SMB2 request */\n\tif (server->sign)\n\t\treq->SecurityMode = SMB2_NEGOTIATE_SIGNING_REQUIRED;\n\telse if (global_secflags & CIFSSEC_MAY_SIGN) /* one flag unlike MUST_ */\n\t\treq->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED;\n\telse\n\t\treq->SecurityMode = 0;\n\n\treq->Capabilities = 0;\n\treq->Channel = 0; /* MBZ */\n\n\tsess_data->iov[0].iov_base = (char *)req;\n\t/* 1 for pad */\n\tsess_data->iov[0].iov_len = total_len - 1;\n\t/*\n\t * This variable will be used to clear the buffer\n\t * allocated above in case of any error in the calling function.\n\t */\n\tsess_data->buf0_type = CIFS_SMALL_BUFFER;\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_sess_alloc_buffer(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct cifs_ses *ses = sess_data->ses;\n\tstruct smb2_sess_setup_req *req;\n\tstruct TCP_Server_Info *server = ses->server;\n\tunsigned int total_len;\n\n\trc = smb2_plain_req_init(SMB2_SESSION_SETUP, NULL, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\t/* First session, not a reauthenticate */\n\treq->sync_hdr.SessionId = 0;\n\n\t/* if reconnect, we need to send previous sess id, otherwise it is 0 */\n\treq->PreviousSessionId = sess_data->previous_session;\n\n\treq->Flags = 0; /* MBZ */\n\n\t/* enough to enable echos and oplocks and one max size write */\n\treq->sync_hdr.CreditRequest = cpu_to_le16(130);\n\n\t/* only one of SMB2 signing flags may be set in SMB2 request */\n\tif (server->sign)\n\t\treq->SecurityMode = SMB2_NEGOTIATE_SIGNING_REQUIRED;\n\telse if (global_secflags & CIFSSEC_MAY_SIGN) /* one flag unlike MUST_ */\n\t\treq->SecurityMode = SMB2_NEGOTIATE_SIGNING_ENABLED;\n\telse\n\t\treq->SecurityMode = 0;\n\n\treq->Capabilities = 0;\n\treq->Channel = 0; /* MBZ */\n\n\tsess_data->iov[0].iov_base = (char *)req;\n\t/* 1 for pad */\n\tsess_data->iov[0].iov_len = total_len - 1;\n\t/*\n\t * This variable will be used to clear the buffer\n\t * allocated above in case of any error in the calling function.\n\t */\n\tsess_data->buf0_type = CIFS_SMALL_BUFFER;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_423": {
        "vulnerable_code": {
            "Code": "SMB2_sess_establish_session(struct SMB2_sess_data *sess_data){\n\tint rc = 0;\n\tstruct cifs_ses *ses = sess_data->ses;\n\n\tmutex_lock(&ses->server->srv_mutex);\n\tif (ses->server->ops->generate_signingkey) {\n\t\trc = ses->server->ops->generate_signingkey(ses);\n\t\tif (rc) {\n\t\t\tcifs_dbg(FYI,\n\t\t\t\t\"SMB3 session key generation failed\\n\");\n\t\t\tmutex_unlock(&ses->server->srv_mutex);\n\t\t\treturn rc;\n\t\t}\n\t}\n\tif (!ses->server->session_estab) {\n\t\tses->server->sequence_number = 0x2;\n\t\tses->server->session_estab = true;\n\t}\n\tmutex_unlock(&ses->server->srv_mutex);\n\n\tcifs_dbg(FYI, \"SMB2/3 session established successfully\\n\");\n\tspin_lock(&GlobalMid_Lock);\n\tses->status = CifsGood;\n\tses->need_reconnect = false;\n\tspin_unlock(&GlobalMid_Lock);\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_sess_establish_session(struct SMB2_sess_data *sess_data){\n\tint rc = 0;\n\tstruct cifs_ses *ses = sess_data->ses;\n\n\tmutex_lock(&ses->server->srv_mutex);\n\tif (ses->server->ops->generate_signingkey) {\n\t\trc = ses->server->ops->generate_signingkey(ses);\n\t\tif (rc) {\n\t\t\tcifs_dbg(FYI,\n\t\t\t\t\"SMB3 session key generation failed\\n\");\n\t\t\tmutex_unlock(&ses->server->srv_mutex);\n\t\t\treturn rc;\n\t\t}\n\t}\n\tif (!ses->server->session_estab) {\n\t\tses->server->sequence_number = 0x2;\n\t\tses->server->session_estab = true;\n\t}\n\tmutex_unlock(&ses->server->srv_mutex);\n\n\tcifs_dbg(FYI, \"SMB2/3 session established successfully\\n\");\n\tspin_lock(&GlobalMid_Lock);\n\tses->status = CifsGood;\n\tses->need_reconnect = false;\n\tspin_unlock(&GlobalMid_Lock);\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_424": {
        "vulnerable_code": {
            "Code": "SMB2_sess_establish_session(struct SMB2_sess_data *sess_data){\n\tint rc = 0;\n\tstruct cifs_ses *ses = sess_data->ses;\n\n\tmutex_lock(&ses->server->srv_mutex);\n\tif (ses->server->ops->generate_signingkey) {\n\t\trc = ses->server->ops->generate_signingkey(ses);\n\t\tif (rc) {\n\t\t\tcifs_dbg(FYI,\n\t\t\t\t\"SMB3 session key generation failed\\n\");\n\t\t\tmutex_unlock(&ses->server->srv_mutex);\n\t\t\treturn rc;\n\t\t}\n\t}\n\tif (!ses->server->session_estab) {\n\t\tses->server->sequence_number = 0x2;\n\t\tses->server->session_estab = true;\n\t}\n\tmutex_unlock(&ses->server->srv_mutex);\n\n\tcifs_dbg(FYI, \"SMB2/3 session established successfully\\n\");\n\tspin_lock(&GlobalMid_Lock);\n\tses->status = CifsGood;\n\tses->need_reconnect = false;\n\tspin_unlock(&GlobalMid_Lock);\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_sess_establish_session(struct SMB2_sess_data *sess_data){\n\tint rc = 0;\n\tstruct cifs_ses *ses = sess_data->ses;\n\n\tmutex_lock(&ses->server->srv_mutex);\n\tif (ses->server->ops->generate_signingkey) {\n\t\trc = ses->server->ops->generate_signingkey(ses);\n\t\tif (rc) {\n\t\t\tcifs_dbg(FYI,\n\t\t\t\t\"SMB3 session key generation failed\\n\");\n\t\t\tmutex_unlock(&ses->server->srv_mutex);\n\t\t\treturn rc;\n\t\t}\n\t}\n\tif (!ses->server->session_estab) {\n\t\tses->server->sequence_number = 0x2;\n\t\tses->server->session_estab = true;\n\t}\n\tmutex_unlock(&ses->server->srv_mutex);\n\n\tcifs_dbg(FYI, \"SMB2/3 session established successfully\\n\");\n\tspin_lock(&GlobalMid_Lock);\n\tses->status = CifsGood;\n\tses->need_reconnect = false;\n\tspin_unlock(&GlobalMid_Lock);\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_425": {
        "vulnerable_code": {
            "Code": "SMB2_sess_free_buffer(struct SMB2_sess_data *sess_data){\n\tfree_rsp_buf(sess_data->buf0_type, sess_data->iov[0].iov_base);\n\tsess_data->buf0_type = CIFS_NO_BUFFER;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_sess_free_buffer(struct SMB2_sess_data *sess_data){\n\tfree_rsp_buf(sess_data->buf0_type, sess_data->iov[0].iov_base);\n\tsess_data->buf0_type = CIFS_NO_BUFFER;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_426": {
        "vulnerable_code": {
            "Code": "SMB2_sess_sendreceive(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct smb_rqst rqst;\n\tstruct smb2_sess_setup_req *req = sess_data->iov[0].iov_base;\n\tstruct kvec rsp_iov = { NULL, 0 };\n\n\t/* Testing shows that buffer offset must be at location of Buffer[0] */\n\treq->SecurityBufferOffset =\n\t\tcpu_to_le16(sizeof(struct smb2_sess_setup_req) - 1 /* pad */);\n\treq->SecurityBufferLength = cpu_to_le16(sess_data->iov[1].iov_len);\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = sess_data->iov;\n\trqst.rq_nvec = 2;\n\n\t/* BB add code to build os and lm fields */\n\trc = cifs_send_recv(sess_data->xid, sess_data->ses,\n\t\t\t    &rqst,\n\t\t\t    &sess_data->buf0_type,\n\t\t\t    CIFS_LOG_ERROR | CIFS_NEG_OP, &rsp_iov);\n\tcifs_small_buf_release(sess_data->iov[0].iov_base);\n\tmemcpy(&sess_data->iov[0], &rsp_iov, sizeof(struct kvec));\n\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_sess_sendreceive(struct SMB2_sess_data *sess_data){\n\tint rc;\n\tstruct smb_rqst rqst;\n\tstruct smb2_sess_setup_req *req = sess_data->iov[0].iov_base;\n\tstruct kvec rsp_iov = { NULL, 0 };\n\n\t/* Testing shows that buffer offset must be at location of Buffer[0] */\n\treq->SecurityBufferOffset =\n\t\tcpu_to_le16(sizeof(struct smb2_sess_setup_req) - 1 /* pad */);\n\treq->SecurityBufferLength = cpu_to_le16(sess_data->iov[1].iov_len);\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = sess_data->iov;\n\trqst.rq_nvec = 2;\n\n\t/* BB add code to build os and lm fields */\n\trc = cifs_send_recv(sess_data->xid, sess_data->ses,\n\t\t\t    &rqst,\n\t\t\t    &sess_data->buf0_type,\n\t\t\t    CIFS_LOG_ERROR | CIFS_NEG_OP, &rsp_iov);\n\tcifs_small_buf_release(sess_data->iov[0].iov_base);\n\tmemcpy(&sess_data->iov[0], &rsp_iov, sizeof(struct kvec));\n\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_427": {
        "vulnerable_code": {
            "Code": "SMB2_set_compression(const unsigned int xid, struct cifs_tcon *tcon,\n\t\t     u64 persistent_fid, u64 volatile_fid){\n\tint rc;\n\tstruct  compress_ioctl fsctl_input;\n\tchar *ret_data = NULL;\n\n\tfsctl_input.CompressionState =\n\t\t\tcpu_to_le16(COMPRESSION_FORMAT_DEFAULT);\n\n\trc = SMB2_ioctl(xid, tcon, persistent_fid, volatile_fid,\n\t\t\tFSCTL_SET_COMPRESSION, true /* is_fsctl */,\n\t\t\t(char *)&fsctl_input /* data input */,\n\t\t\t2 /* in data len */, CIFSMaxBufSize /* max out data */,\n\t\t\t&ret_data /* out data */, NULL);\n\n\tcifs_dbg(FYI, \"set compression rc %d\\n\", rc);\n\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_set_compression(const unsigned int xid, struct cifs_tcon *tcon,\n\t\t     u64 persistent_fid, u64 volatile_fid){\n\tint rc;\n\tstruct  compress_ioctl fsctl_input;\n\tchar *ret_data = NULL;\n\n\tfsctl_input.CompressionState =\n\t\t\tcpu_to_le16(COMPRESSION_FORMAT_DEFAULT);\n\n\trc = SMB2_ioctl(xid, tcon, persistent_fid, volatile_fid,\n\t\t\tFSCTL_SET_COMPRESSION, true /* is_fsctl */,\n\t\t\t(char *)&fsctl_input /* data input */,\n\t\t\t2 /* in data len */, CIFSMaxBufSize /* max out data */,\n\t\t\t&ret_data /* out data */, NULL);\n\n\tcifs_dbg(FYI, \"set compression rc %d\\n\", rc);\n\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_428": {
        "vulnerable_code": {
            "Code": "__smb2_reconnect(const struct nls_table *nlsc,\n\t\t\t    struct cifs_tcon *tcon){\n\tint rc;\n\tstruct dfs_cache_tgt_list tl;\n\tstruct dfs_cache_tgt_iterator *it = NULL;\n\tchar *tree;\n\tconst char *tcp_host;\n\tsize_t tcp_host_len;\n\tconst char *dfs_host;\n\tsize_t dfs_host_len;\n\n\ttree = kzalloc(MAX_TREE_SIZE, GFP_KERNEL);\n\tif (!tree)\n\t\treturn -ENOMEM;\n\n\tif (tcon->ipc) {\n\t\tscnprintf(tree, MAX_TREE_SIZE, \"\\\\\\\\%s\\\\IPC$\",\n\t\t\t  tcon->ses->server->hostname);\n\t\trc = SMB2_tcon(0, tcon->ses, tree, tcon, nlsc);\n\t\tgoto out;\n\t}\n\n\tif (!tcon->dfs_path) {\n\t\trc = SMB2_tcon(0, tcon->ses, tcon->treeName, tcon, nlsc);\n\t\tgoto out;\n\t}\n\n\trc = dfs_cache_noreq_find(tcon->dfs_path + 1, NULL, &tl);\n\tif (rc)\n\t\tgoto out;\n\n\textract_unc_hostname(tcon->ses->server->hostname, &tcp_host,\n\t\t\t     &tcp_host_len);\n\n\tfor (it = dfs_cache_get_tgt_iterator(&tl); it;\n\t     it = dfs_cache_get_next_tgt(&tl, it)) {\n\t\tconst char *tgt = dfs_cache_get_tgt_name(it);\n\n\t\textract_unc_hostname(tgt, &dfs_host, &dfs_host_len);\n\n\t\tif (dfs_host_len != tcp_host_len\n\t\t    || strncasecmp(dfs_host, tcp_host, dfs_host_len) != 0) {\n\t\t\tcifs_dbg(FYI, \"%s: skipping %.*s, doesn't match %.*s\",\n\t\t\t\t __func__,\n\t\t\t\t (int)dfs_host_len, dfs_host,\n\t\t\t\t (int)tcp_host_len, tcp_host);\n\t\t\tcontinue;\n\t\t}\n\n\t\tscnprintf(tree, MAX_TREE_SIZE, \"\\\\%s\", tgt);\n\n\t\trc = SMB2_tcon(0, tcon->ses, tree, tcon, nlsc);\n\t\tif (!rc)\n\t\t\tbreak;\n\t\tif (rc == -EREMOTE)\n\t\t\tbreak;\n\t}\n\n\tif (!rc) {\n\t\tif (it)\n\t\t\trc = dfs_cache_noreq_update_tgthint(tcon->dfs_path + 1,\n\t\t\t\t\t\t\t    it);\n\t\telse\n\t\t\trc = -ENOENT;\n\t}\n\tdfs_cache_free_tgts(&tl);\nout:\n\tkfree(tree);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__smb2_reconnect(const struct nls_table *nlsc,\n\t\t\t    struct cifs_tcon *tcon){\n\tint rc;\n\tstruct dfs_cache_tgt_list tl;\n\tstruct dfs_cache_tgt_iterator *it = NULL;\n\tchar *tree;\n\tconst char *tcp_host;\n\tsize_t tcp_host_len;\n\tconst char *dfs_host;\n\tsize_t dfs_host_len;\n\n\ttree = kzalloc(MAX_TREE_SIZE, GFP_KERNEL);\n\tif (!tree)\n\t\treturn -ENOMEM;\n\n\tif (tcon->ipc) {\n\t\tscnprintf(tree, MAX_TREE_SIZE, \"\\\\\\\\%s\\\\IPC$\",\n\t\t\t  tcon->ses->server->hostname);\n\t\trc = SMB2_tcon(0, tcon->ses, tree, tcon, nlsc);\n\t\tgoto out;\n\t}\n\n\tif (!tcon->dfs_path) {\n\t\trc = SMB2_tcon(0, tcon->ses, tcon->treeName, tcon, nlsc);\n\t\tgoto out;\n\t}\n\n\trc = dfs_cache_noreq_find(tcon->dfs_path + 1, NULL, &tl);\n\tif (rc)\n\t\tgoto out;\n\n\textract_unc_hostname(tcon->ses->server->hostname, &tcp_host,\n\t\t\t     &tcp_host_len);\n\n\tfor (it = dfs_cache_get_tgt_iterator(&tl); it;\n\t     it = dfs_cache_get_next_tgt(&tl, it)) {\n\t\tconst char *tgt = dfs_cache_get_tgt_name(it);\n\n\t\textract_unc_hostname(tgt, &dfs_host, &dfs_host_len);\n\n\t\tif (dfs_host_len != tcp_host_len\n\t\t    || strncasecmp(dfs_host, tcp_host, dfs_host_len) != 0) {\n\t\t\tcifs_dbg(FYI, \"%s: skipping %.*s, doesn't match %.*s\",\n\t\t\t\t __func__,\n\t\t\t\t (int)dfs_host_len, dfs_host,\n\t\t\t\t (int)tcp_host_len, tcp_host);\n\t\t\tcontinue;\n\t\t}\n\n\t\tscnprintf(tree, MAX_TREE_SIZE, \"\\\\%s\", tgt);\n\n\t\trc = SMB2_tcon(0, tcon->ses, tree, tcon, nlsc);\n\t\tif (!rc)\n\t\t\tbreak;\n\t\tif (rc == -EREMOTE)\n\t\t\tbreak;\n\t}\n\n\tif (!rc) {\n\t\tif (it)\n\t\t\trc = dfs_cache_noreq_update_tgthint(tcon->dfs_path + 1,\n\t\t\t\t\t\t\t    it);\n\t\telse\n\t\t\trc = -ENOENT;\n\t}\n\tdfs_cache_free_tgts(&tl);\nout:\n\tkfree(tree);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_429": {
        "vulnerable_code": {
            "Code": "__smb2_reconnect(const struct nls_table *nlsc,\n\t\t\t    struct cifs_tcon *tcon){\n\tint rc;\n\tstruct dfs_cache_tgt_list tl;\n\tstruct dfs_cache_tgt_iterator *it = NULL;\n\tchar *tree;\n\tconst char *tcp_host;\n\tsize_t tcp_host_len;\n\tconst char *dfs_host;\n\tsize_t dfs_host_len;\n\n\ttree = kzalloc(MAX_TREE_SIZE, GFP_KERNEL);\n\tif (!tree)\n\t\treturn -ENOMEM;\n\n\tif (tcon->ipc) {\n\t\tscnprintf(tree, MAX_TREE_SIZE, \"\\\\\\\\%s\\\\IPC$\",\n\t\t\t  tcon->ses->server->hostname);\n\t\trc = SMB2_tcon(0, tcon->ses, tree, tcon, nlsc);\n\t\tgoto out;\n\t}\n\n\tif (!tcon->dfs_path) {\n\t\trc = SMB2_tcon(0, tcon->ses, tcon->treeName, tcon, nlsc);\n\t\tgoto out;\n\t}\n\n\trc = dfs_cache_noreq_find(tcon->dfs_path + 1, NULL, &tl);\n\tif (rc)\n\t\tgoto out;\n\n\textract_unc_hostname(tcon->ses->server->hostname, &tcp_host,\n\t\t\t     &tcp_host_len);\n\n\tfor (it = dfs_cache_get_tgt_iterator(&tl); it;\n\t     it = dfs_cache_get_next_tgt(&tl, it)) {\n\t\tconst char *tgt = dfs_cache_get_tgt_name(it);\n\n\t\textract_unc_hostname(tgt, &dfs_host, &dfs_host_len);\n\n\t\tif (dfs_host_len != tcp_host_len\n\t\t    || strncasecmp(dfs_host, tcp_host, dfs_host_len) != 0) {\n\t\t\tcifs_dbg(FYI, \"%s: skipping %.*s, doesn't match %.*s\",\n\t\t\t\t __func__,\n\t\t\t\t (int)dfs_host_len, dfs_host,\n\t\t\t\t (int)tcp_host_len, tcp_host);\n\t\t\tcontinue;\n\t\t}\n\n\t\tscnprintf(tree, MAX_TREE_SIZE, \"\\\\%s\", tgt);\n\n\t\trc = SMB2_tcon(0, tcon->ses, tree, tcon, nlsc);\n\t\tif (!rc)\n\t\t\tbreak;\n\t\tif (rc == -EREMOTE)\n\t\t\tbreak;\n\t}\n\n\tif (!rc) {\n\t\tif (it)\n\t\t\trc = dfs_cache_noreq_update_tgthint(tcon->dfs_path + 1,\n\t\t\t\t\t\t\t    it);\n\t\telse\n\t\t\trc = -ENOENT;\n\t}\n\tdfs_cache_free_tgts(&tl);\nout:\n\tkfree(tree);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__smb2_reconnect(const struct nls_table *nlsc,\n\t\t\t    struct cifs_tcon *tcon){\n\tint rc;\n\tstruct dfs_cache_tgt_list tl;\n\tstruct dfs_cache_tgt_iterator *it = NULL;\n\tchar *tree;\n\tconst char *tcp_host;\n\tsize_t tcp_host_len;\n\tconst char *dfs_host;\n\tsize_t dfs_host_len;\n\n\ttree = kzalloc(MAX_TREE_SIZE, GFP_KERNEL);\n\tif (!tree)\n\t\treturn -ENOMEM;\n\n\tif (tcon->ipc) {\n\t\tscnprintf(tree, MAX_TREE_SIZE, \"\\\\\\\\%s\\\\IPC$\",\n\t\t\t  tcon->ses->server->hostname);\n\t\trc = SMB2_tcon(0, tcon->ses, tree, tcon, nlsc);\n\t\tgoto out;\n\t}\n\n\tif (!tcon->dfs_path) {\n\t\trc = SMB2_tcon(0, tcon->ses, tcon->treeName, tcon, nlsc);\n\t\tgoto out;\n\t}\n\n\trc = dfs_cache_noreq_find(tcon->dfs_path + 1, NULL, &tl);\n\tif (rc)\n\t\tgoto out;\n\n\textract_unc_hostname(tcon->ses->server->hostname, &tcp_host,\n\t\t\t     &tcp_host_len);\n\n\tfor (it = dfs_cache_get_tgt_iterator(&tl); it;\n\t     it = dfs_cache_get_next_tgt(&tl, it)) {\n\t\tconst char *tgt = dfs_cache_get_tgt_name(it);\n\n\t\textract_unc_hostname(tgt, &dfs_host, &dfs_host_len);\n\n\t\tif (dfs_host_len != tcp_host_len\n\t\t    || strncasecmp(dfs_host, tcp_host, dfs_host_len) != 0) {\n\t\t\tcifs_dbg(FYI, \"%s: skipping %.*s, doesn't match %.*s\",\n\t\t\t\t __func__,\n\t\t\t\t (int)dfs_host_len, dfs_host,\n\t\t\t\t (int)tcp_host_len, tcp_host);\n\t\t\tcontinue;\n\t\t}\n\n\t\tscnprintf(tree, MAX_TREE_SIZE, \"\\\\%s\", tgt);\n\n\t\trc = SMB2_tcon(0, tcon->ses, tree, tcon, nlsc);\n\t\tif (!rc)\n\t\t\tbreak;\n\t\tif (rc == -EREMOTE)\n\t\t\tbreak;\n\t}\n\n\tif (!rc) {\n\t\tif (it)\n\t\t\trc = dfs_cache_noreq_update_tgthint(tcon->dfs_path + 1,\n\t\t\t\t\t\t\t    it);\n\t\telse\n\t\t\trc = -ENOENT;\n\t}\n\tdfs_cache_free_tgts(&tl);\nout:\n\tkfree(tree);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_430": {
        "vulnerable_code": {
            "Code": "add_posix_context(struct kvec *iov, unsigned int *num_iovec, umode_t mode){\n\tstruct smb2_create_req *req = iov[0].iov_base;\n\tunsigned int num = *num_iovec;\n\n\tiov[num].iov_base = create_posix_buf(mode);\n\tif (iov[num].iov_base == NULL)\n\t\treturn -ENOMEM;\n\tiov[num].iov_len = sizeof(struct create_posix);\n\tif (!req->CreateContextsOffset)\n\t\treq->CreateContextsOffset = cpu_to_le32(\n\t\t\t\tsizeof(struct smb2_create_req) +\n\t\t\t\tiov[num - 1].iov_len);\n\tle32_add_cpu(&req->CreateContextsLength, sizeof(struct create_posix));\n\t*num_iovec = num + 1;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "add_posix_context(struct kvec *iov, unsigned int *num_iovec, umode_t mode){\n\tstruct smb2_create_req *req = iov[0].iov_base;\n\tunsigned int num = *num_iovec;\n\n\tiov[num].iov_base = create_posix_buf(mode);\n\tif (iov[num].iov_base == NULL)\n\t\treturn -ENOMEM;\n\tiov[num].iov_len = sizeof(struct create_posix);\n\tif (!req->CreateContextsOffset)\n\t\treq->CreateContextsOffset = cpu_to_le32(\n\t\t\t\tsizeof(struct smb2_create_req) +\n\t\t\t\tiov[num - 1].iov_len);\n\tle32_add_cpu(&req->CreateContextsLength, sizeof(struct create_posix));\n\t*num_iovec = num + 1;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_431": {
        "vulnerable_code": {
            "Code": "create_reconnect_durable_v2_buf(struct cifs_fid *fid){\n\tstruct create_durable_handle_reconnect_v2 *buf;\n\n\tbuf = kzalloc(sizeof(struct create_durable_handle_reconnect_v2),\n\t\t\tGFP_KERNEL);\n\tif (!buf)\n\t\treturn NULL;\n\n\tbuf->ccontext.DataOffset =\n\t\tcpu_to_le16(offsetof(struct create_durable_handle_reconnect_v2,\n\t\t\t\t     dcontext));\n\tbuf->ccontext.DataLength =\n\t\tcpu_to_le32(sizeof(struct durable_reconnect_context_v2));\n\tbuf->ccontext.NameOffset =\n\t\tcpu_to_le16(offsetof(struct create_durable_handle_reconnect_v2,\n\t\t\t    Name));\n\tbuf->ccontext.NameLength = cpu_to_le16(4);\n\n\tbuf->dcontext.Fid.PersistentFileId = fid->persistent_fid;\n\tbuf->dcontext.Fid.VolatileFileId = fid->volatile_fid;\n\tbuf->dcontext.Flags = cpu_to_le32(SMB2_DHANDLE_FLAG_PERSISTENT);\n\tmemcpy(buf->dcontext.CreateGuid, fid->create_guid, 16);\n\n\t/* SMB2_CREATE_DURABLE_HANDLE_RECONNECT_V2 is \"DH2C\" */\n\tbuf->Name[0] = 'D';\n\tbuf->Name[1] = 'H';\n\tbuf->Name[2] = '2';\n\tbuf->Name[3] = 'C';\n\treturn buf;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "create_reconnect_durable_v2_buf(struct cifs_fid *fid){\n\tstruct create_durable_handle_reconnect_v2 *buf;\n\n\tbuf = kzalloc(sizeof(struct create_durable_handle_reconnect_v2),\n\t\t\tGFP_KERNEL);\n\tif (!buf)\n\t\treturn NULL;\n\n\tbuf->ccontext.DataOffset =\n\t\tcpu_to_le16(offsetof(struct create_durable_handle_reconnect_v2,\n\t\t\t\t     dcontext));\n\tbuf->ccontext.DataLength =\n\t\tcpu_to_le32(sizeof(struct durable_reconnect_context_v2));\n\tbuf->ccontext.NameOffset =\n\t\tcpu_to_le16(offsetof(struct create_durable_handle_reconnect_v2,\n\t\t\t    Name));\n\tbuf->ccontext.NameLength = cpu_to_le16(4);\n\n\tbuf->dcontext.Fid.PersistentFileId = fid->persistent_fid;\n\tbuf->dcontext.Fid.VolatileFileId = fid->volatile_fid;\n\tbuf->dcontext.Flags = cpu_to_le32(SMB2_DHANDLE_FLAG_PERSISTENT);\n\tmemcpy(buf->dcontext.CreateGuid, fid->create_guid, 16);\n\n\t/* SMB2_CREATE_DURABLE_HANDLE_RECONNECT_V2 is \"DH2C\" */\n\tbuf->Name[0] = 'D';\n\tbuf->Name[1] = 'H';\n\tbuf->Name[2] = '2';\n\tbuf->Name[3] = 'C';\n\treturn buf;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_432": {
        "vulnerable_code": {
            "Code": "create_twarp_buf(__u64 timewarp){\n\tstruct crt_twarp_ctxt *buf;\n\n\tbuf = kzalloc(sizeof(struct crt_twarp_ctxt), GFP_KERNEL);\n\tif (!buf)\n\t\treturn NULL;\n\n\tbuf->ccontext.DataOffset = cpu_to_le16(offsetof\n\t\t\t\t\t(struct crt_twarp_ctxt, Timestamp));\n\tbuf->ccontext.DataLength = cpu_to_le32(8);\n\tbuf->ccontext.NameOffset = cpu_to_le16(offsetof\n\t\t\t\t(struct crt_twarp_ctxt, Name));\n\tbuf->ccontext.NameLength = cpu_to_le16(4);\n\t/* SMB2_CREATE_TIMEWARP_TOKEN is \"TWrp\" */\n\tbuf->Name[0] = 'T';\n\tbuf->Name[1] = 'W';\n\tbuf->Name[2] = 'r';\n\tbuf->Name[3] = 'p';\n\tbuf->Timestamp = cpu_to_le64(timewarp);\n\treturn buf;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "create_twarp_buf(__u64 timewarp){\n\tstruct crt_twarp_ctxt *buf;\n\n\tbuf = kzalloc(sizeof(struct crt_twarp_ctxt), GFP_KERNEL);\n\tif (!buf)\n\t\treturn NULL;\n\n\tbuf->ccontext.DataOffset = cpu_to_le16(offsetof\n\t\t\t\t\t(struct crt_twarp_ctxt, Timestamp));\n\tbuf->ccontext.DataLength = cpu_to_le32(8);\n\tbuf->ccontext.NameOffset = cpu_to_le16(offsetof\n\t\t\t\t(struct crt_twarp_ctxt, Name));\n\tbuf->ccontext.NameLength = cpu_to_le16(4);\n\t/* SMB2_CREATE_TIMEWARP_TOKEN is \"TWrp\" */\n\tbuf->Name[0] = 'T';\n\tbuf->Name[1] = 'W';\n\tbuf->Name[2] = 'r';\n\tbuf->Name[3] = 'p';\n\tbuf->Timestamp = cpu_to_le64(timewarp);\n\treturn buf;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_433": {
        "vulnerable_code": {
            "Code": "init_copy_chunk_defaults(struct cifs_tcon *tcon){\n\ttcon->max_chunks = 256;\n\ttcon->max_bytes_chunk = 1048576;\n\ttcon->max_bytes_copy = 16777216;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "init_copy_chunk_defaults(struct cifs_tcon *tcon){\n\ttcon->max_chunks = 256;\n\ttcon->max_bytes_chunk = 1048576;\n\ttcon->max_bytes_copy = 16777216;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_434": {
        "vulnerable_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": "null",
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "None",
            "Size": "null",
            "Code Complexity": "null",
            "Memory Management": "null",
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_435": {
        "vulnerable_code": {
            "Code": "smb2_echo_callback(struct mid_q_entry *mid){\n\tstruct TCP_Server_Info *server = mid->callback_data;\n\tstruct smb2_echo_rsp *rsp = (struct smb2_echo_rsp *)mid->resp_buf;\n\tstruct cifs_credits credits = { .value = 0, .instance = 0 };\n\n\tif (mid->mid_state == MID_RESPONSE_RECEIVED\n\t    || mid->mid_state == MID_RESPONSE_MALFORMED) {\n\t\tcredits.value = le16_to_cpu(rsp->sync_hdr.CreditRequest);\n\t\tcredits.instance = server->reconnect_instance;\n\t}\n\n\tDeleteMidQEntry(mid);\n\tadd_credits(server, &credits, CIFS_ECHO_OP);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smb2_echo_callback(struct mid_q_entry *mid){\n\tstruct TCP_Server_Info *server = mid->callback_data;\n\tstruct smb2_echo_rsp *rsp = (struct smb2_echo_rsp *)mid->resp_buf;\n\tstruct cifs_credits credits = { .value = 0, .instance = 0 };\n\n\tif (mid->mid_state == MID_RESPONSE_RECEIVED\n\t    || mid->mid_state == MID_RESPONSE_MALFORMED) {\n\t\tcredits.value = le16_to_cpu(rsp->sync_hdr.CreditRequest);\n\t\tcredits.instance = server->reconnect_instance;\n\t}\n\n\tDeleteMidQEntry(mid);\n\tadd_credits(server, &credits, CIFS_ECHO_OP);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_436": {
        "vulnerable_code": {
            "Code": "smb2_hdr_assemble(struct smb2_sync_hdr *shdr, __le16 smb2_cmd,\n\t\t  const struct cifs_tcon *tcon){\n\tshdr->ProtocolId = SMB2_PROTO_NUMBER;\n\tshdr->StructureSize = cpu_to_le16(64);\n\tshdr->Command = smb2_cmd;\n\tif (tcon && tcon->ses && tcon->ses->server) {\n\t\tstruct TCP_Server_Info *server = tcon->ses->server;\n\n\t\tspin_lock(&server->req_lock);\n\t\t/* Request up to 10 credits but don't go over the limit. */\n\t\tif (server->credits >= server->max_credits)\n\t\t\tshdr->CreditRequest = cpu_to_le16(0);\n\t\telse\n\t\t\tshdr->CreditRequest = cpu_to_le16(\n\t\t\t\tmin_t(int, server->max_credits -\n\t\t\t\t\t\tserver->credits, 10));\n\t\tspin_unlock(&server->req_lock);\n\t} else {\n\t\tshdr->CreditRequest = cpu_to_le16(2);\n\t}\n\tshdr->ProcessId = cpu_to_le32((__u16)current->tgid);\n\n\tif (!tcon)\n\t\tgoto out;\n\n\t/* GLOBAL_CAP_LARGE_MTU will only be set if dialect > SMB2.02 */\n\t/* See sections 2.2.4 and 3.2.4.1.5 of MS-SMB2 */\n\tif ((tcon->ses) && (tcon->ses->server) &&\n\t    (tcon->ses->server->capabilities & SMB2_GLOBAL_CAP_LARGE_MTU))\n\t\tshdr->CreditCharge = cpu_to_le16(1);\n\t/* else CreditCharge MBZ */\n\n\tshdr->TreeId = tcon->tid;\n\t/* Uid is not converted */\n\tif (tcon->ses)\n\t\tshdr->SessionId = tcon->ses->Suid;\n\n\t/*\n\t * If we would set SMB2_FLAGS_DFS_OPERATIONS on open we also would have\n\t * to pass the path on the Open SMB prefixed by \\\\server\\share.\n\t * Not sure when we would need to do the augmented path (if ever) and\n\t * setting this flag breaks the SMB2 open operation since it is\n\t * illegal to send an empty path name (without \\\\server\\share prefix)\n\t * when the DFS flag is set in the SMB open header. We could\n\t * consider setting the flag on all operations other than open\n\t * but it is safer to net set it for now.\n\t */\n/*\tif (tcon->share_flags & SHI1005_FLAGS_DFS)\n\t\tshdr->Flags |= SMB2_FLAGS_DFS_OPERATIONS; */\n\n\tif (tcon->ses && tcon->ses->server && tcon->ses->server->sign &&\n\t    !smb3_encryption_required(tcon))\n\t\tshdr->Flags |= SMB2_FLAGS_SIGNED;\nout:\n\treturn;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smb2_hdr_assemble(struct smb2_sync_hdr *shdr, __le16 smb2_cmd,\n\t\t  const struct cifs_tcon *tcon){\n\tshdr->ProtocolId = SMB2_PROTO_NUMBER;\n\tshdr->StructureSize = cpu_to_le16(64);\n\tshdr->Command = smb2_cmd;\n\tif (tcon && tcon->ses && tcon->ses->server) {\n\t\tstruct TCP_Server_Info *server = tcon->ses->server;\n\n\t\tspin_lock(&server->req_lock);\n\t\t/* Request up to 10 credits but don't go over the limit. */\n\t\tif (server->credits >= server->max_credits)\n\t\t\tshdr->CreditRequest = cpu_to_le16(0);\n\t\telse\n\t\t\tshdr->CreditRequest = cpu_to_le16(\n\t\t\t\tmin_t(int, server->max_credits -\n\t\t\t\t\t\tserver->credits, 10));\n\t\tspin_unlock(&server->req_lock);\n\t} else {\n\t\tshdr->CreditRequest = cpu_to_le16(2);\n\t}\n\tshdr->ProcessId = cpu_to_le32((__u16)current->tgid);\n\n\tif (!tcon)\n\t\tgoto out;\n\n\t/* GLOBAL_CAP_LARGE_MTU will only be set if dialect > SMB2.02 */\n\t/* See sections 2.2.4 and 3.2.4.1.5 of MS-SMB2 */\n\tif ((tcon->ses) && (tcon->ses->server) &&\n\t    (tcon->ses->server->capabilities & SMB2_GLOBAL_CAP_LARGE_MTU))\n\t\tshdr->CreditCharge = cpu_to_le16(1);\n\t/* else CreditCharge MBZ */\n\n\tshdr->TreeId = tcon->tid;\n\t/* Uid is not converted */\n\tif (tcon->ses)\n\t\tshdr->SessionId = tcon->ses->Suid;\n\n\t/*\n\t * If we would set SMB2_FLAGS_DFS_OPERATIONS on open we also would have\n\t * to pass the path on the Open SMB prefixed by \\\\server\\share.\n\t * Not sure when we would need to do the augmented path (if ever) and\n\t * setting this flag breaks the SMB2 open operation since it is\n\t * illegal to send an empty path name (without \\\\server\\share prefix)\n\t * when the DFS flag is set in the SMB open header. We could\n\t * consider setting the flag on all operations other than open\n\t * but it is safer to net set it for now.\n\t */\n/*\tif (tcon->share_flags & SHI1005_FLAGS_DFS)\n\t\tshdr->Flags |= SMB2_FLAGS_DFS_OPERATIONS; */\n\n\tif (tcon->ses && tcon->ses->server && tcon->ses->server->sign &&\n\t    !smb3_encryption_required(tcon))\n\t\tshdr->Flags |= SMB2_FLAGS_SIGNED;\nout:\n\treturn;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_437": {
        "vulnerable_code": {
            "Code": "smb2_readv_callback(struct mid_q_entry *mid){\n\tstruct cifs_readdata *rdata = mid->callback_data;\n\tstruct cifs_tcon *tcon = tlink_tcon(rdata->cfile->tlink);\n\tstruct TCP_Server_Info *server = tcon->ses->server;\n\tstruct smb2_sync_hdr *shdr =\n\t\t\t\t(struct smb2_sync_hdr *)rdata->iov[0].iov_base;\n\tstruct cifs_credits credits = { .value = 0, .instance = 0 };\n\tstruct smb_rqst rqst = { .rq_iov = rdata->iov,\n\t\t\t\t .rq_nvec = 2,\n\t\t\t\t .rq_pages = rdata->pages,\n\t\t\t\t .rq_offset = rdata->page_offset,\n\t\t\t\t .rq_npages = rdata->nr_pages,\n\t\t\t\t .rq_pagesz = rdata->pagesz,\n\t\t\t\t .rq_tailsz = rdata->tailsz };\n\n\tcifs_dbg(FYI, \"%s: mid=%llu state=%d result=%d bytes=%u\\n\",\n\t\t __func__, mid->mid, mid->mid_state, rdata->result,\n\t\t rdata->bytes);\n\n\tswitch (mid->mid_state) {\n\tcase MID_RESPONSE_RECEIVED:\n\t\tcredits.value = le16_to_cpu(shdr->CreditRequest);\n\t\tcredits.instance = server->reconnect_instance;\n\t\t/* result already set, check signature */\n\t\tif (server->sign && !mid->decrypted) {\n\t\t\tint rc;\n\n\t\t\trc = smb2_verify_signature(&rqst, server);\n\t\t\tif (rc)\n\t\t\t\tcifs_dbg(VFS, \"SMB signature verification returned error = %d\\n\",\n\t\t\t\t\t rc);\n\t\t}\n\t\t/* FIXME: should this be counted toward the initiating task? */\n\t\ttask_io_account_read(rdata->got_bytes);\n\t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n\t\tbreak;\n\tcase MID_REQUEST_SUBMITTED:\n\tcase MID_RETRY_NEEDED:\n\t\trdata->result = -EAGAIN;\n\t\tif (server->sign && rdata->got_bytes)\n\t\t\t/* reset bytes number since we can not check a sign */\n\t\t\trdata->got_bytes = 0;\n\t\t/* FIXME: should this be counted toward the initiating task? */\n\t\ttask_io_account_read(rdata->got_bytes);\n\t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n\t\tbreak;\n\tcase MID_RESPONSE_MALFORMED:\n\t\tcredits.value = le16_to_cpu(shdr->CreditRequest);\n\t\tcredits.instance = server->reconnect_instance;\n\t\t/* fall through */\n\tdefault:\n\t\trdata->result = -EIO;\n\t}\n#ifdef CONFIG_CIFS_SMB_DIRECT\n\t/*\n\t * If this rdata has a memmory registered, the MR can be freed\n\t * MR needs to be freed as soon as I/O finishes to prevent deadlock\n\t * because they have limited number and are used for future I/Os\n\t */\n\tif (rdata->mr) {\n\t\tsmbd_deregister_mr(rdata->mr);\n\t\trdata->mr = NULL;\n\t}\n#endif\n\tif (rdata->result && rdata->result != -ENODATA) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_READ_HE);\n\t\ttrace_smb3_read_err(0 /* xid */,\n\t\t\t\t    rdata->cfile->fid.persistent_fid,\n\t\t\t\t    tcon->tid, tcon->ses->Suid, rdata->offset,\n\t\t\t\t    rdata->bytes, rdata->result);\n\t} else\n\t\ttrace_smb3_read_done(0 /* xid */,\n\t\t\t\t     rdata->cfile->fid.persistent_fid,\n\t\t\t\t     tcon->tid, tcon->ses->Suid,\n\t\t\t\t     rdata->offset, rdata->got_bytes);\n\n\tqueue_work(cifsiod_wq, &rdata->work);\n\tDeleteMidQEntry(mid);\n\tadd_credits(server, &credits, 0);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smb2_readv_callback(struct mid_q_entry *mid){\n\tstruct cifs_readdata *rdata = mid->callback_data;\n\tstruct cifs_tcon *tcon = tlink_tcon(rdata->cfile->tlink);\n\tstruct TCP_Server_Info *server = tcon->ses->server;\n\tstruct smb2_sync_hdr *shdr =\n\t\t\t\t(struct smb2_sync_hdr *)rdata->iov[0].iov_base;\n\tstruct cifs_credits credits = { .value = 0, .instance = 0 };\n\tstruct smb_rqst rqst = { .rq_iov = rdata->iov,\n\t\t\t\t .rq_nvec = 2,\n\t\t\t\t .rq_pages = rdata->pages,\n\t\t\t\t .rq_offset = rdata->page_offset,\n\t\t\t\t .rq_npages = rdata->nr_pages,\n\t\t\t\t .rq_pagesz = rdata->pagesz,\n\t\t\t\t .rq_tailsz = rdata->tailsz };\n\n\tcifs_dbg(FYI, \"%s: mid=%llu state=%d result=%d bytes=%u\\n\",\n\t\t __func__, mid->mid, mid->mid_state, rdata->result,\n\t\t rdata->bytes);\n\n\tswitch (mid->mid_state) {\n\tcase MID_RESPONSE_RECEIVED:\n\t\tcredits.value = le16_to_cpu(shdr->CreditRequest);\n\t\tcredits.instance = server->reconnect_instance;\n\t\t/* result already set, check signature */\n\t\tif (server->sign && !mid->decrypted) {\n\t\t\tint rc;\n\n\t\t\trc = smb2_verify_signature(&rqst, server);\n\t\t\tif (rc)\n\t\t\t\tcifs_dbg(VFS, \"SMB signature verification returned error = %d\\n\",\n\t\t\t\t\t rc);\n\t\t}\n\t\t/* FIXME: should this be counted toward the initiating task? */\n\t\ttask_io_account_read(rdata->got_bytes);\n\t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n\t\tbreak;\n\tcase MID_REQUEST_SUBMITTED:\n\tcase MID_RETRY_NEEDED:\n\t\trdata->result = -EAGAIN;\n\t\tif (server->sign && rdata->got_bytes)\n\t\t\t/* reset bytes number since we can not check a sign */\n\t\t\trdata->got_bytes = 0;\n\t\t/* FIXME: should this be counted toward the initiating task? */\n\t\ttask_io_account_read(rdata->got_bytes);\n\t\tcifs_stats_bytes_read(tcon, rdata->got_bytes);\n\t\tbreak;\n\tcase MID_RESPONSE_MALFORMED:\n\t\tcredits.value = le16_to_cpu(shdr->CreditRequest);\n\t\tcredits.instance = server->reconnect_instance;\n\t\t/* fall through */\n\tdefault:\n\t\trdata->result = -EIO;\n\t}\n#ifdef CONFIG_CIFS_SMB_DIRECT\n\t/*\n\t * If this rdata has a memmory registered, the MR can be freed\n\t * MR needs to be freed as soon as I/O finishes to prevent deadlock\n\t * because they have limited number and are used for future I/Os\n\t */\n\tif (rdata->mr) {\n\t\tsmbd_deregister_mr(rdata->mr);\n\t\trdata->mr = NULL;\n\t}\n#endif\n\tif (rdata->result && rdata->result != -ENODATA) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_READ_HE);\n\t\ttrace_smb3_read_err(0 /* xid */,\n\t\t\t\t    rdata->cfile->fid.persistent_fid,\n\t\t\t\t    tcon->tid, tcon->ses->Suid, rdata->offset,\n\t\t\t\t    rdata->bytes, rdata->result);\n\t} else\n\t\ttrace_smb3_read_done(0 /* xid */,\n\t\t\t\t     rdata->cfile->fid.persistent_fid,\n\t\t\t\t     tcon->tid, tcon->ses->Suid,\n\t\t\t\t     rdata->offset, rdata->got_bytes);\n\n\tqueue_work(cifsiod_wq, &rdata->work);\n\tDeleteMidQEntry(mid);\n\tadd_credits(server, &credits, 0);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_438": {
        "vulnerable_code": {
            "Code": "smb311_decode_neg_context(struct smb2_negotiate_rsp *rsp,\n\t\t\t\t     struct TCP_Server_Info *server,\n\t\t\t\t     unsigned int len_of_smb){\n\tstruct smb2_neg_context *pctx;\n\tunsigned int offset = le32_to_cpu(rsp->NegotiateContextOffset);\n\tunsigned int ctxt_cnt = le16_to_cpu(rsp->NegotiateContextCount);\n\tunsigned int len_of_ctxts, i;\n\tint rc = 0;\n\n\tcifs_dbg(FYI, \"decoding %d negotiate contexts\\n\", ctxt_cnt);\n\tif (len_of_smb <= offset) {\n\t\tcifs_dbg(VFS, \"Invalid response: negotiate context offset\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tlen_of_ctxts = len_of_smb - offset;\n\n\tfor (i = 0; i < ctxt_cnt; i++) {\n\t\tint clen;\n\t\t/* check that offset is not beyond end of SMB */\n\t\tif (len_of_ctxts == 0)\n\t\t\tbreak;\n\n\t\tif (len_of_ctxts < sizeof(struct smb2_neg_context))\n\t\t\tbreak;\n\n\t\tpctx = (struct smb2_neg_context *)(offset + (char *)rsp);\n\t\tclen = le16_to_cpu(pctx->DataLength);\n\t\tif (clen > len_of_ctxts)\n\t\t\tbreak;\n\n\t\tif (pctx->ContextType == SMB2_PREAUTH_INTEGRITY_CAPABILITIES)\n\t\t\tdecode_preauth_context(\n\t\t\t\t(struct smb2_preauth_neg_context *)pctx);\n\t\telse if (pctx->ContextType == SMB2_ENCRYPTION_CAPABILITIES)\n\t\t\trc = decode_encrypt_ctx(server,\n\t\t\t\t(struct smb2_encryption_neg_context *)pctx);\n\t\telse if (pctx->ContextType == SMB2_POSIX_EXTENSIONS_AVAILABLE)\n\t\t\tserver->posix_ext_supported = true;\n\t\telse\n\t\t\tcifs_dbg(VFS, \"unknown negcontext of type %d ignored\\n\",\n\t\t\t\tle16_to_cpu(pctx->ContextType));\n\n\t\tif (rc)\n\t\t\tbreak;\n\t\t/* offsets must be 8 byte aligned */\n\t\tclen = (clen + 7) & ~0x7;\n\t\toffset += clen + sizeof(struct smb2_neg_context);\n\t\tlen_of_ctxts -= clen;\n\t}\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smb311_decode_neg_context(struct smb2_negotiate_rsp *rsp,\n\t\t\t\t     struct TCP_Server_Info *server,\n\t\t\t\t     unsigned int len_of_smb){\n\tstruct smb2_neg_context *pctx;\n\tunsigned int offset = le32_to_cpu(rsp->NegotiateContextOffset);\n\tunsigned int ctxt_cnt = le16_to_cpu(rsp->NegotiateContextCount);\n\tunsigned int len_of_ctxts, i;\n\tint rc = 0;\n\n\tcifs_dbg(FYI, \"decoding %d negotiate contexts\\n\", ctxt_cnt);\n\tif (len_of_smb <= offset) {\n\t\tcifs_dbg(VFS, \"Invalid response: negotiate context offset\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tlen_of_ctxts = len_of_smb - offset;\n\n\tfor (i = 0; i < ctxt_cnt; i++) {\n\t\tint clen;\n\t\t/* check that offset is not beyond end of SMB */\n\t\tif (len_of_ctxts == 0)\n\t\t\tbreak;\n\n\t\tif (len_of_ctxts < sizeof(struct smb2_neg_context))\n\t\t\tbreak;\n\n\t\tpctx = (struct smb2_neg_context *)(offset + (char *)rsp);\n\t\tclen = le16_to_cpu(pctx->DataLength);\n\t\tif (clen > len_of_ctxts)\n\t\t\tbreak;\n\n\t\tif (pctx->ContextType == SMB2_PREAUTH_INTEGRITY_CAPABILITIES)\n\t\t\tdecode_preauth_context(\n\t\t\t\t(struct smb2_preauth_neg_context *)pctx);\n\t\telse if (pctx->ContextType == SMB2_ENCRYPTION_CAPABILITIES)\n\t\t\trc = decode_encrypt_ctx(server,\n\t\t\t\t(struct smb2_encryption_neg_context *)pctx);\n\t\telse if (pctx->ContextType == SMB2_POSIX_EXTENSIONS_AVAILABLE)\n\t\t\tserver->posix_ext_supported = true;\n\t\telse\n\t\t\tcifs_dbg(VFS, \"unknown negcontext of type %d ignored\\n\",\n\t\t\t\tle16_to_cpu(pctx->ContextType));\n\n\t\tif (rc)\n\t\t\tbreak;\n\t\t/* offsets must be 8 byte aligned */\n\t\tclen = (clen + 7) & ~0x7;\n\t\toffset += clen + sizeof(struct smb2_neg_context);\n\t\tlen_of_ctxts -= clen;\n\t}\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_439": {
        "vulnerable_code": {
            "Code": "SMB2_close_free(struct smb_rqst *rqst){\n\tif (rqst && rqst->rq_iov)\n\t\tcifs_small_buf_release(rqst->rq_iov[0].iov_base); /* request */\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_close_free(struct smb_rqst *rqst){\n\tif (rqst && rqst->rq_iov)\n\t\tcifs_small_buf_release(rqst->rq_iov[0].iov_base); /* request */\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_440": {
        "vulnerable_code": {
            "Code": "SMB2_close_init(struct cifs_tcon *tcon, struct smb_rqst *rqst,\n\t\tu64 persistent_fid, u64 volatile_fid){\n\tstruct smb2_close_req *req;\n\tstruct kvec *iov = rqst->rq_iov;\n\tunsigned int total_len;\n\tint rc;\n\n\trc = smb2_plain_req_init(SMB2_CLOSE, tcon, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_close_init(struct cifs_tcon *tcon, struct smb_rqst *rqst,\n\t\tu64 persistent_fid, u64 volatile_fid){\n\tstruct smb2_close_req *req;\n\tstruct kvec *iov = rqst->rq_iov;\n\tunsigned int total_len;\n\tint rc;\n\n\trc = smb2_plain_req_init(SMB2_CLOSE, tcon, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\treq->PersistentFileId = persistent_fid;\n\treq->VolatileFileId = volatile_fid;\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_441": {
        "vulnerable_code": {
            "Code": "SMB2_get_srv_num(const unsigned int xid, struct cifs_tcon *tcon,\n\t\t u64 persistent_fid, u64 volatile_fid, __le64 *uniqueid){\n\treturn query_info(xid, tcon, persistent_fid, volatile_fid,\n\t\t\t  FILE_INTERNAL_INFORMATION, SMB2_O_INFO_FILE, 0,\n\t\t\t  sizeof(struct smb2_file_internal_info),\n\t\t\t  sizeof(struct smb2_file_internal_info),\n\t\t\t  (void **)&uniqueid, NULL);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_get_srv_num(const unsigned int xid, struct cifs_tcon *tcon,\n\t\t u64 persistent_fid, u64 volatile_fid, __le64 *uniqueid){\n\treturn query_info(xid, tcon, persistent_fid, volatile_fid,\n\t\t\t  FILE_INTERNAL_INFORMATION, SMB2_O_INFO_FILE, 0,\n\t\t\t  sizeof(struct smb2_file_internal_info),\n\t\t\t  sizeof(struct smb2_file_internal_info),\n\t\t\t  (void **)&uniqueid, NULL);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_442": {
        "vulnerable_code": {
            "Code": "SMB2_ioctl(const unsigned int xid, struct cifs_tcon *tcon, u64 persistent_fid,\n\t   u64 volatile_fid, u32 opcode, bool is_fsctl,\n\t   char *in_data, u32 indatalen, u32 max_out_data_len,\n\t   char **out_data, u32 *plen /* returned data len */){\n\tstruct smb_rqst rqst;\n\tstruct smb2_ioctl_rsp *rsp = NULL;\n\tstruct cifs_ses *ses;\n\tstruct kvec iov[SMB2_IOCTL_IOV_SIZE];\n\tstruct kvec rsp_iov = {NULL, 0};\n\tint resp_buftype = CIFS_NO_BUFFER;\n\tint rc = 0;\n\tint flags = 0;\n\n\tcifs_dbg(FYI, \"SMB2 IOCTL\\n\");\n\n\tif (out_data != NULL)\n\t\t*out_data = NULL;\n\n\t/* zero out returned data len, in case of error */\n\tif (plen)\n\t\t*plen = 0;\n\n\tif (tcon)\n\t\tses = tcon->ses;\n\telse\n\t\treturn -EIO;\n\n\tif (!ses || !(ses->server))\n\t\treturn -EIO;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\tmemset(&iov, 0, sizeof(iov));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = SMB2_IOCTL_IOV_SIZE;\n\n\trc = SMB2_ioctl_init(tcon, &rqst, persistent_fid, volatile_fid, opcode,\n\t\t\t     is_fsctl, in_data, indatalen, max_out_data_len);\n\tif (rc)\n\t\tgoto ioctl_exit;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags,\n\t\t\t    &rsp_iov);\n\trsp = (struct smb2_ioctl_rsp *)rsp_iov.iov_base;\n\n\tif (rc != 0)\n\t\ttrace_smb3_fsctl_err(xid, persistent_fid, tcon->tid,\n\t\t\t\tses->Suid, 0, opcode, rc);\n\n\tif ((rc != 0) && (rc != -EINVAL)) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\tgoto ioctl_exit;\n\t} else if (rc == -EINVAL) {\n\t\tif ((opcode != FSCTL_SRV_COPYCHUNK_WRITE) &&\n\t\t    (opcode != FSCTL_SRV_COPYCHUNK)) {\n\t\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\t\tgoto ioctl_exit;\n\t\t}\n\t}\n\n\t/* check if caller wants to look at return data or just return rc */\n\tif ((plen == NULL) || (out_data == NULL))\n\t\tgoto ioctl_exit;\n\n\t*plen = le32_to_cpu(rsp->OutputCount);\n\n\t/* We check for obvious errors in the output buffer length and offset */\n\tif (*plen == 0)\n\t\tgoto ioctl_exit; /* server returned no data */\n\telse if (*plen > rsp_iov.iov_len || *plen > 0xFF00) {\n\t\tcifs_dbg(VFS, \"srv returned invalid ioctl length: %d\\n\", *plen);\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\tif (rsp_iov.iov_len - *plen < le32_to_cpu(rsp->OutputOffset)) {\n\t\tcifs_dbg(VFS, \"Malformed ioctl resp: len %d offset %d\\n\", *plen,\n\t\t\tle32_to_cpu(rsp->OutputOffset));\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\t*out_data = kmemdup((char *)rsp + le32_to_cpu(rsp->OutputOffset),\n\t\t\t    *plen, GFP_KERNEL);\n\tif (*out_data == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto ioctl_exit;\n\t}\n\nioctl_exit:\n\tSMB2_ioctl_free(&rqst);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_ioctl(const unsigned int xid, struct cifs_tcon *tcon, u64 persistent_fid,\n\t   u64 volatile_fid, u32 opcode, bool is_fsctl,\n\t   char *in_data, u32 indatalen, u32 max_out_data_len,\n\t   char **out_data, u32 *plen /* returned data len */){\n\tstruct smb_rqst rqst;\n\tstruct smb2_ioctl_rsp *rsp = NULL;\n\tstruct cifs_ses *ses;\n\tstruct kvec iov[SMB2_IOCTL_IOV_SIZE];\n\tstruct kvec rsp_iov = {NULL, 0};\n\tint resp_buftype = CIFS_NO_BUFFER;\n\tint rc = 0;\n\tint flags = 0;\n\n\tcifs_dbg(FYI, \"SMB2 IOCTL\\n\");\n\n\tif (out_data != NULL)\n\t\t*out_data = NULL;\n\n\t/* zero out returned data len, in case of error */\n\tif (plen)\n\t\t*plen = 0;\n\n\tif (tcon)\n\t\tses = tcon->ses;\n\telse\n\t\treturn -EIO;\n\n\tif (!ses || !(ses->server))\n\t\treturn -EIO;\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\tmemset(&iov, 0, sizeof(iov));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = SMB2_IOCTL_IOV_SIZE;\n\n\trc = SMB2_ioctl_init(tcon, &rqst, persistent_fid, volatile_fid, opcode,\n\t\t\t     is_fsctl, in_data, indatalen, max_out_data_len);\n\tif (rc)\n\t\tgoto ioctl_exit;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags,\n\t\t\t    &rsp_iov);\n\trsp = (struct smb2_ioctl_rsp *)rsp_iov.iov_base;\n\n\tif (rc != 0)\n\t\ttrace_smb3_fsctl_err(xid, persistent_fid, tcon->tid,\n\t\t\t\tses->Suid, 0, opcode, rc);\n\n\tif ((rc != 0) && (rc != -EINVAL)) {\n\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\tgoto ioctl_exit;\n\t} else if (rc == -EINVAL) {\n\t\tif ((opcode != FSCTL_SRV_COPYCHUNK_WRITE) &&\n\t\t    (opcode != FSCTL_SRV_COPYCHUNK)) {\n\t\t\tcifs_stats_fail_inc(tcon, SMB2_IOCTL_HE);\n\t\t\tgoto ioctl_exit;\n\t\t}\n\t}\n\n\t/* check if caller wants to look at return data or just return rc */\n\tif ((plen == NULL) || (out_data == NULL))\n\t\tgoto ioctl_exit;\n\n\t*plen = le32_to_cpu(rsp->OutputCount);\n\n\t/* We check for obvious errors in the output buffer length and offset */\n\tif (*plen == 0)\n\t\tgoto ioctl_exit; /* server returned no data */\n\telse if (*plen > rsp_iov.iov_len || *plen > 0xFF00) {\n\t\tcifs_dbg(VFS, \"srv returned invalid ioctl length: %d\\n\", *plen);\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\tif (rsp_iov.iov_len - *plen < le32_to_cpu(rsp->OutputOffset)) {\n\t\tcifs_dbg(VFS, \"Malformed ioctl resp: len %d offset %d\\n\", *plen,\n\t\t\tle32_to_cpu(rsp->OutputOffset));\n\t\t*plen = 0;\n\t\trc = -EIO;\n\t\tgoto ioctl_exit;\n\t}\n\n\t*out_data = kmemdup((char *)rsp + le32_to_cpu(rsp->OutputOffset),\n\t\t\t    *plen, GFP_KERNEL);\n\tif (*out_data == NULL) {\n\t\trc = -ENOMEM;\n\t\tgoto ioctl_exit;\n\t}\n\nioctl_exit:\n\tSMB2_ioctl_free(&rqst);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_443": {
        "vulnerable_code": {
            "Code": "SMB2_logoff(const unsigned int xid, struct cifs_ses *ses){\n\tstruct smb_rqst rqst;\n\tstruct smb2_logoff_req *req; /* response is also trivial struct */\n\tint rc = 0;\n\tstruct TCP_Server_Info *server;\n\tint flags = 0;\n\tunsigned int total_len;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tint resp_buf_type;\n\n\tcifs_dbg(FYI, \"disconnect session %p\\n\", ses);\n\n\tif (ses && (ses->server))\n\t\tserver = ses->server;\n\telse\n\t\treturn -EIO;\n\n\t/* no need to send SMB logoff if uid already closed due to reconnect */\n\tif (ses->need_reconnect)\n\t\tgoto smb2_session_already_dead;\n\n\trc = smb2_plain_req_init(SMB2_LOGOFF, NULL, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\t /* since no tcon, smb2_init can not do this, so do here */\n\treq->sync_hdr.SessionId = ses->Suid;\n\n\tif (ses->session_flags & SMB2_SESSION_FLAG_ENCRYPT_DATA)\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\telse if (server->sign)\n\t\treq->sync_hdr.Flags |= SMB2_FLAGS_SIGNED;\n\n\tflags |= CIFS_NO_RESP;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buf_type, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\t/*\n\t * No tcon so can't do\n\t * cifs_stats_inc(&tcon->stats.smb2_stats.smb2_com_fail[SMB2...]);\n\t */\n\nsmb2_session_already_dead:\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_logoff(const unsigned int xid, struct cifs_ses *ses){\n\tstruct smb_rqst rqst;\n\tstruct smb2_logoff_req *req; /* response is also trivial struct */\n\tint rc = 0;\n\tstruct TCP_Server_Info *server;\n\tint flags = 0;\n\tunsigned int total_len;\n\tstruct kvec iov[1];\n\tstruct kvec rsp_iov;\n\tint resp_buf_type;\n\n\tcifs_dbg(FYI, \"disconnect session %p\\n\", ses);\n\n\tif (ses && (ses->server))\n\t\tserver = ses->server;\n\telse\n\t\treturn -EIO;\n\n\t/* no need to send SMB logoff if uid already closed due to reconnect */\n\tif (ses->need_reconnect)\n\t\tgoto smb2_session_already_dead;\n\n\trc = smb2_plain_req_init(SMB2_LOGOFF, NULL, (void **) &req, &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\t /* since no tcon, smb2_init can not do this, so do here */\n\treq->sync_hdr.SessionId = ses->Suid;\n\n\tif (ses->session_flags & SMB2_SESSION_FLAG_ENCRYPT_DATA)\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\telse if (server->sign)\n\t\treq->sync_hdr.Flags |= SMB2_FLAGS_SIGNED;\n\n\tflags |= CIFS_NO_RESP;\n\n\tiov[0].iov_base = (char *)req;\n\tiov[0].iov_len = total_len;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 1;\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buf_type, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\t/*\n\t * No tcon so can't do\n\t * cifs_stats_inc(&tcon->stats.smb2_stats.smb2_com_fail[SMB2...]);\n\t */\n\nsmb2_session_already_dead:\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_444": {
        "vulnerable_code": {
            "Code": "SMB2_tcon(const unsigned int xid, struct cifs_ses *ses, const char *tree,\n\t  struct cifs_tcon *tcon, const struct nls_table *cp){\n\tstruct smb_rqst rqst;\n\tstruct smb2_tree_connect_req *req;\n\tstruct smb2_tree_connect_rsp *rsp = NULL;\n\tstruct kvec iov[2];\n\tstruct kvec rsp_iov = { NULL, 0 };\n\tint rc = 0;\n\tint resp_buftype;\n\tint unc_path_len;\n\t__le16 *unc_path = NULL;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\tcifs_dbg(FYI, \"TCON\\n\");\n\n\tif (!(ses->server) || !tree)\n\t\treturn -EIO;\n\n\tunc_path = kmalloc(MAX_SHARENAME_LENGTH * 2, GFP_KERNEL);\n\tif (unc_path == NULL)\n\t\treturn -ENOMEM;\n\n\tunc_path_len = cifs_strtoUTF16(unc_path, tree, strlen(tree), cp) + 1;\n\tunc_path_len *= 2;\n\tif (unc_path_len < 2) {\n\t\tkfree(unc_path);\n\t\treturn -EINVAL;\n\t}\n\n\t/* SMB2 TREE_CONNECT request must be called with TreeId == 0 */\n\ttcon->tid = 0;\n\tatomic_set(&tcon->num_remote_opens, 0);\n\trc = smb2_plain_req_init(SMB2_TREE_CONNECT, tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc) {\n\t\tkfree(unc_path);\n\t\treturn rc;\n\t}\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for pad */\n\tiov[0].iov_len = total_len - 1;\n\n\t/* Testing shows that buffer offset must be at location of Buffer[0] */\n\treq->PathOffset = cpu_to_le16(sizeof(struct smb2_tree_connect_req)\n\t\t\t- 1 /* pad */);\n\treq->PathLength = cpu_to_le16(unc_path_len - 2);\n\tiov[1].iov_base = unc_path;\n\tiov[1].iov_len = unc_path_len;\n\n\t/*\n\t * 3.11 tcon req must be signed if not encrypted. See MS-SMB2 3.2.4.1.1\n\t * unless it is guest or anonymous user. See MS-SMB2 3.2.5.3.1\n\t * (Samba servers don't always set the flag so also check if null user)\n\t */\n\tif ((ses->server->dialect == SMB311_PROT_ID) &&\n\t    !smb3_encryption_required(tcon) &&\n\t    !(ses->session_flags &\n\t\t    (SMB2_SESSION_FLAG_IS_GUEST|SMB2_SESSION_FLAG_IS_NULL)) &&\n\t    ((ses->user_name != NULL) || (ses->sectype == Kerberos)))\n\t\treq->sync_hdr.Flags |= SMB2_FLAGS_SIGNED;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 2;\n\n\t/* Need 64 for max size write so ask for more in case not there yet */\n\treq->sync_hdr.CreditRequest = cpu_to_le16(64);\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_tree_connect_rsp *)rsp_iov.iov_base;\n\ttrace_smb3_tcon(xid, tcon->tid, ses->Suid, tree, rc);\n\tif (rc != 0) {\n\t\tif (tcon) {\n\t\t\tcifs_stats_fail_inc(tcon, SMB2_TREE_CONNECT_HE);\n\t\t\ttcon->need_reconnect = true;\n\t\t}\n\t\tgoto tcon_error_exit;\n\t}\n\n\tswitch (rsp->ShareType) {\n\tcase SMB2_SHARE_TYPE_DISK:\n\t\tcifs_dbg(FYI, \"connection to disk share\\n\");\n\t\tbreak;\n\tcase SMB2_SHARE_TYPE_PIPE:\n\t\ttcon->pipe = true;\n\t\tcifs_dbg(FYI, \"connection to pipe share\\n\");\n\t\tbreak;\n\tcase SMB2_SHARE_TYPE_PRINT:\n\t\ttcon->print = true;\n\t\tcifs_dbg(FYI, \"connection to printer\\n\");\n\t\tbreak;\n\tdefault:\n\t\tcifs_dbg(VFS, \"unknown share type %d\\n\", rsp->ShareType);\n\t\trc = -EOPNOTSUPP;\n\t\tgoto tcon_error_exit;\n\t}\n\n\ttcon->share_flags = le32_to_cpu(rsp->ShareFlags);\n\ttcon->capabilities = rsp->Capabilities; /* we keep caps little endian */\n\ttcon->maximal_access = le32_to_cpu(rsp->MaximalAccess);\n\ttcon->tidStatus = CifsGood;\n\ttcon->need_reconnect = false;\n\ttcon->tid = rsp->sync_hdr.TreeId;\n\tstrlcpy(tcon->treeName, tree, sizeof(tcon->treeName));\n\n\tif ((rsp->Capabilities & SMB2_SHARE_CAP_DFS) &&\n\t    ((tcon->share_flags & SHI1005_FLAGS_DFS) == 0))\n\t\tcifs_dbg(VFS, \"DFS capability contradicts DFS flag\\n\");\n\n\tif (tcon->seal &&\n\t    !(tcon->ses->server->capabilities & SMB2_GLOBAL_CAP_ENCRYPTION))\n\t\tcifs_dbg(VFS, \"Encryption is requested but not supported\\n\");\n\n\tinit_copy_chunk_defaults(tcon);\n\tif (tcon->ses->server->ops->validate_negotiate)\n\t\trc = tcon->ses->server->ops->validate_negotiate(xid, tcon);\ntcon_exit:\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\tkfree(unc_path);\n\treturn rc;\n\ntcon_error_exit:\n\tif (rsp && rsp->sync_hdr.Status == STATUS_BAD_NETWORK_NAME) {\n\t\tcifs_dbg(VFS, \"BAD_NETWORK_NAME: %s\\n\", tree);\n\t}\n\tgoto tcon_exit;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_tcon(const unsigned int xid, struct cifs_ses *ses, const char *tree,\n\t  struct cifs_tcon *tcon, const struct nls_table *cp){\n\tstruct smb_rqst rqst;\n\tstruct smb2_tree_connect_req *req;\n\tstruct smb2_tree_connect_rsp *rsp = NULL;\n\tstruct kvec iov[2];\n\tstruct kvec rsp_iov = { NULL, 0 };\n\tint rc = 0;\n\tint resp_buftype;\n\tint unc_path_len;\n\t__le16 *unc_path = NULL;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\tcifs_dbg(FYI, \"TCON\\n\");\n\n\tif (!(ses->server) || !tree)\n\t\treturn -EIO;\n\n\tunc_path = kmalloc(MAX_SHARENAME_LENGTH * 2, GFP_KERNEL);\n\tif (unc_path == NULL)\n\t\treturn -ENOMEM;\n\n\tunc_path_len = cifs_strtoUTF16(unc_path, tree, strlen(tree), cp) + 1;\n\tunc_path_len *= 2;\n\tif (unc_path_len < 2) {\n\t\tkfree(unc_path);\n\t\treturn -EINVAL;\n\t}\n\n\t/* SMB2 TREE_CONNECT request must be called with TreeId == 0 */\n\ttcon->tid = 0;\n\tatomic_set(&tcon->num_remote_opens, 0);\n\trc = smb2_plain_req_init(SMB2_TREE_CONNECT, tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc) {\n\t\tkfree(unc_path);\n\t\treturn rc;\n\t}\n\n\tif (smb3_encryption_required(tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for pad */\n\tiov[0].iov_len = total_len - 1;\n\n\t/* Testing shows that buffer offset must be at location of Buffer[0] */\n\treq->PathOffset = cpu_to_le16(sizeof(struct smb2_tree_connect_req)\n\t\t\t- 1 /* pad */);\n\treq->PathLength = cpu_to_le16(unc_path_len - 2);\n\tiov[1].iov_base = unc_path;\n\tiov[1].iov_len = unc_path_len;\n\n\t/*\n\t * 3.11 tcon req must be signed if not encrypted. See MS-SMB2 3.2.4.1.1\n\t * unless it is guest or anonymous user. See MS-SMB2 3.2.5.3.1\n\t * (Samba servers don't always set the flag so also check if null user)\n\t */\n\tif ((ses->server->dialect == SMB311_PROT_ID) &&\n\t    !smb3_encryption_required(tcon) &&\n\t    !(ses->session_flags &\n\t\t    (SMB2_SESSION_FLAG_IS_GUEST|SMB2_SESSION_FLAG_IS_NULL)) &&\n\t    ((ses->user_name != NULL) || (ses->sectype == Kerberos)))\n\t\treq->sync_hdr.Flags |= SMB2_FLAGS_SIGNED;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = 2;\n\n\t/* Need 64 for max size write so ask for more in case not there yet */\n\treq->sync_hdr.CreditRequest = cpu_to_le16(64);\n\n\trc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);\n\tcifs_small_buf_release(req);\n\trsp = (struct smb2_tree_connect_rsp *)rsp_iov.iov_base;\n\ttrace_smb3_tcon(xid, tcon->tid, ses->Suid, tree, rc);\n\tif (rc != 0) {\n\t\tif (tcon) {\n\t\t\tcifs_stats_fail_inc(tcon, SMB2_TREE_CONNECT_HE);\n\t\t\ttcon->need_reconnect = true;\n\t\t}\n\t\tgoto tcon_error_exit;\n\t}\n\n\tswitch (rsp->ShareType) {\n\tcase SMB2_SHARE_TYPE_DISK:\n\t\tcifs_dbg(FYI, \"connection to disk share\\n\");\n\t\tbreak;\n\tcase SMB2_SHARE_TYPE_PIPE:\n\t\ttcon->pipe = true;\n\t\tcifs_dbg(FYI, \"connection to pipe share\\n\");\n\t\tbreak;\n\tcase SMB2_SHARE_TYPE_PRINT:\n\t\ttcon->print = true;\n\t\tcifs_dbg(FYI, \"connection to printer\\n\");\n\t\tbreak;\n\tdefault:\n\t\tcifs_dbg(VFS, \"unknown share type %d\\n\", rsp->ShareType);\n\t\trc = -EOPNOTSUPP;\n\t\tgoto tcon_error_exit;\n\t}\n\n\ttcon->share_flags = le32_to_cpu(rsp->ShareFlags);\n\ttcon->capabilities = rsp->Capabilities; /* we keep caps little endian */\n\ttcon->maximal_access = le32_to_cpu(rsp->MaximalAccess);\n\ttcon->tidStatus = CifsGood;\n\ttcon->need_reconnect = false;\n\ttcon->tid = rsp->sync_hdr.TreeId;\n\tstrlcpy(tcon->treeName, tree, sizeof(tcon->treeName));\n\n\tif ((rsp->Capabilities & SMB2_SHARE_CAP_DFS) &&\n\t    ((tcon->share_flags & SHI1005_FLAGS_DFS) == 0))\n\t\tcifs_dbg(VFS, \"DFS capability contradicts DFS flag\\n\");\n\n\tif (tcon->seal &&\n\t    !(tcon->ses->server->capabilities & SMB2_GLOBAL_CAP_ENCRYPTION))\n\t\tcifs_dbg(VFS, \"Encryption is requested but not supported\\n\");\n\n\tinit_copy_chunk_defaults(tcon);\n\tif (tcon->ses->server->ops->validate_negotiate)\n\t\trc = tcon->ses->server->ops->validate_negotiate(xid, tcon);\ntcon_exit:\n\n\tfree_rsp_buf(resp_buftype, rsp);\n\tkfree(unc_path);\n\treturn rc;\n\ntcon_error_exit:\n\tif (rsp && rsp->sync_hdr.Status == STATUS_BAD_NETWORK_NAME) {\n\t\tcifs_dbg(VFS, \"BAD_NETWORK_NAME: %s\\n\", tree);\n\t}\n\tgoto tcon_exit;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_445": {
        "vulnerable_code": {
            "Code": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec){\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tcifs_small_buf_release(req);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "SMB2_write(const unsigned int xid, struct cifs_io_parms *io_parms,\n\t   unsigned int *nbytes, struct kvec *iov, int n_vec){\n\tstruct smb_rqst rqst;\n\tint rc = 0;\n\tstruct smb2_write_req *req = NULL;\n\tstruct smb2_write_rsp *rsp = NULL;\n\tint resp_buftype;\n\tstruct kvec rsp_iov;\n\tint flags = 0;\n\tunsigned int total_len;\n\n\t*nbytes = 0;\n\n\tif (n_vec < 1)\n\t\treturn rc;\n\n\trc = smb2_plain_req_init(SMB2_WRITE, io_parms->tcon, (void **) &req,\n\t\t\t     &total_len);\n\tif (rc)\n\t\treturn rc;\n\n\tif (io_parms->tcon->ses->server == NULL)\n\t\treturn -ECONNABORTED;\n\n\tif (smb3_encryption_required(io_parms->tcon))\n\t\tflags |= CIFS_TRANSFORM_REQ;\n\n\treq->sync_hdr.ProcessId = cpu_to_le32(io_parms->pid);\n\n\treq->PersistentFileId = io_parms->persistent_fid;\n\treq->VolatileFileId = io_parms->volatile_fid;\n\treq->WriteChannelInfoOffset = 0;\n\treq->WriteChannelInfoLength = 0;\n\treq->Channel = 0;\n\treq->Length = cpu_to_le32(io_parms->length);\n\treq->Offset = cpu_to_le64(io_parms->offset);\n\treq->DataOffset = cpu_to_le16(\n\t\t\t\toffsetof(struct smb2_write_req, Buffer));\n\treq->RemainingBytes = 0;\n\n\ttrace_smb3_write_enter(xid, io_parms->persistent_fid,\n\t\tio_parms->tcon->tid, io_parms->tcon->ses->Suid,\n\t\tio_parms->offset, io_parms->length);\n\n\tiov[0].iov_base = (char *)req;\n\t/* 1 for Buffer */\n\tiov[0].iov_len = total_len - 1;\n\n\tmemset(&rqst, 0, sizeof(struct smb_rqst));\n\trqst.rq_iov = iov;\n\trqst.rq_nvec = n_vec + 1;\n\n\trc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,\n\t\t\t    &resp_buftype, flags, &rsp_iov);\n\trsp = (struct smb2_write_rsp *)rsp_iov.iov_base;\n\n\tif (rc) {\n\t\ttrace_smb3_write_err(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, io_parms->length, rc);\n\t\tcifs_stats_fail_inc(io_parms->tcon, SMB2_WRITE_HE);\n\t\tcifs_dbg(VFS, \"Send error in write = %d\\n\", rc);\n\t} else {\n\t\t*nbytes = le32_to_cpu(rsp->DataLength);\n\t\ttrace_smb3_write_done(xid, req->PersistentFileId,\n\t\t\t\t     io_parms->tcon->tid,\n\t\t\t\t     io_parms->tcon->ses->Suid,\n\t\t\t\t     io_parms->offset, *nbytes);\n\t}\n\n\tcifs_small_buf_release(req);\n\tfree_rsp_buf(resp_buftype, rsp);\n\treturn rc;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_446": {
        "vulnerable_code": {
            "Code": "add_durable_v2_context(struct kvec *iov, unsigned int *num_iovec,\n\t\t    struct cifs_open_parms *oparms){\n\tstruct smb2_create_req *req = iov[0].iov_base;\n\tunsigned int num = *num_iovec;\n\n\tiov[num].iov_base = create_durable_v2_buf(oparms);\n\tif (iov[num].iov_base == NULL)\n\t\treturn -ENOMEM;\n\tiov[num].iov_len = sizeof(struct create_durable_v2);\n\tif (!req->CreateContextsOffset)\n\t\treq->CreateContextsOffset =\n\t\t\tcpu_to_le32(sizeof(struct smb2_create_req) +\n\t\t\t\t\t\t\t\tiov[1].iov_len);\n\tle32_add_cpu(&req->CreateContextsLength, sizeof(struct create_durable_v2));\n\t*num_iovec = num + 1;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "add_durable_v2_context(struct kvec *iov, unsigned int *num_iovec,\n\t\t    struct cifs_open_parms *oparms){\n\tstruct smb2_create_req *req = iov[0].iov_base;\n\tunsigned int num = *num_iovec;\n\n\tiov[num].iov_base = create_durable_v2_buf(oparms);\n\tif (iov[num].iov_base == NULL)\n\t\treturn -ENOMEM;\n\tiov[num].iov_len = sizeof(struct create_durable_v2);\n\tif (!req->CreateContextsOffset)\n\t\treq->CreateContextsOffset =\n\t\t\tcpu_to_le32(sizeof(struct smb2_create_req) +\n\t\t\t\t\t\t\t\tiov[1].iov_len);\n\tle32_add_cpu(&req->CreateContextsLength, sizeof(struct create_durable_v2));\n\t*num_iovec = num + 1;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_447": {
        "vulnerable_code": {
            "Code": "smb2_validate_iov(unsigned int offset, unsigned int buffer_length,\n\t\t  struct kvec *iov, unsigned int min_buf_size){\n\tunsigned int smb_len = iov->iov_len;\n\tchar *end_of_smb = smb_len + (char *)iov->iov_base;\n\tchar *begin_of_buf = offset + (char *)iov->iov_base;\n\tchar *end_of_buf = begin_of_buf + buffer_length;\n\n\n\tif (buffer_length < min_buf_size) {\n\t\tcifs_dbg(VFS, \"buffer length %d smaller than minimum size %d\\n\",\n\t\t\t buffer_length, min_buf_size);\n\t\treturn -EINVAL;\n\t}\n\n\t/* check if beyond RFC1001 maximum length */\n\tif ((smb_len > 0x7FFFFF) || (buffer_length > 0x7FFFFF)) {\n\t\tcifs_dbg(VFS, \"buffer length %d or smb length %d too large\\n\",\n\t\t\t buffer_length, smb_len);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((begin_of_buf > end_of_smb) || (end_of_buf > end_of_smb)) {\n\t\tcifs_dbg(VFS, \"illegal server response, bad offset to data\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smb2_validate_iov(unsigned int offset, unsigned int buffer_length,\n\t\t  struct kvec *iov, unsigned int min_buf_size){\n\tunsigned int smb_len = iov->iov_len;\n\tchar *end_of_smb = smb_len + (char *)iov->iov_base;\n\tchar *begin_of_buf = offset + (char *)iov->iov_base;\n\tchar *end_of_buf = begin_of_buf + buffer_length;\n\n\n\tif (buffer_length < min_buf_size) {\n\t\tcifs_dbg(VFS, \"buffer length %d smaller than minimum size %d\\n\",\n\t\t\t buffer_length, min_buf_size);\n\t\treturn -EINVAL;\n\t}\n\n\t/* check if beyond RFC1001 maximum length */\n\tif ((smb_len > 0x7FFFFF) || (buffer_length > 0x7FFFFF)) {\n\t\tcifs_dbg(VFS, \"buffer length %d or smb length %d too large\\n\",\n\t\t\t buffer_length, smb_len);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((begin_of_buf > end_of_smb) || (end_of_buf > end_of_smb)) {\n\t\tcifs_dbg(VFS, \"illegal server response, bad offset to data\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_448": {
        "vulnerable_code": {
            "Code": "__tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\n\t\t\t       int nonagle){\n\t/* If we are closed, the bytes will have to remain here.\n\t * In time closedown will finish, we empty the write queue and\n\t * all will be happy.\n\t */\n\tif (unlikely(sk->sk_state == TCP_CLOSE))\n\t\treturn;\n\n\tif (tcp_write_xmit(sk, cur_mss, nonagle, 0,\n\t\t\t   sk_gfp_mask(sk, GFP_ATOMIC)))\n\t\ttcp_check_probe_timer(sk);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "__tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\n\t\t\t       int nonagle){\n\t/* If we are closed, the bytes will have to remain here.\n\t * In time closedown will finish, we empty the write queue and\n\t * all will be happy.\n\t */\n\tif (unlikely(sk->sk_state == TCP_CLOSE))\n\t\treturn;\n\n\tif (tcp_write_xmit(sk, cur_mss, nonagle, 0,\n\t\t\t   sk_gfp_mask(sk, GFP_ATOMIC)))\n\t\ttcp_check_probe_timer(sk);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_449": {
        "vulnerable_code": {
            "Code": "smc_options_write(__be32 *ptr, u16 *options){\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (unlikely(OPTION_SMC & *options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t\t       (TCPOPT_EXP <<  8) |\n\t\t\t\t       (TCPOLEN_EXP_SMC_BASE));\n\t\t\t*ptr++ = htonl(TCPOPT_SMC_MAGIC);\n\t\t}\n\t}\n#endif\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smc_options_write(__be32 *ptr, u16 *options){\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (unlikely(OPTION_SMC & *options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t\t       (TCPOPT_EXP <<  8) |\n\t\t\t\t       (TCPOLEN_EXP_SMC_BASE));\n\t\t\t*ptr++ = htonl(TCPOPT_SMC_MAGIC);\n\t\t}\n\t}\n#endif\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_450": {
        "vulnerable_code": {
            "Code": "tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *next_skb = skb_rb_next(skb);\n\tint next_skb_size;\n\n\tnext_skb_size = next_skb->len;\n\n\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\n\n\tif (next_skb_size) {\n\t\tif (next_skb_size <= skb_availroom(skb))\n\t\t\tskb_copy_bits(next_skb, 0, skb_put(skb, next_skb_size),\n\t\t\t\t      next_skb_size);\n\t\telse if (!skb_shift(skb, next_skb, next_skb_size))\n\t\t\treturn false;\n\t}\n\ttcp_highest_sack_replace(sk, next_skb, skb);\n\n\t/* Update sequence range on original skb. */\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;\n\n\t/* Merge over control information. This moves PSH/FIN etc. over */\n\tTCP_SKB_CB(skb)->tcp_flags |= TCP_SKB_CB(next_skb)->tcp_flags;\n\n\t/* All done, get rid of second SKB and account for it so\n\t * packet counting does not break.\n\t */\n\tTCP_SKB_CB(skb)->sacked |= TCP_SKB_CB(next_skb)->sacked & TCPCB_EVER_RETRANS;\n\tTCP_SKB_CB(skb)->eor = TCP_SKB_CB(next_skb)->eor;\n\n\t/* changed transmit queue under us so clear hints */\n\ttcp_clear_retrans_hints_partial(tp);\n\tif (next_skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = skb;\n\n\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\n\n\ttcp_skb_collapse_tstamp(skb, next_skb);\n\n\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\n\treturn true;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *next_skb = skb_rb_next(skb);\n\tint next_skb_size;\n\n\tnext_skb_size = next_skb->len;\n\n\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\n\n\tif (next_skb_size) {\n\t\tif (next_skb_size <= skb_availroom(skb))\n\t\t\tskb_copy_bits(next_skb, 0, skb_put(skb, next_skb_size),\n\t\t\t\t      next_skb_size);\n\t\telse if (!tcp_skb_shift(skb, next_skb, 1, next_skb_size))\n\t\t\treturn false;\n\t}\n\ttcp_highest_sack_replace(sk, next_skb, skb);\n\n\t/* Update sequence range on original skb. */\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;\n\n\t/* Merge over control information. This moves PSH/FIN etc. over */\n\tTCP_SKB_CB(skb)->tcp_flags |= TCP_SKB_CB(next_skb)->tcp_flags;\n\n\t/* All done, get rid of second SKB and account for it so\n\t * packet counting does not break.\n\t */\n\tTCP_SKB_CB(skb)->sacked |= TCP_SKB_CB(next_skb)->sacked & TCPCB_EVER_RETRANS;\n\tTCP_SKB_CB(skb)->eor = TCP_SKB_CB(next_skb)->eor;\n\n\t/* changed transmit queue under us so clear hints */\n\ttcp_clear_retrans_hints_partial(tp);\n\tif (next_skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = skb;\n\n\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\n\n\ttcp_skb_collapse_tstamp(skb, next_skb);\n\n\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_451": {
        "vulnerable_code": {
            "Code": "tcp_connect_queue_skb(struct sock *sk, struct sk_buff *skb){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\ttcb->end_seq += skb->len;\n\t__skb_header_release(skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n\ttp->write_seq = tcb->end_seq;\n\ttp->packets_out += tcp_skb_pcount(skb);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_connect_queue_skb(struct sock *sk, struct sk_buff *skb){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\ttcb->end_seq += skb->len;\n\t__skb_header_release(skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n\ttp->write_seq = tcb->end_seq;\n\ttp->packets_out += tcp_skb_pcount(skb);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_452": {
        "vulnerable_code": {
            "Code": "tcp_minshall_update(struct tcp_sock *tp, unsigned int mss_now,\n\t\t\t\tconst struct sk_buff *skb){\n\tif (skb->len < tcp_skb_pcount(skb) * mss_now)\n\t\ttp->snd_sml = TCP_SKB_CB(skb)->end_seq;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_minshall_update(struct tcp_sock *tp, unsigned int mss_now,\n\t\t\t\tconst struct sk_buff *skb){\n\tif (skb->len < tcp_skb_pcount(skb) * mss_now)\n\t\ttp->snd_sml = TCP_SKB_CB(skb)->end_seq;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_453": {
        "vulnerable_code": {
            "Code": "tcp_release_cb(struct sock *sk){\n\tunsigned long flags, nflags;\n\n\t/* perform an atomic operation only if at least one flag is set */\n\tdo {\n\t\tflags = sk->sk_tsq_flags;\n\t\tif (!(flags & TCP_DEFERRED_ALL))\n\t\t\treturn;\n\t\tnflags = flags & ~TCP_DEFERRED_ALL;\n\t} while (cmpxchg(&sk->sk_tsq_flags, flags, nflags) != flags);\n\n\tif (flags & TCPF_TSQ_DEFERRED) {\n\t\ttcp_tsq_write(sk);\n\t\t__sock_put(sk);\n\t}\n\t/* Here begins the tricky part :\n\t * We are called from release_sock() with :\n\t * 1) BH disabled\n\t * 2) sk_lock.slock spinlock held\n\t * 3) socket owned by us (sk->sk_lock.owned == 1)\n\t *\n\t * But following code is meant to be called from BH handlers,\n\t * so we should keep BH disabled, but early release socket ownership\n\t */\n\tsock_release_ownership(sk);\n\n\tif (flags & TCPF_WRITE_TIMER_DEFERRED) {\n\t\ttcp_write_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_DELACK_TIMER_DEFERRED) {\n\t\ttcp_delack_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_MTU_REDUCED_DEFERRED) {\n\t\tinet_csk(sk)->icsk_af_ops->mtu_reduced(sk);\n\t\t__sock_put(sk);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_release_cb(struct sock *sk){\n\tunsigned long flags, nflags;\n\n\t/* perform an atomic operation only if at least one flag is set */\n\tdo {\n\t\tflags = sk->sk_tsq_flags;\n\t\tif (!(flags & TCP_DEFERRED_ALL))\n\t\t\treturn;\n\t\tnflags = flags & ~TCP_DEFERRED_ALL;\n\t} while (cmpxchg(&sk->sk_tsq_flags, flags, nflags) != flags);\n\n\tif (flags & TCPF_TSQ_DEFERRED) {\n\t\ttcp_tsq_write(sk);\n\t\t__sock_put(sk);\n\t}\n\t/* Here begins the tricky part :\n\t * We are called from release_sock() with :\n\t * 1) BH disabled\n\t * 2) sk_lock.slock spinlock held\n\t * 3) socket owned by us (sk->sk_lock.owned == 1)\n\t *\n\t * But following code is meant to be called from BH handlers,\n\t * so we should keep BH disabled, but early release socket ownership\n\t */\n\tsock_release_ownership(sk);\n\n\tif (flags & TCPF_WRITE_TIMER_DEFERRED) {\n\t\ttcp_write_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_DELACK_TIMER_DEFERRED) {\n\t\ttcp_delack_timer_handler(sk);\n\t\t__sock_put(sk);\n\t}\n\tif (flags & TCPF_MTU_REDUCED_DEFERRED) {\n\t\tinet_csk(sk)->icsk_af_ops->mtu_reduced(sk);\n\t\t__sock_put(sk);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_454": {
        "vulnerable_code": {
            "Code": "tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto){\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 timeout, rto_delta_us;\n\tint early_retrans;\n\n\t/* Don't do any loss probe on a Fast Open connection before 3WHS\n\t * finishes.\n\t */\n\tif (tp->fastopen_rsk)\n\t\treturn false;\n\n\tearly_retrans = sock_net(sk)->ipv4.sysctl_tcp_early_retrans;\n\t/* Schedule a loss probe in 2*RTT for SACK capable connections\n\t * not in loss recovery, that are either limited by cwnd or application.\n\t */\n\tif ((early_retrans != 3 && early_retrans != 4) ||\n\t    !tp->packets_out || !tcp_is_sack(tp) ||\n\t    (icsk->icsk_ca_state != TCP_CA_Open &&\n\t     icsk->icsk_ca_state != TCP_CA_CWR))\n\t\treturn false;\n\n\t/* Probe timeout is 2*rtt. Add minimum RTO to account\n\t * for delayed ack when there's one outstanding packet. If no RTT\n\t * sample is available then probe after TCP_TIMEOUT_INIT.\n\t */\n\tif (tp->srtt_us) {\n\t\ttimeout = usecs_to_jiffies(tp->srtt_us >> 2);\n\t\tif (tp->packets_out == 1)\n\t\t\ttimeout += TCP_RTO_MIN;\n\t\telse\n\t\t\ttimeout += TCP_TIMEOUT_MIN;\n\t} else {\n\t\ttimeout = TCP_TIMEOUT_INIT;\n\t}\n\n\t/* If the RTO formula yields an earlier time, then use that time. */\n\trto_delta_us = advancing_rto ?\n\t\t\tjiffies_to_usecs(inet_csk(sk)->icsk_rto) :\n\t\t\ttcp_rto_delta_us(sk);  /* How far in future is RTO? */\n\tif (rto_delta_us > 0)\n\t\ttimeout = min_t(u32, timeout, usecs_to_jiffies(rto_delta_us));\n\n\ttcp_reset_xmit_timer(sk, ICSK_TIME_LOSS_PROBE, timeout,\n\t\t\t     TCP_RTO_MAX, NULL);\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto){\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 timeout, rto_delta_us;\n\tint early_retrans;\n\n\t/* Don't do any loss probe on a Fast Open connection before 3WHS\n\t * finishes.\n\t */\n\tif (tp->fastopen_rsk)\n\t\treturn false;\n\n\tearly_retrans = sock_net(sk)->ipv4.sysctl_tcp_early_retrans;\n\t/* Schedule a loss probe in 2*RTT for SACK capable connections\n\t * not in loss recovery, that are either limited by cwnd or application.\n\t */\n\tif ((early_retrans != 3 && early_retrans != 4) ||\n\t    !tp->packets_out || !tcp_is_sack(tp) ||\n\t    (icsk->icsk_ca_state != TCP_CA_Open &&\n\t     icsk->icsk_ca_state != TCP_CA_CWR))\n\t\treturn false;\n\n\t/* Probe timeout is 2*rtt. Add minimum RTO to account\n\t * for delayed ack when there's one outstanding packet. If no RTT\n\t * sample is available then probe after TCP_TIMEOUT_INIT.\n\t */\n\tif (tp->srtt_us) {\n\t\ttimeout = usecs_to_jiffies(tp->srtt_us >> 2);\n\t\tif (tp->packets_out == 1)\n\t\t\ttimeout += TCP_RTO_MIN;\n\t\telse\n\t\t\ttimeout += TCP_TIMEOUT_MIN;\n\t} else {\n\t\ttimeout = TCP_TIMEOUT_INIT;\n\t}\n\n\t/* If the RTO formula yields an earlier time, then use that time. */\n\trto_delta_us = advancing_rto ?\n\t\t\tjiffies_to_usecs(inet_csk(sk)->icsk_rto) :\n\t\t\ttcp_rto_delta_us(sk);  /* How far in future is RTO? */\n\tif (rto_delta_us > 0)\n\t\ttimeout = min_t(u32, timeout, usecs_to_jiffies(rto_delta_us));\n\n\ttcp_reset_xmit_timer(sk, ICSK_TIME_LOSS_PROBE, timeout,\n\t\t\t     TCP_RTO_MAX, NULL);\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_455": {
        "vulnerable_code": {
            "Code": "tcp_select_window(struct sock *sk){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 old_win = tp->rcv_wnd;\n\tu32 cur_win = tcp_receive_window(tp);\n\tu32 new_win = __tcp_select_window(sk);\n\n\t/* Never shrink the offered window */\n\tif (new_win < cur_win) {\n\t\t/* Danger Will Robinson!\n\t\t * Don't update rcv_wup/rcv_wnd here or else\n\t\t * we will not be able to advertise a zero\n\t\t * window in time.  --DaveM\n\t\t *\n\t\t * Relax Will Robinson.\n\t\t */\n\t\tif (new_win == 0)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPWANTZEROWINDOWADV);\n\t\tnew_win = ALIGN(cur_win, 1 << tp->rx_opt.rcv_wscale);\n\t}\n\ttp->rcv_wnd = new_win;\n\ttp->rcv_wup = tp->rcv_nxt;\n\n\t/* Make sure we do not exceed the maximum possible\n\t * scaled window.\n\t */\n\tif (!tp->rx_opt.rcv_wscale &&\n\t    sock_net(sk)->ipv4.sysctl_tcp_workaround_signed_windows)\n\t\tnew_win = min(new_win, MAX_TCP_WINDOW);\n\telse\n\t\tnew_win = min(new_win, (65535U << tp->rx_opt.rcv_wscale));\n\n\t/* RFC1323 scaling applied */\n\tnew_win >>= tp->rx_opt.rcv_wscale;\n\n\t/* If we advertise zero window, disable fast path. */\n\tif (new_win == 0) {\n\t\ttp->pred_flags = 0;\n\t\tif (old_win)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPTOZEROWINDOWADV);\n\t} else if (old_win == 0) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFROMZEROWINDOWADV);\n\t}\n\n\treturn new_win;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_select_window(struct sock *sk){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 old_win = tp->rcv_wnd;\n\tu32 cur_win = tcp_receive_window(tp);\n\tu32 new_win = __tcp_select_window(sk);\n\n\t/* Never shrink the offered window */\n\tif (new_win < cur_win) {\n\t\t/* Danger Will Robinson!\n\t\t * Don't update rcv_wup/rcv_wnd here or else\n\t\t * we will not be able to advertise a zero\n\t\t * window in time.  --DaveM\n\t\t *\n\t\t * Relax Will Robinson.\n\t\t */\n\t\tif (new_win == 0)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPWANTZEROWINDOWADV);\n\t\tnew_win = ALIGN(cur_win, 1 << tp->rx_opt.rcv_wscale);\n\t}\n\ttp->rcv_wnd = new_win;\n\ttp->rcv_wup = tp->rcv_nxt;\n\n\t/* Make sure we do not exceed the maximum possible\n\t * scaled window.\n\t */\n\tif (!tp->rx_opt.rcv_wscale &&\n\t    sock_net(sk)->ipv4.sysctl_tcp_workaround_signed_windows)\n\t\tnew_win = min(new_win, MAX_TCP_WINDOW);\n\telse\n\t\tnew_win = min(new_win, (65535U << tp->rx_opt.rcv_wscale));\n\n\t/* RFC1323 scaling applied */\n\tnew_win >>= tp->rx_opt.rcv_wscale;\n\n\t/* If we advertise zero window, disable fast path. */\n\tif (new_win == 0) {\n\t\ttp->pred_flags = 0;\n\t\tif (old_win)\n\t\t\tNET_INC_STATS(sock_net(sk),\n\t\t\t\t      LINUX_MIB_TCPTOZEROWINDOWADV);\n\t} else if (old_win == 0) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPFROMZEROWINDOWADV);\n\t}\n\n\treturn new_win;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_456": {
        "vulnerable_code": {
            "Code": "tcp_send_ack(struct sock *sk){\n\t__tcp_send_ack(sk, tcp_sk(sk)->rcv_nxt);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_send_ack(struct sock *sk){\n\t__tcp_send_ack(sk, tcp_sk(sk)->rcv_nxt);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_457": {
        "vulnerable_code": {
            "Code": "tcp_send_probe0(struct sock *sk){\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tunsigned long timeout;\n\tint err;\n\n\terr = tcp_write_wakeup(sk, LINUX_MIB_TCPWINPROBE);\n\n\tif (tp->packets_out || tcp_write_queue_empty(sk)) {\n\t\t/* Cancel probe timer, if it is not required. */\n\t\ticsk->icsk_probes_out = 0;\n\t\ticsk->icsk_backoff = 0;\n\t\treturn;\n\t}\n\n\ticsk->icsk_probes_out++;\n\tif (err <= 0) {\n\t\tif (icsk->icsk_backoff < net->ipv4.sysctl_tcp_retries2)\n\t\t\ticsk->icsk_backoff++;\n\t\ttimeout = tcp_probe0_when(sk, TCP_RTO_MAX);\n\t} else {\n\t\t/* If packet was not sent due to local congestion,\n\t\t * Let senders fight for local resources conservatively.\n\t\t */\n\t\ttimeout = TCP_RESOURCE_PROBE_INTERVAL;\n\t}\n\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, timeout, TCP_RTO_MAX, NULL);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_send_probe0(struct sock *sk){\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tunsigned long timeout;\n\tint err;\n\n\terr = tcp_write_wakeup(sk, LINUX_MIB_TCPWINPROBE);\n\n\tif (tp->packets_out || tcp_write_queue_empty(sk)) {\n\t\t/* Cancel probe timer, if it is not required. */\n\t\ticsk->icsk_probes_out = 0;\n\t\ticsk->icsk_backoff = 0;\n\t\treturn;\n\t}\n\n\ticsk->icsk_probes_out++;\n\tif (err <= 0) {\n\t\tif (icsk->icsk_backoff < net->ipv4.sysctl_tcp_retries2)\n\t\t\ticsk->icsk_backoff++;\n\t\ttimeout = tcp_probe0_when(sk, TCP_RTO_MAX);\n\t} else {\n\t\t/* If packet was not sent due to local congestion,\n\t\t * Let senders fight for local resources conservatively.\n\t\t */\n\t\ttimeout = TCP_RESOURCE_PROBE_INTERVAL;\n\t}\n\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0, timeout, TCP_RTO_MAX, NULL);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_458": {
        "vulnerable_code": {
            "Code": "tcp_send_synack(struct sock *sk){\n\tstruct sk_buff *skb;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (!skb || !(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\tpr_err(\"%s: wrong queue state\\n\", __func__);\n\t\treturn -EFAULT;\n\t}\n\tif (!(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_ACK)) {\n\t\tif (skb_cloned(skb)) {\n\t\t\tstruct sk_buff *nskb;\n\n\t\t\ttcp_skb_tsorted_save(skb) {\n\t\t\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\t\t\t} tcp_skb_tsorted_restore(skb);\n\t\t\tif (!nskb)\n\t\t\t\treturn -ENOMEM;\n\t\t\tINIT_LIST_HEAD(&nskb->tcp_tsorted_anchor);\n\t\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\t\t\t__skb_header_release(nskb);\n\t\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, nskb);\n\t\t\tsk->sk_wmem_queued += nskb->truesize;\n\t\t\tsk_mem_charge(sk, nskb->truesize);\n\t\t\tskb = nskb;\n\t\t}\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ACK;\n\t\ttcp_ecn_send_synack(sk, skb);\n\t}\n\treturn tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_send_synack(struct sock *sk){\n\tstruct sk_buff *skb;\n\n\tskb = tcp_rtx_queue_head(sk);\n\tif (!skb || !(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\tpr_err(\"%s: wrong queue state\\n\", __func__);\n\t\treturn -EFAULT;\n\t}\n\tif (!(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_ACK)) {\n\t\tif (skb_cloned(skb)) {\n\t\t\tstruct sk_buff *nskb;\n\n\t\t\ttcp_skb_tsorted_save(skb) {\n\t\t\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\t\t\t} tcp_skb_tsorted_restore(skb);\n\t\t\tif (!nskb)\n\t\t\t\treturn -ENOMEM;\n\t\t\tINIT_LIST_HEAD(&nskb->tcp_tsorted_anchor);\n\t\t\ttcp_rtx_queue_unlink_and_free(skb, sk);\n\t\t\t__skb_header_release(nskb);\n\t\t\ttcp_rbtree_insert(&sk->tcp_rtx_queue, nskb);\n\t\t\tsk->sk_wmem_queued += nskb->truesize;\n\t\t\tsk_mem_charge(sk, nskb->truesize);\n\t\t\tskb = nskb;\n\t\t}\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_ACK;\n\t\ttcp_ecn_send_synack(sk, skb);\n\t}\n\treturn tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_459": {
        "vulnerable_code": {
            "Code": "tcp_small_queue_check(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t  unsigned int factor){\n\tunsigned long limit;\n\n\tlimit = max_t(unsigned long,\n\t\t      2 * skb->truesize,\n\t\t      sk->sk_pacing_rate >> sk->sk_pacing_shift);\n\tif (sk->sk_pacing_status == SK_PACING_NONE)\n\t\tlimit = min_t(unsigned long, limit,\n\t\t\t      sock_net(sk)->ipv4.sysctl_tcp_limit_output_bytes);\n\tlimit <<= factor;\n\n\tif (refcount_read(&sk->sk_wmem_alloc) > limit) {\n\t\t/* Always send skb if rtx queue is empty.\n\t\t * No need to wait for TX completion to call us back,\n\t\t * after softirq/tasklet schedule.\n\t\t * This helps when TX completions are delayed too much.\n\t\t */\n\t\tif (tcp_rtx_queue_empty(sk))\n\t\t\treturn false;\n\n\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t/* It is possible TX completion already happened\n\t\t * before we set TSQ_THROTTLED, so we must\n\t\t * test again the condition.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tif (refcount_read(&sk->sk_wmem_alloc) > limit)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_small_queue_check(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t  unsigned int factor){\n\tunsigned long limit;\n\n\tlimit = max_t(unsigned long,\n\t\t      2 * skb->truesize,\n\t\t      sk->sk_pacing_rate >> sk->sk_pacing_shift);\n\tif (sk->sk_pacing_status == SK_PACING_NONE)\n\t\tlimit = min_t(unsigned long, limit,\n\t\t\t      sock_net(sk)->ipv4.sysctl_tcp_limit_output_bytes);\n\tlimit <<= factor;\n\n\tif (refcount_read(&sk->sk_wmem_alloc) > limit) {\n\t\t/* Always send skb if rtx queue is empty.\n\t\t * No need to wait for TX completion to call us back,\n\t\t * after softirq/tasklet schedule.\n\t\t * This helps when TX completions are delayed too much.\n\t\t */\n\t\tif (tcp_rtx_queue_empty(sk))\n\t\t\treturn false;\n\n\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t/* It is possible TX completion already happened\n\t\t * before we set TSQ_THROTTLED, so we must\n\t\t * test again the condition.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tif (refcount_read(&sk->sk_wmem_alloc) > limit)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_460": {
        "vulnerable_code": {
            "Code": "tcp_tasklet_func(unsigned long data){\n\tstruct tsq_tasklet *tsq = (struct tsq_tasklet *)data;\n\tLIST_HEAD(list);\n\tunsigned long flags;\n\tstruct list_head *q, *n;\n\tstruct tcp_sock *tp;\n\tstruct sock *sk;\n\n\tlocal_irq_save(flags);\n\tlist_splice_init(&tsq->head, &list);\n\tlocal_irq_restore(flags);\n\n\tlist_for_each_safe(q, n, &list) {\n\t\ttp = list_entry(q, struct tcp_sock, tsq_node);\n\t\tlist_del(&tp->tsq_node);\n\n\t\tsk = (struct sock *)tp;\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(TSQ_QUEUED, &sk->sk_tsq_flags);\n\n\t\ttcp_tsq_handler(sk);\n\t\tsk_free(sk);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_tasklet_func(unsigned long data){\n\tstruct tsq_tasklet *tsq = (struct tsq_tasklet *)data;\n\tLIST_HEAD(list);\n\tunsigned long flags;\n\tstruct list_head *q, *n;\n\tstruct tcp_sock *tp;\n\tstruct sock *sk;\n\n\tlocal_irq_save(flags);\n\tlist_splice_init(&tsq->head, &list);\n\tlocal_irq_restore(flags);\n\n\tlist_for_each_safe(q, n, &list) {\n\t\ttp = list_entry(q, struct tcp_sock, tsq_node);\n\t\tlist_del(&tp->tsq_node);\n\n\t\tsk = (struct sock *)tp;\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(TSQ_QUEUED, &sk->sk_tsq_flags);\n\n\t\ttcp_tsq_handler(sk);\n\t\tsk_free(sk);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_461": {
        "vulnerable_code": {
            "Code": "tcp_write_wakeup(struct sock *sk, int mib){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\treturn -1;\n\n\tskb = tcp_send_head(sk);\n\tif (skb && before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp))) {\n\t\tint err;\n\t\tunsigned int mss = tcp_current_mss(sk);\n\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t\tif (before(tp->pushed_seq, TCP_SKB_CB(skb)->end_seq))\n\t\t\ttp->pushed_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t/* We are probing the opening of a window\n\t\t * but the window size is != 0\n\t\t * must have been a result SWS avoidance ( sender )\n\t\t */\n\t\tif (seg_size < TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq ||\n\t\t    skb->len > mss) {\n\t\t\tseg_size = min(seg_size, mss);\n\t\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\n\t\t\t\treturn -1;\n\t\t} else if (!tcp_skb_pcount(skb))\n\t\t\ttcp_set_skb_tso_segs(skb, mss);\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n\t\tif (!err)\n\t\t\ttcp_event_new_data_sent(sk, skb);\n\t\treturn err;\n\t} else {\n\t\tif (between(tp->snd_up, tp->snd_una + 1, tp->snd_una + 0xFFFF))\n\t\t\ttcp_xmit_probe_skb(sk, 1, mib);\n\t\treturn tcp_xmit_probe_skb(sk, 0, mib);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_write_wakeup(struct sock *sk, int mib){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (sk->sk_state == TCP_CLOSE)\n\t\treturn -1;\n\n\tskb = tcp_send_head(sk);\n\tif (skb && before(TCP_SKB_CB(skb)->seq, tcp_wnd_end(tp))) {\n\t\tint err;\n\t\tunsigned int mss = tcp_current_mss(sk);\n\t\tunsigned int seg_size = tcp_wnd_end(tp) - TCP_SKB_CB(skb)->seq;\n\n\t\tif (before(tp->pushed_seq, TCP_SKB_CB(skb)->end_seq))\n\t\t\ttp->pushed_seq = TCP_SKB_CB(skb)->end_seq;\n\n\t\t/* We are probing the opening of a window\n\t\t * but the window size is != 0\n\t\t * must have been a result SWS avoidance ( sender )\n\t\t */\n\t\tif (seg_size < TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq ||\n\t\t    skb->len > mss) {\n\t\t\tseg_size = min(seg_size, mss);\n\t\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\t\tif (tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t\t skb, seg_size, mss, GFP_ATOMIC))\n\t\t\t\treturn -1;\n\t\t} else if (!tcp_skb_pcount(skb))\n\t\t\ttcp_set_skb_tso_segs(skb, mss);\n\n\t\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\t\terr = tcp_transmit_skb(sk, skb, 1, GFP_ATOMIC);\n\t\tif (!err)\n\t\t\ttcp_event_new_data_sent(sk, skb);\n\t\treturn err;\n\t} else {\n\t\tif (between(tp->snd_up, tp->snd_una + 1, tp->snd_una + 0xFFFF))\n\t\t\ttcp_xmit_probe_skb(sk, 1, mib);\n\t\treturn tcp_xmit_probe_skb(sk, 0, mib);\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_462": {
        "vulnerable_code": {
            "Code": "smc_options_write(__be32 *ptr, u16 *options){\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (unlikely(OPTION_SMC & *options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t\t       (TCPOPT_EXP <<  8) |\n\t\t\t\t       (TCPOLEN_EXP_SMC_BASE));\n\t\t\t*ptr++ = htonl(TCPOPT_SMC_MAGIC);\n\t\t}\n\t}\n#endif\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smc_options_write(__be32 *ptr, u16 *options){\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (unlikely(OPTION_SMC & *options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t\t       (TCPOPT_EXP <<  8) |\n\t\t\t\t       (TCPOLEN_EXP_SMC_BASE));\n\t\t\t*ptr++ = htonl(TCPOPT_SMC_MAGIC);\n\t\t}\n\t}\n#endif\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_463": {
        "vulnerable_code": {
            "Code": "smc_set_option(const struct tcp_sock *tp,\n\t\t\t   struct tcp_out_options *opts,\n\t\t\t   unsigned int *remaining){\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc) {\n\t\t\tif (*remaining >= TCPOLEN_EXP_SMC_BASE_ALIGNED) {\n\t\t\t\topts->options |= OPTION_SMC;\n\t\t\t\t*remaining -= TCPOLEN_EXP_SMC_BASE_ALIGNED;\n\t\t\t}\n\t\t}\n\t}\n#endif\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "smc_set_option(const struct tcp_sock *tp,\n\t\t\t   struct tcp_out_options *opts,\n\t\t\t   unsigned int *remaining){\n#if IS_ENABLED(CONFIG_SMC)\n\tif (static_branch_unlikely(&tcp_have_smc)) {\n\t\tif (tp->syn_smc) {\n\t\t\tif (*remaining >= TCPOLEN_EXP_SMC_BASE_ALIGNED) {\n\t\t\t\topts->options |= OPTION_SMC;\n\t\t\t\t*remaining -= TCPOLEN_EXP_SMC_BASE_ALIGNED;\n\t\t\t}\n\t\t}\n\t}\n#endif\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_464": {
        "vulnerable_code": {
            "Code": "tcp_can_collapse(const struct sock *sk, const struct sk_buff *skb){\n\tif (tcp_skb_pcount(skb) > 1)\n\t\treturn false;\n\tif (skb_cloned(skb))\n\t\treturn false;\n\t/* Some heuristics for collapsing over SACK'd could be invented */\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\treturn false;\n\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_can_collapse(const struct sock *sk, const struct sk_buff *skb){\n\tif (tcp_skb_pcount(skb) > 1)\n\t\treturn false;\n\tif (skb_cloned(skb))\n\t\treturn false;\n\t/* Some heuristics for collapsing over SACK'd could be invented */\n\tif (TCP_SKB_CB(skb)->sacked & TCPCB_SACKED_ACKED)\n\t\treturn false;\n\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_465": {
        "vulnerable_code": {
            "Code": "tcp_chrono_set(struct tcp_sock *tp, const enum tcp_chrono new){\n\tconst u32 now = tcp_jiffies32;\n\tenum tcp_chrono old = tp->chrono_type;\n\n\tif (old > TCP_CHRONO_UNSPEC)\n\t\ttp->chrono_stat[old - 1] += now - tp->chrono_start;\n\ttp->chrono_start = now;\n\ttp->chrono_type = new;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_chrono_set(struct tcp_sock *tp, const enum tcp_chrono new){\n\tconst u32 now = tcp_jiffies32;\n\tenum tcp_chrono old = tp->chrono_type;\n\n\tif (old > TCP_CHRONO_UNSPEC)\n\t\ttp->chrono_stat[old - 1] += now - tp->chrono_start;\n\ttp->chrono_start = now;\n\ttp->chrono_type = new;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_466": {
        "vulnerable_code": {
            "Code": "tcp_chrono_start(struct sock *sk, const enum tcp_chrono type){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If there are multiple conditions worthy of tracking in a\n\t * chronograph then the highest priority enum takes precedence\n\t * over the other conditions. So that if something \"more interesting\"\n\t * starts happening, stop the previous chrono and start a new one.\n\t */\n\tif (type > tp->chrono_type)\n\t\ttcp_chrono_set(tp, type);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_chrono_start(struct sock *sk, const enum tcp_chrono type){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* If there are multiple conditions worthy of tracking in a\n\t * chronograph then the highest priority enum takes precedence\n\t * over the other conditions. So that if something \"more interesting\"\n\t * starts happening, stop the previous chrono and start a new one.\n\t */\n\tif (type > tp->chrono_type)\n\t\ttcp_chrono_set(tp, type);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_467": {
        "vulnerable_code": {
            "Code": "tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *next_skb = skb_rb_next(skb);\n\tint next_skb_size;\n\n\tnext_skb_size = next_skb->len;\n\n\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\n\n\tif (next_skb_size) {\n\t\tif (next_skb_size <= skb_availroom(skb))\n\t\t\tskb_copy_bits(next_skb, 0, skb_put(skb, next_skb_size),\n\t\t\t\t      next_skb_size);\n\t\telse if (!tcp_skb_shift(skb, next_skb, 1, next_skb_size))\n\t\t\treturn false;\n\t}\n\ttcp_highest_sack_replace(sk, next_skb, skb);\n\n\t/* Update sequence range on original skb. */\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;\n\n\t/* Merge over control information. This moves PSH/FIN etc. over */\n\tTCP_SKB_CB(skb)->tcp_flags |= TCP_SKB_CB(next_skb)->tcp_flags;\n\n\t/* All done, get rid of second SKB and account for it so\n\t * packet counting does not break.\n\t */\n\tTCP_SKB_CB(skb)->sacked |= TCP_SKB_CB(next_skb)->sacked & TCPCB_EVER_RETRANS;\n\tTCP_SKB_CB(skb)->eor = TCP_SKB_CB(next_skb)->eor;\n\n\t/* changed transmit queue under us so clear hints */\n\ttcp_clear_retrans_hints_partial(tp);\n\tif (next_skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = skb;\n\n\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\n\n\ttcp_skb_collapse_tstamp(skb, next_skb);\n\n\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_collapse_retrans(struct sock *sk, struct sk_buff *skb){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *next_skb = skb_rb_next(skb);\n\tint next_skb_size;\n\n\tnext_skb_size = next_skb->len;\n\n\tBUG_ON(tcp_skb_pcount(skb) != 1 || tcp_skb_pcount(next_skb) != 1);\n\n\tif (next_skb_size) {\n\t\tif (next_skb_size <= skb_availroom(skb))\n\t\t\tskb_copy_bits(next_skb, 0, skb_put(skb, next_skb_size),\n\t\t\t\t      next_skb_size);\n\t\telse if (!tcp_skb_shift(skb, next_skb, 1, next_skb_size))\n\t\t\treturn false;\n\t}\n\ttcp_highest_sack_replace(sk, next_skb, skb);\n\n\t/* Update sequence range on original skb. */\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(next_skb)->end_seq;\n\n\t/* Merge over control information. This moves PSH/FIN etc. over */\n\tTCP_SKB_CB(skb)->tcp_flags |= TCP_SKB_CB(next_skb)->tcp_flags;\n\n\t/* All done, get rid of second SKB and account for it so\n\t * packet counting does not break.\n\t */\n\tTCP_SKB_CB(skb)->sacked |= TCP_SKB_CB(next_skb)->sacked & TCPCB_EVER_RETRANS;\n\tTCP_SKB_CB(skb)->eor = TCP_SKB_CB(next_skb)->eor;\n\n\t/* changed transmit queue under us so clear hints */\n\ttcp_clear_retrans_hints_partial(tp);\n\tif (next_skb == tp->retransmit_skb_hint)\n\t\ttp->retransmit_skb_hint = skb;\n\n\ttcp_adjust_pcount(sk, next_skb, tcp_skb_pcount(next_skb));\n\n\ttcp_skb_collapse_tstamp(skb, next_skb);\n\n\ttcp_rtx_queue_unlink_and_free(next_skb, sk);\n\treturn true;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_468": {
        "vulnerable_code": {
            "Code": "tcp_event_data_sent(struct tcp_sock *tp,\n\t\t\t\tstruct sock *sk){\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst u32 now = tcp_jiffies32;\n\n\tif (tcp_packets_in_flight(tp) == 0)\n\t\ttcp_ca_event(sk, CA_EVENT_TX_START);\n\n\t/* If this is the first data packet sent in response to the\n\t * previous received data,\n\t * and it is a reply for ato after last received packet,\n\t * increase pingpong count.\n\t */\n\tif (before(tp->lsndtime, icsk->icsk_ack.lrcvtime) &&\n\t    (u32)(now - icsk->icsk_ack.lrcvtime) < icsk->icsk_ack.ato)\n\t\tinet_csk_inc_pingpong_cnt(sk);\n\n\ttp->lsndtime = now;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_event_data_sent(struct tcp_sock *tp,\n\t\t\t\tstruct sock *sk){\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tconst u32 now = tcp_jiffies32;\n\n\tif (tcp_packets_in_flight(tp) == 0)\n\t\ttcp_ca_event(sk, CA_EVENT_TX_START);\n\n\t/* If this is the first data packet sent in response to the\n\t * previous received data,\n\t * and it is a reply for ato after last received packet,\n\t * increase pingpong count.\n\t */\n\tif (before(tp->lsndtime, icsk->icsk_ack.lrcvtime) &&\n\t    (u32)(now - icsk->icsk_ack.lrcvtime) < icsk->icsk_ack.ato)\n\t\tinet_csk_inc_pingpong_cnt(sk);\n\n\ttp->lsndtime = now;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_469": {
        "vulnerable_code": {
            "Code": "tcp_init_tso_segs(struct sk_buff *skb, unsigned int mss_now){\n\tint tso_segs = tcp_skb_pcount(skb);\n\n\tif (!tso_segs || (tso_segs > 1 && tcp_skb_mss(skb) != mss_now)) {\n\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\ttso_segs = tcp_skb_pcount(skb);\n\t}\n\treturn tso_segs;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_init_tso_segs(struct sk_buff *skb, unsigned int mss_now){\n\tint tso_segs = tcp_skb_pcount(skb);\n\n\tif (!tso_segs || (tso_segs > 1 && tcp_skb_mss(skb) != mss_now)) {\n\t\ttcp_set_skb_tso_segs(skb, mss_now);\n\t\ttso_segs = tcp_skb_pcount(skb);\n\t}\n\treturn tso_segs;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_470": {
        "vulnerable_code": {
            "Code": "tcp_mtup_init(struct sock *sk){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\n\ticsk->icsk_mtup.enabled = net->ipv4.sysctl_tcp_mtu_probing > 1;\n\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp + sizeof(struct tcphdr) +\n\t\t\t       icsk->icsk_af_ops->net_header_len;\n\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, net->ipv4.sysctl_tcp_base_mss);\n\ticsk->icsk_mtup.probe_size = 0;\n\tif (icsk->icsk_mtup.enabled)\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_mtup_init(struct sock *sk){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\n\ticsk->icsk_mtup.enabled = net->ipv4.sysctl_tcp_mtu_probing > 1;\n\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp + sizeof(struct tcphdr) +\n\t\t\t       icsk->icsk_af_ops->net_header_len;\n\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, net->ipv4.sysctl_tcp_base_mss);\n\ticsk->icsk_mtup.probe_size = 0;\n\tif (icsk->icsk_mtup.enabled)\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_471": {
        "vulnerable_code": {
            "Code": "tcp_nagle_check(bool partial, const struct tcp_sock *tp,\n\t\t\t    int nonagle){\n\treturn partial &&\n\t\t((nonagle & TCP_NAGLE_CORK) ||\n\t\t (!nonagle && tp->packets_out && tcp_minshall_check(tp)));\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_nagle_check(bool partial, const struct tcp_sock *tp,\n\t\t\t    int nonagle){\n\treturn partial &&\n\t\t((nonagle & TCP_NAGLE_CORK) ||\n\t\t (!nonagle && tp->packets_out && tcp_minshall_check(tp)));\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 5,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_472": {
        "vulnerable_code": {
            "Code": "tcp_rtx_synack(const struct sock *sk, struct request_sock *req){\n\tconst struct tcp_request_sock_ops *af_ops = tcp_rsk(req)->af_specific;\n\tstruct flowi fl;\n\tint res;\n\n\ttcp_rsk(req)->txhash = net_tx_rndhash();\n\tres = af_ops->send_synack(sk, NULL, &fl, req, NULL, TCP_SYNACK_NORMAL);\n\tif (!res) {\n\t\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\n\t\tif (unlikely(tcp_passive_fastopen(sk)))\n\t\t\ttcp_sk(sk)->total_retrans++;\n\t\ttrace_tcp_retransmit_synack(sk, req);\n\t}\n\treturn res;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_rtx_synack(const struct sock *sk, struct request_sock *req){\n\tconst struct tcp_request_sock_ops *af_ops = tcp_rsk(req)->af_specific;\n\tstruct flowi fl;\n\tint res;\n\n\ttcp_rsk(req)->txhash = net_tx_rndhash();\n\tres = af_ops->send_synack(sk, NULL, &fl, req, NULL, TCP_SYNACK_NORMAL);\n\tif (!res) {\n\t\t__TCP_INC_STATS(sock_net(sk), TCP_MIB_RETRANSSEGS);\n\t\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSYNRETRANS);\n\t\tif (unlikely(tcp_passive_fastopen(sk)))\n\t\t\ttcp_sk(sk)->total_retrans++;\n\t\ttrace_tcp_retransmit_synack(sk, req);\n\t}\n\treturn res;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_473": {
        "vulnerable_code": {
            "Code": "tcp_send_loss_probe(struct sock *sk){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint pcount;\n\tint mss = tcp_current_mss(sk);\n\n\tskb = tcp_send_head(sk);\n\tif (skb && tcp_snd_wnd_test(tp, skb, mss)) {\n\t\tpcount = tp->packets_out;\n\t\ttcp_write_xmit(sk, mss, TCP_NAGLE_OFF, 2, GFP_ATOMIC);\n\t\tif (tp->packets_out > pcount)\n\t\t\tgoto probe_sent;\n\t\tgoto rearm_timer;\n\t}\n\tskb = skb_rb_last(&sk->tcp_rtx_queue);\n\tif (unlikely(!skb)) {\n\t\tWARN_ONCE(tp->packets_out,\n\t\t\t  \"invalid inflight: %u state %u cwnd %u mss %d\\n\",\n\t\t\t  tp->packets_out, sk->sk_state, tp->snd_cwnd, mss);\n\t\tinet_csk(sk)->icsk_pending = 0;\n\t\treturn;\n\t}\n\n\t/* At most one outstanding TLP retransmission. */\n\tif (tp->tlp_high_seq)\n\t\tgoto rearm_timer;\n\n\tif (skb_still_in_host_queue(sk, skb))\n\t\tgoto rearm_timer;\n\n\tpcount = tcp_skb_pcount(skb);\n\tif (WARN_ON(!pcount))\n\t\tgoto rearm_timer;\n\n\tif ((pcount > 1) && (skb->len > (pcount - 1) * mss)) {\n\t\tif (unlikely(tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t\t  (pcount - 1) * mss, mss,\n\t\t\t\t\t  GFP_ATOMIC)))\n\t\t\tgoto rearm_timer;\n\t\tskb = skb_rb_next(skb);\n\t}\n\n\tif (WARN_ON(!skb || !tcp_skb_pcount(skb)))\n\t\tgoto rearm_timer;\n\n\tif (__tcp_retransmit_skb(sk, skb, 1))\n\t\tgoto rearm_timer;\n\n\t/* Record snd_nxt for loss detection. */\n\ttp->tlp_high_seq = tp->snd_nxt;\n\nprobe_sent:\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSPROBES);\n\t/* Reset s.t. tcp_rearm_rto will restart timer from now */\n\tinet_csk(sk)->icsk_pending = 0;\nrearm_timer:\n\ttcp_rearm_rto(sk);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_send_loss_probe(struct sock *sk){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tint pcount;\n\tint mss = tcp_current_mss(sk);\n\n\tskb = tcp_send_head(sk);\n\tif (skb && tcp_snd_wnd_test(tp, skb, mss)) {\n\t\tpcount = tp->packets_out;\n\t\ttcp_write_xmit(sk, mss, TCP_NAGLE_OFF, 2, GFP_ATOMIC);\n\t\tif (tp->packets_out > pcount)\n\t\t\tgoto probe_sent;\n\t\tgoto rearm_timer;\n\t}\n\tskb = skb_rb_last(&sk->tcp_rtx_queue);\n\tif (unlikely(!skb)) {\n\t\tWARN_ONCE(tp->packets_out,\n\t\t\t  \"invalid inflight: %u state %u cwnd %u mss %d\\n\",\n\t\t\t  tp->packets_out, sk->sk_state, tp->snd_cwnd, mss);\n\t\tinet_csk(sk)->icsk_pending = 0;\n\t\treturn;\n\t}\n\n\t/* At most one outstanding TLP retransmission. */\n\tif (tp->tlp_high_seq)\n\t\tgoto rearm_timer;\n\n\tif (skb_still_in_host_queue(sk, skb))\n\t\tgoto rearm_timer;\n\n\tpcount = tcp_skb_pcount(skb);\n\tif (WARN_ON(!pcount))\n\t\tgoto rearm_timer;\n\n\tif ((pcount > 1) && (skb->len > (pcount - 1) * mss)) {\n\t\tif (unlikely(tcp_fragment(sk, TCP_FRAG_IN_RTX_QUEUE, skb,\n\t\t\t\t\t  (pcount - 1) * mss, mss,\n\t\t\t\t\t  GFP_ATOMIC)))\n\t\t\tgoto rearm_timer;\n\t\tskb = skb_rb_next(skb);\n\t}\n\n\tif (WARN_ON(!skb || !tcp_skb_pcount(skb)))\n\t\tgoto rearm_timer;\n\n\tif (__tcp_retransmit_skb(sk, skb, 1))\n\t\tgoto rearm_timer;\n\n\t/* Record snd_nxt for loss detection. */\n\ttp->tlp_high_seq = tp->snd_nxt;\n\nprobe_sent:\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPLOSSPROBES);\n\t/* Reset s.t. tcp_rearm_rto will restart timer from now */\n\tinet_csk(sk)->icsk_pending = 0;\nrearm_timer:\n\ttcp_rearm_rto(sk);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_474": {
        "vulnerable_code": {
            "Code": "tcp_sync_mss(struct sock *sk, u32 pmtu){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint mss_now;\n\n\tif (icsk->icsk_mtup.search_high > pmtu)\n\t\ticsk->icsk_mtup.search_high = pmtu;\n\n\tmss_now = tcp_mtu_to_mss(sk, pmtu);\n\tmss_now = tcp_bound_to_half_wnd(tp, mss_now);\n\n\t/* And store cached results */\n\ticsk->icsk_pmtu_cookie = pmtu;\n\tif (icsk->icsk_mtup.enabled)\n\t\tmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low));\n\ttp->mss_cache = mss_now;\n\n\treturn mss_now;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_sync_mss(struct sock *sk, u32 pmtu){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tint mss_now;\n\n\tif (icsk->icsk_mtup.search_high > pmtu)\n\t\ticsk->icsk_mtup.search_high = pmtu;\n\n\tmss_now = tcp_mtu_to_mss(sk, pmtu);\n\tmss_now = tcp_bound_to_half_wnd(tp, mss_now);\n\n\t/* And store cached results */\n\ticsk->icsk_pmtu_cookie = pmtu;\n\tif (icsk->icsk_mtup.enabled)\n\t\tmss_now = min(mss_now, tcp_mtu_to_mss(sk, icsk->icsk_mtup.search_low));\n\ttp->mss_cache = mss_now;\n\n\treturn mss_now;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_475": {
        "vulnerable_code": {
            "Code": "sk_forced_mem_schedule(struct sock *sk, int size){\n\tint amt;\n\n\tif (size <= sk->sk_forward_alloc)\n\t\treturn;\n\tamt = sk_mem_pages(size);\n\tsk->sk_forward_alloc += amt * SK_MEM_QUANTUM;\n\tsk_memory_allocated_add(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_charge_skmem(sk->sk_memcg, amt);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "sk_forced_mem_schedule(struct sock *sk, int size){\n\tint amt;\n\n\tif (size <= sk->sk_forward_alloc)\n\t\treturn;\n\tamt = sk_mem_pages(size);\n\tsk->sk_forward_alloc += amt * SK_MEM_QUANTUM;\n\tsk_memory_allocated_add(sk, amt);\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg)\n\t\tmem_cgroup_charge_skmem(sk->sk_memcg, amt);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_476": {
        "vulnerable_code": {
            "Code": "tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\n\t/* There are multiple conditions worthy of tracking in a\n\t * chronograph, so that the highest priority enum takes\n\t * precedence over the other conditions (see tcp_chrono_start).\n\t * If a condition stops, we only stop chrono tracking if\n\t * it's the \"most interesting\" or current chrono we are\n\t * tracking and starts busy chrono if we have pending data.\n\t */\n\tif (tcp_rtx_and_write_queues_empty(sk))\n\t\ttcp_chrono_set(tp, TCP_CHRONO_UNSPEC);\n\telse if (type == tp->chrono_type)\n\t\ttcp_chrono_set(tp, TCP_CHRONO_BUSY);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\n\t/* There are multiple conditions worthy of tracking in a\n\t * chronograph, so that the highest priority enum takes\n\t * precedence over the other conditions (see tcp_chrono_start).\n\t * If a condition stops, we only stop chrono tracking if\n\t * it's the \"most interesting\" or current chrono we are\n\t * tracking and starts busy chrono if we have pending data.\n\t */\n\tif (tcp_rtx_and_write_queues_empty(sk))\n\t\ttcp_chrono_set(tp, TCP_CHRONO_UNSPEC);\n\telse if (type == tp->chrono_type)\n\t\ttcp_chrono_set(tp, TCP_CHRONO_BUSY);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_477": {
        "vulnerable_code": {
            "Code": "tcp_connect(struct sock *sk){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint err;\n\n\ttcp_call_bpf(sk, BPF_SOCK_OPS_TCP_CONNECT_CB, 0, NULL);\n\n\tif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\n\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\n\n\ttcp_connect_init(sk);\n\n\tif (unlikely(tp->repair)) {\n\t\ttcp_finish_connect(sk, NULL);\n\t\treturn 0;\n\t}\n\n\tbuff = sk_stream_alloc_skb(sk, 0, sk->sk_allocation, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOBUFS;\n\n\ttcp_init_nondata_skb(buff, tp->write_seq++, TCPHDR_SYN);\n\ttcp_mstamp_refresh(tp);\n\ttp->retrans_stamp = tcp_time_stamp(tp);\n\ttcp_connect_queue_skb(sk, buff);\n\ttcp_ecn_send_syn(sk, buff);\n\ttcp_rbtree_insert(&sk->tcp_rtx_queue, buff);\n\n\t/* Send off SYN; include data in Fast Open. */\n\terr = tp->fastopen_req ? tcp_send_syn_data(sk, buff) :\n\t      tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);\n\tif (err == -ECONNREFUSED)\n\t\treturn err;\n\n\t/* We change tp->snd_nxt after the tcp_transmit_skb() call\n\t * in order to make this packet get counted in tcpOutSegs.\n\t */\n\ttp->snd_nxt = tp->write_seq;\n\ttp->pushed_seq = tp->write_seq;\n\tbuff = tcp_send_head(sk);\n\tif (unlikely(buff)) {\n\t\ttp->snd_nxt\t= TCP_SKB_CB(buff)->seq;\n\t\ttp->pushed_seq\t= TCP_SKB_CB(buff)->seq;\n\t}\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ACTIVEOPENS);\n\n\t/* Timer for repeating the SYN until an answer. */\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_connect(struct sock *sk){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *buff;\n\tint err;\n\n\ttcp_call_bpf(sk, BPF_SOCK_OPS_TCP_CONNECT_CB, 0, NULL);\n\n\tif (inet_csk(sk)->icsk_af_ops->rebuild_header(sk))\n\t\treturn -EHOSTUNREACH; /* Routing failure or similar. */\n\n\ttcp_connect_init(sk);\n\n\tif (unlikely(tp->repair)) {\n\t\ttcp_finish_connect(sk, NULL);\n\t\treturn 0;\n\t}\n\n\tbuff = sk_stream_alloc_skb(sk, 0, sk->sk_allocation, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOBUFS;\n\n\ttcp_init_nondata_skb(buff, tp->write_seq++, TCPHDR_SYN);\n\ttcp_mstamp_refresh(tp);\n\ttp->retrans_stamp = tcp_time_stamp(tp);\n\ttcp_connect_queue_skb(sk, buff);\n\ttcp_ecn_send_syn(sk, buff);\n\ttcp_rbtree_insert(&sk->tcp_rtx_queue, buff);\n\n\t/* Send off SYN; include data in Fast Open. */\n\terr = tp->fastopen_req ? tcp_send_syn_data(sk, buff) :\n\t      tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);\n\tif (err == -ECONNREFUSED)\n\t\treturn err;\n\n\t/* We change tp->snd_nxt after the tcp_transmit_skb() call\n\t * in order to make this packet get counted in tcpOutSegs.\n\t */\n\ttp->snd_nxt = tp->write_seq;\n\ttp->pushed_seq = tp->write_seq;\n\tbuff = tcp_send_head(sk);\n\tif (unlikely(buff)) {\n\t\ttp->snd_nxt\t= TCP_SKB_CB(buff)->seq;\n\t\ttp->pushed_seq\t= TCP_SKB_CB(buff)->seq;\n\t}\n\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ACTIVEOPENS);\n\n\t/* Timer for repeating the SYN until an answer. */\n\tinet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,\n\t\t\t\t  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_478": {
        "vulnerable_code": {
            "Code": "tcp_mstamp_refresh(struct tcp_sock *tp){\n\tu64 val = tcp_clock_ns();\n\n\ttp->tcp_clock_cache = val;\n\ttp->tcp_mstamp = div_u64(val, NSEC_PER_USEC);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_mstamp_refresh(struct tcp_sock *tp){\n\tu64 val = tcp_clock_ns();\n\n\ttp->tcp_clock_cache = val;\n\ttp->tcp_mstamp = div_u64(val, NSEC_PER_USEC);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_479": {
        "vulnerable_code": {
            "Code": "tcp_mtu_check_reprobe(struct sock *sk){\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 interval;\n\ts32 delta;\n\n\tinterval = net->ipv4.sysctl_tcp_probe_interval;\n\tdelta = tcp_jiffies32 - icsk->icsk_mtup.probe_timestamp;\n\tif (unlikely(delta >= interval * HZ)) {\n\t\tint mss = tcp_current_mss(sk);\n\n\t\t/* Update current search range */\n\t\ticsk->icsk_mtup.probe_size = 0;\n\t\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp +\n\t\t\tsizeof(struct tcphdr) +\n\t\t\ticsk->icsk_af_ops->net_header_len;\n\t\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);\n\n\t\t/* Update probe time stamp */\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_mtu_check_reprobe(struct sock *sk){\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tu32 interval;\n\ts32 delta;\n\n\tinterval = net->ipv4.sysctl_tcp_probe_interval;\n\tdelta = tcp_jiffies32 - icsk->icsk_mtup.probe_timestamp;\n\tif (unlikely(delta >= interval * HZ)) {\n\t\tint mss = tcp_current_mss(sk);\n\n\t\t/* Update current search range */\n\t\ticsk->icsk_mtup.probe_size = 0;\n\t\ticsk->icsk_mtup.search_high = tp->rx_opt.mss_clamp +\n\t\t\tsizeof(struct tcphdr) +\n\t\t\ticsk->icsk_af_ops->net_header_len;\n\t\ticsk->icsk_mtup.search_low = tcp_mss_to_mtu(sk, mss);\n\n\t\t/* Update probe time stamp */\n\t\ticsk->icsk_mtup.probe_timestamp = tcp_jiffies32;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_480": {
        "vulnerable_code": {
            "Code": "tcp_options_write(__be32 *ptr, struct tcp_sock *tp,\n\t\t\t      struct tcp_out_options *opts){\n\tu16 options = opts->options;\t/* mungable copy */\n\n\tif (unlikely(OPTION_MD5 & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\t/* overload cookie hash location */\n\t\topts->hash_location = (__u8 *)ptr;\n\t\tptr += 4;\n\t}\n\n\tif (unlikely(opts->mss)) {\n\t\t*ptr++ = htonl((TCPOPT_MSS << 24) |\n\t\t\t       (TCPOLEN_MSS << 16) |\n\t\t\t       opts->mss);\n\t}\n\n\tif (likely(OPTION_TS & options)) {\n\t\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |\n\t\t\t\t       (TCPOLEN_SACK_PERM << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t\toptions &= ~OPTION_SACK_ADVERTISE;\n\t\t} else {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t}\n\t\t*ptr++ = htonl(opts->tsval);\n\t\t*ptr++ = htonl(opts->tsecr);\n\t}\n\n\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_SACK_PERM << 8) |\n\t\t\t       TCPOLEN_SACK_PERM);\n\t}\n\n\tif (unlikely(OPTION_WSCALE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_WINDOW << 16) |\n\t\t\t       (TCPOLEN_WINDOW << 8) |\n\t\t\t       opts->ws);\n\t}\n\n\tif (unlikely(opts->num_sack_blocks)) {\n\t\tstruct tcp_sack_block *sp = tp->rx_opt.dsack ?\n\t\t\ttp->duplicate_sack : tp->selective_acks;\n\t\tint this_sack;\n\n\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t       (TCPOPT_SACK <<  8) |\n\t\t\t       (TCPOLEN_SACK_BASE + (opts->num_sack_blocks *\n\t\t\t\t\t\t     TCPOLEN_SACK_PERBLOCK)));\n\n\t\tfor (this_sack = 0; this_sack < opts->num_sack_blocks;\n\t\t     ++this_sack) {\n\t\t\t*ptr++ = htonl(sp[this_sack].start_seq);\n\t\t\t*ptr++ = htonl(sp[this_sack].end_seq);\n\t\t}\n\n\t\ttp->rx_opt.dsack = 0;\n\t}\n\n\tif (unlikely(OPTION_FAST_OPEN_COOKIE & options)) {\n\t\tstruct tcp_fastopen_cookie *foc = opts->fastopen_cookie;\n\t\tu8 *p = (u8 *)ptr;\n\t\tu32 len; /* Fast Open option length */\n\n\t\tif (foc->exp) {\n\t\t\tlen = TCPOLEN_EXP_FASTOPEN_BASE + foc->len;\n\t\t\t*ptr = htonl((TCPOPT_EXP << 24) | (len << 16) |\n\t\t\t\t     TCPOPT_FASTOPEN_MAGIC);\n\t\t\tp += TCPOLEN_EXP_FASTOPEN_BASE;\n\t\t} else {\n\t\t\tlen = TCPOLEN_FASTOPEN_BASE + foc->len;\n\t\t\t*p++ = TCPOPT_FASTOPEN;\n\t\t\t*p++ = len;\n\t\t}\n\n\t\tmemcpy(p, foc->val, foc->len);\n\t\tif ((len & 3) == 2) {\n\t\t\tp[foc->len] = TCPOPT_NOP;\n\t\t\tp[foc->len + 1] = TCPOPT_NOP;\n\t\t}\n\t\tptr += (len + 3) >> 2;\n\t}\n\n\tsmc_options_write(ptr, &options);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_options_write(__be32 *ptr, struct tcp_sock *tp,\n\t\t\t      struct tcp_out_options *opts){\n\tu16 options = opts->options;\t/* mungable copy */\n\n\tif (unlikely(OPTION_MD5 & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_MD5SIG << 8) | TCPOLEN_MD5SIG);\n\t\t/* overload cookie hash location */\n\t\topts->hash_location = (__u8 *)ptr;\n\t\tptr += 4;\n\t}\n\n\tif (unlikely(opts->mss)) {\n\t\t*ptr++ = htonl((TCPOPT_MSS << 24) |\n\t\t\t       (TCPOLEN_MSS << 16) |\n\t\t\t       opts->mss);\n\t}\n\n\tif (likely(OPTION_TS & options)) {\n\t\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t\t*ptr++ = htonl((TCPOPT_SACK_PERM << 24) |\n\t\t\t\t       (TCPOLEN_SACK_PERM << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t\toptions &= ~OPTION_SACK_ADVERTISE;\n\t\t} else {\n\t\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t\t       (TCPOPT_TIMESTAMP << 8) |\n\t\t\t\t       TCPOLEN_TIMESTAMP);\n\t\t}\n\t\t*ptr++ = htonl(opts->tsval);\n\t\t*ptr++ = htonl(opts->tsecr);\n\t}\n\n\tif (unlikely(OPTION_SACK_ADVERTISE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_NOP << 16) |\n\t\t\t       (TCPOPT_SACK_PERM << 8) |\n\t\t\t       TCPOLEN_SACK_PERM);\n\t}\n\n\tif (unlikely(OPTION_WSCALE & options)) {\n\t\t*ptr++ = htonl((TCPOPT_NOP << 24) |\n\t\t\t       (TCPOPT_WINDOW << 16) |\n\t\t\t       (TCPOLEN_WINDOW << 8) |\n\t\t\t       opts->ws);\n\t}\n\n\tif (unlikely(opts->num_sack_blocks)) {\n\t\tstruct tcp_sack_block *sp = tp->rx_opt.dsack ?\n\t\t\ttp->duplicate_sack : tp->selective_acks;\n\t\tint this_sack;\n\n\t\t*ptr++ = htonl((TCPOPT_NOP  << 24) |\n\t\t\t       (TCPOPT_NOP  << 16) |\n\t\t\t       (TCPOPT_SACK <<  8) |\n\t\t\t       (TCPOLEN_SACK_BASE + (opts->num_sack_blocks *\n\t\t\t\t\t\t     TCPOLEN_SACK_PERBLOCK)));\n\n\t\tfor (this_sack = 0; this_sack < opts->num_sack_blocks;\n\t\t     ++this_sack) {\n\t\t\t*ptr++ = htonl(sp[this_sack].start_seq);\n\t\t\t*ptr++ = htonl(sp[this_sack].end_seq);\n\t\t}\n\n\t\ttp->rx_opt.dsack = 0;\n\t}\n\n\tif (unlikely(OPTION_FAST_OPEN_COOKIE & options)) {\n\t\tstruct tcp_fastopen_cookie *foc = opts->fastopen_cookie;\n\t\tu8 *p = (u8 *)ptr;\n\t\tu32 len; /* Fast Open option length */\n\n\t\tif (foc->exp) {\n\t\t\tlen = TCPOLEN_EXP_FASTOPEN_BASE + foc->len;\n\t\t\t*ptr = htonl((TCPOPT_EXP << 24) | (len << 16) |\n\t\t\t\t     TCPOPT_FASTOPEN_MAGIC);\n\t\t\tp += TCPOLEN_EXP_FASTOPEN_BASE;\n\t\t} else {\n\t\t\tlen = TCPOLEN_FASTOPEN_BASE + foc->len;\n\t\t\t*p++ = TCPOPT_FASTOPEN;\n\t\t\t*p++ = len;\n\t\t}\n\n\t\tmemcpy(p, foc->val, foc->len);\n\t\tif ((len & 3) == 2) {\n\t\t\tp[foc->len] = TCPOPT_NOP;\n\t\t\tp[foc->len + 1] = TCPOPT_NOP;\n\t\t}\n\t\tptr += (len + 3) >> 2;\n\t}\n\n\tsmc_options_write(ptr, &options);\n}",
            "Size": 4,
            "Code Complexity": 0,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_481": {
        "vulnerable_code": {
            "Code": "tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *to,\n\t\t\t\t     int space){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb = to, *tmp;\n\tbool first = true;\n\n\tif (!sock_net(sk)->ipv4.sysctl_tcp_retrans_collapse)\n\t\treturn;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)\n\t\treturn;\n\n\tskb_rbtree_walk_from_safe(skb, tmp) {\n\t\tif (!tcp_can_collapse(sk, skb))\n\t\t\tbreak;\n\n\t\tif (!tcp_skb_can_collapse_to(to))\n\t\t\tbreak;\n\n\t\tspace -= skb->len;\n\n\t\tif (first) {\n\t\t\tfirst = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (space < 0)\n\t\t\tbreak;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, tcp_wnd_end(tp)))\n\t\t\tbreak;\n\n\t\tif (!tcp_collapse_retrans(sk, to))\n\t\t\tbreak;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_retrans_try_collapse(struct sock *sk, struct sk_buff *to,\n\t\t\t\t     int space){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb = to, *tmp;\n\tbool first = true;\n\n\tif (!sock_net(sk)->ipv4.sysctl_tcp_retrans_collapse)\n\t\treturn;\n\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)\n\t\treturn;\n\n\tskb_rbtree_walk_from_safe(skb, tmp) {\n\t\tif (!tcp_can_collapse(sk, skb))\n\t\t\tbreak;\n\n\t\tif (!tcp_skb_can_collapse_to(to))\n\t\t\tbreak;\n\n\t\tspace -= skb->len;\n\n\t\tif (first) {\n\t\t\tfirst = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (space < 0)\n\t\t\tbreak;\n\n\t\tif (after(TCP_SKB_CB(skb)->end_seq, tcp_wnd_end(tp)))\n\t\t\tbreak;\n\n\t\tif (!tcp_collapse_retrans(sk, to))\n\t\t\tbreak;\n\t}\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_482": {
        "vulnerable_code": {
            "Code": "tcp_send_ack(struct sock *sk){\n\t__tcp_send_ack(sk, tcp_sk(sk)->rcv_nxt);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_send_ack(struct sock *sk){\n\t__tcp_send_ack(sk, tcp_sk(sk)->rcv_nxt);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_483": {
        "vulnerable_code": {
            "Code": "tcp_small_queue_check(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t  unsigned int factor){\n\tunsigned long limit;\n\n\tlimit = max_t(unsigned long,\n\t\t      2 * skb->truesize,\n\t\t      sk->sk_pacing_rate >> sk->sk_pacing_shift);\n\tif (sk->sk_pacing_status == SK_PACING_NONE)\n\t\tlimit = min_t(unsigned long, limit,\n\t\t\t      sock_net(sk)->ipv4.sysctl_tcp_limit_output_bytes);\n\tlimit <<= factor;\n\n\tif (refcount_read(&sk->sk_wmem_alloc) > limit) {\n\t\t/* Always send skb if rtx queue is empty.\n\t\t * No need to wait for TX completion to call us back,\n\t\t * after softirq/tasklet schedule.\n\t\t * This helps when TX completions are delayed too much.\n\t\t */\n\t\tif (tcp_rtx_queue_empty(sk))\n\t\t\treturn false;\n\n\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t/* It is possible TX completion already happened\n\t\t * before we set TSQ_THROTTLED, so we must\n\t\t * test again the condition.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tif (refcount_read(&sk->sk_wmem_alloc) > limit)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_small_queue_check(struct sock *sk, const struct sk_buff *skb,\n\t\t\t\t  unsigned int factor){\n\tunsigned long limit;\n\n\tlimit = max_t(unsigned long,\n\t\t      2 * skb->truesize,\n\t\t      sk->sk_pacing_rate >> sk->sk_pacing_shift);\n\tif (sk->sk_pacing_status == SK_PACING_NONE)\n\t\tlimit = min_t(unsigned long, limit,\n\t\t\t      sock_net(sk)->ipv4.sysctl_tcp_limit_output_bytes);\n\tlimit <<= factor;\n\n\tif (refcount_read(&sk->sk_wmem_alloc) > limit) {\n\t\t/* Always send skb if rtx queue is empty.\n\t\t * No need to wait for TX completion to call us back,\n\t\t * after softirq/tasklet schedule.\n\t\t * This helps when TX completions are delayed too much.\n\t\t */\n\t\tif (tcp_rtx_queue_empty(sk))\n\t\t\treturn false;\n\n\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t/* It is possible TX completion already happened\n\t\t * before we set TSQ_THROTTLED, so we must\n\t\t * test again the condition.\n\t\t */\n\t\tsmp_mb__after_atomic();\n\t\tif (refcount_read(&sk->sk_wmem_alloc) > limit)\n\t\t\treturn true;\n\t}\n\treturn false;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_484": {
        "vulnerable_code": {
            "Code": "tcp_syn_options(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\tstruct tcp_md5sig_key **md5){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int remaining = MAX_TCP_OPTION_SPACE;\n\tstruct tcp_fastopen_request *fastopen = tp->fastopen_req;\n\n\t*md5 = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (static_branch_unlikely(&tcp_md5_needed) &&\n\t    rcu_access_pointer(tp->md5sig_info)) {\n\t\t*md5 = tp->af_specific->md5_lookup(sk, sk);\n\t\tif (*md5) {\n\t\t\topts->options |= OPTION_MD5;\n\t\t\tremaining -= TCPOLEN_MD5SIG_ALIGNED;\n\t\t}\n\t}\n#endif\n\n\t/* We always get an MSS option.  The option bytes which will be seen in\n\t * normal data packets should timestamps be used, must be in the MSS\n\t * advertised.  But we subtract them from tp->mss_cache so that\n\t * calculations in tcp_sendmsg are simpler etc.  So account for this\n\t * fact here if necessary.  If we don't do this correctly, as a\n\t * receiver we won't recognize data packets as being full sized when we\n\t * should, and thus we won't abide by the delayed ACK rules correctly.\n\t * SACKs don't matter, we never delay an ACK when we have any of those\n\t * going out.  */\n\topts->mss = tcp_advertise_mss(sk);\n\tremaining -= TCPOLEN_MSS_ALIGNED;\n\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_timestamps && !*md5)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = tcp_skb_timestamp(skb) + tp->tsoffset;\n\t\topts->tsecr = tp->rx_opt.ts_recent;\n\t\tremaining -= TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_window_scaling)) {\n\t\topts->ws = tp->rx_opt.rcv_wscale;\n\t\topts->options |= OPTION_WSCALE;\n\t\tremaining -= TCPOLEN_WSCALE_ALIGNED;\n\t}\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_sack)) {\n\t\topts->options |= OPTION_SACK_ADVERTISE;\n\t\tif (unlikely(!(OPTION_TS & opts->options)))\n\t\t\tremaining -= TCPOLEN_SACKPERM_ALIGNED;\n\t}\n\n\tif (fastopen && fastopen->cookie.len >= 0) {\n\t\tu32 need = fastopen->cookie.len;\n\n\t\tneed += fastopen->cookie.exp ? TCPOLEN_EXP_FASTOPEN_BASE :\n\t\t\t\t\t       TCPOLEN_FASTOPEN_BASE;\n\t\tneed = (need + 3) & ~3U;  /* Align to 32 bits */\n\t\tif (remaining >= need) {\n\t\t\topts->options |= OPTION_FAST_OPEN_COOKIE;\n\t\t\topts->fastopen_cookie = &fastopen->cookie;\n\t\t\tremaining -= need;\n\t\t\ttp->syn_fastopen = 1;\n\t\t\ttp->syn_fastopen_exp = fastopen->cookie.exp ? 1 : 0;\n\t\t}\n\t}\n\n\tsmc_set_option(tp, opts, &remaining);\n\n\treturn MAX_TCP_OPTION_SPACE - remaining;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_syn_options(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tstruct tcp_out_options *opts,\n\t\t\t\tstruct tcp_md5sig_key **md5){\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tunsigned int remaining = MAX_TCP_OPTION_SPACE;\n\tstruct tcp_fastopen_request *fastopen = tp->fastopen_req;\n\n\t*md5 = NULL;\n#ifdef CONFIG_TCP_MD5SIG\n\tif (static_branch_unlikely(&tcp_md5_needed) &&\n\t    rcu_access_pointer(tp->md5sig_info)) {\n\t\t*md5 = tp->af_specific->md5_lookup(sk, sk);\n\t\tif (*md5) {\n\t\t\topts->options |= OPTION_MD5;\n\t\t\tremaining -= TCPOLEN_MD5SIG_ALIGNED;\n\t\t}\n\t}\n#endif\n\n\t/* We always get an MSS option.  The option bytes which will be seen in\n\t * normal data packets should timestamps be used, must be in the MSS\n\t * advertised.  But we subtract them from tp->mss_cache so that\n\t * calculations in tcp_sendmsg are simpler etc.  So account for this\n\t * fact here if necessary.  If we don't do this correctly, as a\n\t * receiver we won't recognize data packets as being full sized when we\n\t * should, and thus we won't abide by the delayed ACK rules correctly.\n\t * SACKs don't matter, we never delay an ACK when we have any of those\n\t * going out.  */\n\topts->mss = tcp_advertise_mss(sk);\n\tremaining -= TCPOLEN_MSS_ALIGNED;\n\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_timestamps && !*md5)) {\n\t\topts->options |= OPTION_TS;\n\t\topts->tsval = tcp_skb_timestamp(skb) + tp->tsoffset;\n\t\topts->tsecr = tp->rx_opt.ts_recent;\n\t\tremaining -= TCPOLEN_TSTAMP_ALIGNED;\n\t}\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_window_scaling)) {\n\t\topts->ws = tp->rx_opt.rcv_wscale;\n\t\topts->options |= OPTION_WSCALE;\n\t\tremaining -= TCPOLEN_WSCALE_ALIGNED;\n\t}\n\tif (likely(sock_net(sk)->ipv4.sysctl_tcp_sack)) {\n\t\topts->options |= OPTION_SACK_ADVERTISE;\n\t\tif (unlikely(!(OPTION_TS & opts->options)))\n\t\t\tremaining -= TCPOLEN_SACKPERM_ALIGNED;\n\t}\n\n\tif (fastopen && fastopen->cookie.len >= 0) {\n\t\tu32 need = fastopen->cookie.len;\n\n\t\tneed += fastopen->cookie.exp ? TCPOLEN_EXP_FASTOPEN_BASE :\n\t\t\t\t\t       TCPOLEN_FASTOPEN_BASE;\n\t\tneed = (need + 3) & ~3U;  /* Align to 32 bits */\n\t\tif (remaining >= need) {\n\t\t\topts->options |= OPTION_FAST_OPEN_COOKIE;\n\t\t\topts->fastopen_cookie = &fastopen->cookie;\n\t\t\tremaining -= need;\n\t\t\ttp->syn_fastopen = 1;\n\t\t\ttp->syn_fastopen_exp = fastopen->cookie.exp ? 1 : 0;\n\t\t}\n\t}\n\n\tsmc_set_option(tp, opts, &remaining);\n\n\treturn MAX_TCP_OPTION_SPACE - remaining;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_485": {
        "vulnerable_code": {
            "Code": "tcp_tso_segs(struct sock *sk, unsigned int mss_now){\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tu32 min_tso, tso_segs;\n\n\tmin_tso = ca_ops->min_tso_segs ?\n\t\t\tca_ops->min_tso_segs(sk) :\n\t\t\tsock_net(sk)->ipv4.sysctl_tcp_min_tso_segs;\n\n\ttso_segs = tcp_tso_autosize(sk, mss_now, min_tso);\n\treturn min_t(u32, tso_segs, sk->sk_gso_max_segs);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_tso_segs(struct sock *sk, unsigned int mss_now){\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tu32 min_tso, tso_segs;\n\n\tmin_tso = ca_ops->min_tso_segs ?\n\t\t\tca_ops->min_tso_segs(sk) :\n\t\t\tsock_net(sk)->ipv4.sysctl_tcp_min_tso_segs;\n\n\ttso_segs = tcp_tso_autosize(sk, mss_now, min_tso);\n\treturn min_t(u32, tso_segs, sk->sk_gso_max_segs);\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_486": {
        "vulnerable_code": {
            "Code": "tcp_tsq_write(struct sock *sk){\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_FIN_WAIT1 | TCPF_CLOSING |\n\t     TCPF_CLOSE_WAIT  | TCPF_LAST_ACK)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (tp->lost_out > tp->retrans_out &&\n\t\t    tp->snd_cwnd > tcp_packets_in_flight(tp)) {\n\t\t\ttcp_mstamp_refresh(tp);\n\t\t\ttcp_xmit_retransmit_queue(sk);\n\t\t}\n\n\t\ttcp_write_xmit(sk, tcp_current_mss(sk), tp->nonagle,\n\t\t\t       0, GFP_ATOMIC);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tcp_tsq_write(struct sock *sk){\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_FIN_WAIT1 | TCPF_CLOSING |\n\t     TCPF_CLOSE_WAIT  | TCPF_LAST_ACK)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\tif (tp->lost_out > tp->retrans_out &&\n\t\t    tp->snd_cwnd > tcp_packets_in_flight(tp)) {\n\t\t\ttcp_mstamp_refresh(tp);\n\t\t\ttcp_xmit_retransmit_queue(sk);\n\t\t}\n\n\t\ttcp_write_xmit(sk, tcp_current_mss(sk), tp->nonagle,\n\t\t\t       0, GFP_ATOMIC);\n\t}\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_487": {
        "vulnerable_code": {
            "Code": "tso_fragment(struct sock *sk, struct sk_buff *skb, unsigned int len,\n\t\t\tunsigned int mss_now, gfp_t gfp){\n\tint nlen = skb->len - len;\n\tstruct sk_buff *buff;\n\tu8 flags;\n\n\t/* All of a TSO frame must be composed of paged data.  */\n\tif (skb->len != skb->data_len)\n\t\treturn tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t    skb, len, mss_now, gfp);\n\n\tbuff = sk_stream_alloc_skb(sk, 0, gfp, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOMEM;\n\n\tsk->sk_wmem_queued += buff->truesize;\n\tsk_mem_charge(sk, buff->truesize);\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t/* Correct the sequence numbers. */\n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t/* PSH and FIN should only be set in the second packet. */\n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\n\t/* This packet was never sent out yet, so no SACK bits. */\n\tTCP_SKB_CB(buff)->sacked = 0;\n\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\tskb_split(skb, buff, len);\n\ttcp_fragment_tstamp(skb, buff);\n\n\t/* Fix up tso_factor for both original and new SKB.  */\n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t/* Link BUFF into the send queue. */\n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, TCP_FRAG_IN_WRITE_QUEUE);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tso_fragment(struct sock *sk, struct sk_buff *skb, unsigned int len,\n\t\t\tunsigned int mss_now, gfp_t gfp){\n\tint nlen = skb->len - len;\n\tstruct sk_buff *buff;\n\tu8 flags;\n\n\t/* All of a TSO frame must be composed of paged data.  */\n\tif (skb->len != skb->data_len)\n\t\treturn tcp_fragment(sk, TCP_FRAG_IN_WRITE_QUEUE,\n\t\t\t\t    skb, len, mss_now, gfp);\n\n\tbuff = sk_stream_alloc_skb(sk, 0, gfp, true);\n\tif (unlikely(!buff))\n\t\treturn -ENOMEM;\n\n\tsk->sk_wmem_queued += buff->truesize;\n\tsk_mem_charge(sk, buff->truesize);\n\tbuff->truesize += nlen;\n\tskb->truesize -= nlen;\n\n\t/* Correct the sequence numbers. */\n\tTCP_SKB_CB(buff)->seq = TCP_SKB_CB(skb)->seq + len;\n\tTCP_SKB_CB(buff)->end_seq = TCP_SKB_CB(skb)->end_seq;\n\tTCP_SKB_CB(skb)->end_seq = TCP_SKB_CB(buff)->seq;\n\n\t/* PSH and FIN should only be set in the second packet. */\n\tflags = TCP_SKB_CB(skb)->tcp_flags;\n\tTCP_SKB_CB(skb)->tcp_flags = flags & ~(TCPHDR_FIN | TCPHDR_PSH);\n\tTCP_SKB_CB(buff)->tcp_flags = flags;\n\n\t/* This packet was never sent out yet, so no SACK bits. */\n\tTCP_SKB_CB(buff)->sacked = 0;\n\n\ttcp_skb_fragment_eor(skb, buff);\n\n\tbuff->ip_summed = CHECKSUM_PARTIAL;\n\tskb_split(skb, buff, len);\n\ttcp_fragment_tstamp(skb, buff);\n\n\t/* Fix up tso_factor for both original and new SKB.  */\n\ttcp_set_skb_tso_segs(skb, mss_now);\n\ttcp_set_skb_tso_segs(buff, mss_now);\n\n\t/* Link BUFF into the send queue. */\n\t__skb_header_release(buff);\n\ttcp_insert_write_queue_after(skb, buff, sk, TCP_FRAG_IN_WRITE_QUEUE);\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_488": {
        "vulnerable_code": {
            "Code": "get_ctl_value_v1(struct usb_mixer_elem_info *cval, int request,\n\t\t\t    int validx, int *value_ret){\n\tstruct snd_usb_audio *chip = cval->head.mixer->chip;\n\tunsigned char buf[2];\n\tint val_len = cval->val_type >= USB_MIXER_S16 ? 2 : 1;\n\tint timeout = 10;\n\tint idx = 0, err;\n\n\terr = snd_usb_lock_shutdown(chip);\n\tif (err < 0)\n\t\treturn -EIO;\n\n\twhile (timeout-- > 0) {\n\t\tidx = snd_usb_ctrl_intf(chip) | (cval->head.id << 8);\n\t\terr = snd_usb_ctl_msg(chip->dev, usb_rcvctrlpipe(chip->dev, 0), request,\n\t\t\t\t      USB_RECIP_INTERFACE | USB_TYPE_CLASS | USB_DIR_IN,\n\t\t\t\t      validx, idx, buf, val_len);\n\t\tif (err >= val_len) {\n\t\t\t*value_ret = convert_signed_value(cval, snd_usb_combine_bytes(buf, val_len));\n\t\t\terr = 0;\n\t\t\tgoto out;\n\t\t} else if (err == -ETIMEDOUT) {\n\t\t\tgoto out;\n\t\t}\n\t}\n\tusb_audio_dbg(chip,\n\t\t\"cannot get ctl value: req = %#x, wValue = %#x, wIndex = %#x, type = %d\\n\",\n\t\trequest, validx, idx, cval->val_type);\n\terr = -EINVAL;\n\n out:\n\tsnd_usb_unlock_shutdown(chip);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_ctl_value_v1(struct usb_mixer_elem_info *cval, int request,\n\t\t\t    int validx, int *value_ret){\n\tstruct snd_usb_audio *chip = cval->head.mixer->chip;\n\tunsigned char buf[2];\n\tint val_len = cval->val_type >= USB_MIXER_S16 ? 2 : 1;\n\tint timeout = 10;\n\tint idx = 0, err;\n\n\terr = snd_usb_lock_shutdown(chip);\n\tif (err < 0)\n\t\treturn -EIO;\n\n\twhile (timeout-- > 0) {\n\t\tidx = snd_usb_ctrl_intf(chip) | (cval->head.id << 8);\n\t\terr = snd_usb_ctl_msg(chip->dev, usb_rcvctrlpipe(chip->dev, 0), request,\n\t\t\t\t      USB_RECIP_INTERFACE | USB_TYPE_CLASS | USB_DIR_IN,\n\t\t\t\t      validx, idx, buf, val_len);\n\t\tif (err >= val_len) {\n\t\t\t*value_ret = convert_signed_value(cval, snd_usb_combine_bytes(buf, val_len));\n\t\t\terr = 0;\n\t\t\tgoto out;\n\t\t} else if (err == -ETIMEDOUT) {\n\t\t\tgoto out;\n\t\t}\n\t}\n\tusb_audio_dbg(chip,\n\t\t\"cannot get ctl value: req = %#x, wValue = %#x, wIndex = %#x, type = %d\\n\",\n\t\trequest, validx, idx, cval->val_type);\n\terr = -EINVAL;\n\n out:\n\tsnd_usb_unlock_shutdown(chip);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_489": {
        "vulnerable_code": {
            "Code": "get_ctl_value_v2(struct usb_mixer_elem_info *cval, int request,\n\t\t\t    int validx, int *value_ret){\n\tstruct snd_usb_audio *chip = cval->head.mixer->chip;\n\t/* enough space for one range */\n\tunsigned char buf[sizeof(__u16) + 3 * sizeof(__u32)];\n\tunsigned char *val;\n\tint idx = 0, ret, val_size, size;\n\t__u8 bRequest;\n\n\tval_size = uac2_ctl_value_size(cval->val_type);\n\n\tif (request == UAC_GET_CUR) {\n\t\tbRequest = UAC2_CS_CUR;\n\t\tsize = val_size;\n\t} else {\n\t\tbRequest = UAC2_CS_RANGE;\n\t\tsize = sizeof(__u16) + 3 * val_size;\n\t}\n\n\tmemset(buf, 0, sizeof(buf));\n\n\tret = snd_usb_lock_shutdown(chip) ? -EIO : 0;\n\tif (ret)\n\t\tgoto error;\n\n\tidx = snd_usb_ctrl_intf(chip) | (cval->head.id << 8);\n\tret = snd_usb_ctl_msg(chip->dev, usb_rcvctrlpipe(chip->dev, 0), bRequest,\n\t\t\t      USB_RECIP_INTERFACE | USB_TYPE_CLASS | USB_DIR_IN,\n\t\t\t      validx, idx, buf, size);\n\tsnd_usb_unlock_shutdown(chip);\n\n\tif (ret < 0) {\nerror:\n\t\tusb_audio_err(chip,\n\t\t\t\"cannot get ctl value: req = %#x, wValue = %#x, wIndex = %#x, type = %d\\n\",\n\t\t\trequest, validx, idx, cval->val_type);\n\t\treturn ret;\n\t}\n\n\t/* FIXME: how should we handle multiple triplets here? */\n\n\tswitch (request) {\n\tcase UAC_GET_CUR:\n\t\tval = buf;\n\t\tbreak;\n\tcase UAC_GET_MIN:\n\t\tval = buf + sizeof(__u16);\n\t\tbreak;\n\tcase UAC_GET_MAX:\n\t\tval = buf + sizeof(__u16) + val_size;\n\t\tbreak;\n\tcase UAC_GET_RES:\n\t\tval = buf + sizeof(__u16) + val_size * 2;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t*value_ret = convert_signed_value(cval,\n\t\t\t\t\t  snd_usb_combine_bytes(val, val_size));\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 3,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_ctl_value_v2(struct usb_mixer_elem_info *cval, int request,\n\t\t\t    int validx, int *value_ret){\n\tstruct snd_usb_audio *chip = cval->head.mixer->chip;\n\t/* enough space for one range */\n\tunsigned char buf[sizeof(__u16) + 3 * sizeof(__u32)];\n\tunsigned char *val;\n\tint idx = 0, ret, val_size, size;\n\t__u8 bRequest;\n\n\tval_size = uac2_ctl_value_size(cval->val_type);\n\n\tif (request == UAC_GET_CUR) {\n\t\tbRequest = UAC2_CS_CUR;\n\t\tsize = val_size;\n\t} else {\n\t\tbRequest = UAC2_CS_RANGE;\n\t\tsize = sizeof(__u16) + 3 * val_size;\n\t}\n\n\tmemset(buf, 0, sizeof(buf));\n\n\tret = snd_usb_lock_shutdown(chip) ? -EIO : 0;\n\tif (ret)\n\t\tgoto error;\n\n\tidx = snd_usb_ctrl_intf(chip) | (cval->head.id << 8);\n\tret = snd_usb_ctl_msg(chip->dev, usb_rcvctrlpipe(chip->dev, 0), bRequest,\n\t\t\t      USB_RECIP_INTERFACE | USB_TYPE_CLASS | USB_DIR_IN,\n\t\t\t      validx, idx, buf, size);\n\tsnd_usb_unlock_shutdown(chip);\n\n\tif (ret < 0) {\nerror:\n\t\tusb_audio_err(chip,\n\t\t\t\"cannot get ctl value: req = %#x, wValue = %#x, wIndex = %#x, type = %d\\n\",\n\t\t\trequest, validx, idx, cval->val_type);\n\t\treturn ret;\n\t}\n\n\t/* FIXME: how should we handle multiple triplets here? */\n\n\tswitch (request) {\n\tcase UAC_GET_CUR:\n\t\tval = buf;\n\t\tbreak;\n\tcase UAC_GET_MIN:\n\t\tval = buf + sizeof(__u16);\n\t\tbreak;\n\tcase UAC_GET_MAX:\n\t\tval = buf + sizeof(__u16) + val_size;\n\t\tbreak;\n\tcase UAC_GET_RES:\n\t\tval = buf + sizeof(__u16) + val_size * 2;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t*value_ret = convert_signed_value(cval,\n\t\t\t\t\t  snd_usb_combine_bytes(val, val_size));\n\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_490": {
        "vulnerable_code": {
            "Code": "keep_iface_ctl_get(struct snd_kcontrol *kcontrol,\n\t\t\t      struct snd_ctl_elem_value *ucontrol){\n\tstruct usb_mixer_interface *mixer = snd_kcontrol_chip(kcontrol);\n\n\tucontrol->value.integer.value[0] = mixer->chip->keep_iface;\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "keep_iface_ctl_get(struct snd_kcontrol *kcontrol,\n\t\t\t      struct snd_ctl_elem_value *ucontrol){\n\tstruct usb_mixer_interface *mixer = snd_kcontrol_chip(kcontrol);\n\n\tucontrol->value.integer.value[0] = mixer->chip->keep_iface;\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_491": {
        "vulnerable_code": {
            "Code": "mixer_ctl_feature_info(struct snd_kcontrol *kcontrol,\n\t\t\t\t  struct snd_ctl_elem_info *uinfo){\n\tstruct usb_mixer_elem_info *cval = kcontrol->private_data;\n\n\tif (cval->val_type == USB_MIXER_BOOLEAN ||\n\t    cval->val_type == USB_MIXER_INV_BOOLEAN)\n\t\tuinfo->type = SNDRV_CTL_ELEM_TYPE_BOOLEAN;\n\telse\n\t\tuinfo->type = SNDRV_CTL_ELEM_TYPE_INTEGER;\n\tuinfo->count = cval->channels;\n\tif (cval->val_type == USB_MIXER_BOOLEAN ||\n\t    cval->val_type == USB_MIXER_INV_BOOLEAN) {\n\t\tuinfo->value.integer.min = 0;\n\t\tuinfo->value.integer.max = 1;\n\t} else {\n\t\tif (!cval->initialized) {\n\t\t\tget_min_max_with_quirks(cval, 0, kcontrol);\n\t\t\tif (cval->initialized && cval->dBmin >= cval->dBmax) {\n\t\t\t\tkcontrol->vd[0].access &= \n\t\t\t\t\t~(SNDRV_CTL_ELEM_ACCESS_TLV_READ |\n\t\t\t\t\t  SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK);\n\t\t\t\tsnd_ctl_notify(cval->head.mixer->chip->card,\n\t\t\t\t\t       SNDRV_CTL_EVENT_MASK_INFO,\n\t\t\t\t\t       &kcontrol->id);\n\t\t\t}\n\t\t}\n\t\tuinfo->value.integer.min = 0;\n\t\tuinfo->value.integer.max =\n\t\t\t(cval->max - cval->min + cval->res - 1) / cval->res;\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "mixer_ctl_feature_info(struct snd_kcontrol *kcontrol,\n\t\t\t\t  struct snd_ctl_elem_info *uinfo){\n\tstruct usb_mixer_elem_info *cval = kcontrol->private_data;\n\n\tif (cval->val_type == USB_MIXER_BOOLEAN ||\n\t    cval->val_type == USB_MIXER_INV_BOOLEAN)\n\t\tuinfo->type = SNDRV_CTL_ELEM_TYPE_BOOLEAN;\n\telse\n\t\tuinfo->type = SNDRV_CTL_ELEM_TYPE_INTEGER;\n\tuinfo->count = cval->channels;\n\tif (cval->val_type == USB_MIXER_BOOLEAN ||\n\t    cval->val_type == USB_MIXER_INV_BOOLEAN) {\n\t\tuinfo->value.integer.min = 0;\n\t\tuinfo->value.integer.max = 1;\n\t} else {\n\t\tif (!cval->initialized) {\n\t\t\tget_min_max_with_quirks(cval, 0, kcontrol);\n\t\t\tif (cval->initialized && cval->dBmin >= cval->dBmax) {\n\t\t\t\tkcontrol->vd[0].access &= \n\t\t\t\t\t~(SNDRV_CTL_ELEM_ACCESS_TLV_READ |\n\t\t\t\t\t  SNDRV_CTL_ELEM_ACCESS_TLV_CALLBACK);\n\t\t\t\tsnd_ctl_notify(cval->head.mixer->chip->card,\n\t\t\t\t\t       SNDRV_CTL_EVENT_MASK_INFO,\n\t\t\t\t\t       &kcontrol->id);\n\t\t\t}\n\t\t}\n\t\tuinfo->value.integer.min = 0;\n\t\tuinfo->value.integer.max =\n\t\t\t(cval->max - cval->min + cval->res - 1) / cval->res;\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_492": {
        "vulnerable_code": {
            "Code": "mixer_ctl_selector_info(struct snd_kcontrol *kcontrol,\n\t\t\t\t   struct snd_ctl_elem_info *uinfo){\n\tstruct usb_mixer_elem_info *cval = kcontrol->private_data;\n\tconst char **itemlist = (const char **)kcontrol->private_value;\n\n\tif (snd_BUG_ON(!itemlist))\n\t\treturn -EINVAL;\n\treturn snd_ctl_enum_info(uinfo, 1, cval->max, itemlist);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "mixer_ctl_selector_info(struct snd_kcontrol *kcontrol,\n\t\t\t\t   struct snd_ctl_elem_info *uinfo){\n\tstruct usb_mixer_elem_info *cval = kcontrol->private_data;\n\tconst char **itemlist = (const char **)kcontrol->private_value;\n\n\tif (snd_BUG_ON(!itemlist))\n\t\treturn -EINVAL;\n\treturn snd_ctl_enum_info(uinfo, 1, cval->max, itemlist);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_493": {
        "vulnerable_code": {
            "Code": "mixer_ctl_selector_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t  struct snd_ctl_elem_value *ucontrol){\n\tstruct usb_mixer_elem_info *cval = kcontrol->private_data;\n\tint val, oval, err;\n\n\terr = get_cur_ctl_value(cval, cval->control << 8, &oval);\n\tif (err < 0)\n\t\treturn filter_error(cval, err);\n\tval = ucontrol->value.enumerated.item[0];\n\tval = get_abs_value(cval, val);\n\tif (val != oval) {\n\t\tset_cur_ctl_value(cval, cval->control << 8, val);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "mixer_ctl_selector_put(struct snd_kcontrol *kcontrol,\n\t\t\t\t  struct snd_ctl_elem_value *ucontrol){\n\tstruct usb_mixer_elem_info *cval = kcontrol->private_data;\n\tint val, oval, err;\n\n\terr = get_cur_ctl_value(cval, cval->control << 8, &oval);\n\tif (err < 0)\n\t\treturn filter_error(cval, err);\n\tval = ucontrol->value.enumerated.item[0];\n\tval = get_abs_value(cval, val);\n\tif (val != oval) {\n\t\tset_cur_ctl_value(cval, cval->control << 8, val);\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_494": {
        "vulnerable_code": {
            "Code": "parse_audio_feature_unit(struct mixer_build *state, int unitid,\n\t\t\t\t    void *_ftr){\n\tint channels, i, j;\n\tstruct usb_audio_term iterm;\n\tunsigned int master_bits;\n\tint err, csize;\n\tstruct uac_feature_unit_descriptor *hdr = _ftr;\n\t__u8 *bmaControls;\n\n\tif (state->mixer->protocol == UAC_VERSION_1) {\n\t\tif (hdr->bLength < 7) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcsize = hdr->bControlSize;\n\t\tif (!csize) {\n\t\t\tusb_audio_dbg(state->chip,\n\t\t\t\t      \"unit %u: invalid bControlSize == 0\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tchannels = (hdr->bLength - 7) / csize - 1;\n\t\tbmaControls = hdr->bmaControls;\n\t\tif (hdr->bLength < 7 + csize) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (state->mixer->protocol == UAC_VERSION_2) {\n\t\tstruct uac2_feature_unit_descriptor *ftr = _ftr;\n\t\tif (hdr->bLength < 6) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcsize = 4;\n\t\tchannels = (hdr->bLength - 6) / 4 - 1;\n\t\tbmaControls = ftr->bmaControls;\n\t\tif (hdr->bLength < 6 + csize) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else { /* UAC_VERSION_3 */\n\t\tstruct uac3_feature_unit_descriptor *ftr = _ftr;\n\n\t\tif (hdr->bLength < 7) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC3_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcsize = 4;\n\t\tchannels = (ftr->bLength - 7) / 4 - 1;\n\t\tbmaControls = ftr->bmaControls;\n\t\tif (hdr->bLength < 7 + csize) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC3_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* parse the source unit */\n\terr = parse_audio_unit(state, hdr->bSourceID);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* determine the input source type and name */\n\terr = check_input_term(state, hdr->bSourceID, &iterm);\n\tif (err < 0)\n\t\treturn err;\n\n\tmaster_bits = snd_usb_combine_bytes(bmaControls, csize);\n\t/* master configuration quirks */\n\tswitch (state->chip->usb_id) {\n\tcase USB_ID(0x08bb, 0x2702):\n\t\tusb_audio_info(state->chip,\n\t\t\t       \"usbmixer: master volume quirk for PCM2702 chip\\n\");\n\t\t/* disable non-functional volume control */\n\t\tmaster_bits &= ~UAC_CONTROL_BIT(UAC_FU_VOLUME);\n\t\tbreak;\n\tcase USB_ID(0x1130, 0xf211):\n\t\tusb_audio_info(state->chip,\n\t\t\t       \"usbmixer: volume control quirk for Tenx TP6911 Audio Headset\\n\");\n\t\t/* disable non-functional volume control */\n\t\tchannels = 0;\n\t\tbreak;\n\n\t}\n\n\tif (state->mixer->protocol == UAC_VERSION_1) {\n\t\t/* check all control types */\n\t\tfor (i = 0; i < 10; i++) {\n\t\t\tunsigned int ch_bits = 0;\n\t\t\tint control = audio_feature_info[i].control;\n\n\t\t\tfor (j = 0; j < channels; j++) {\n\t\t\t\tunsigned int mask;\n\n\t\t\t\tmask = snd_usb_combine_bytes(bmaControls +\n\t\t\t\t\t\t\t     csize * (j+1), csize);\n\t\t\t\tif (mask & (1 << i))\n\t\t\t\t\tch_bits |= (1 << j);\n\t\t\t}\n\t\t\t/* audio class v1 controls are never read-only */\n\n\t\t\t/*\n\t\t\t * The first channel must be set\n\t\t\t * (for ease of programming).\n\t\t\t */\n\t\t\tif (ch_bits & 1)\n\t\t\t\tbuild_feature_ctl(state, _ftr, ch_bits, control,\n\t\t\t\t\t\t  &iterm, unitid, 0);\n\t\t\tif (master_bits & (1 << i))\n\t\t\t\tbuild_feature_ctl(state, _ftr, 0, control,\n\t\t\t\t\t\t  &iterm, unitid, 0);\n\t\t}\n\t} else { /* UAC_VERSION_2/3 */\n\t\tfor (i = 0; i < ARRAY_SIZE(audio_feature_info); i++) {\n\t\t\tunsigned int ch_bits = 0;\n\t\t\tunsigned int ch_read_only = 0;\n\t\t\tint control = audio_feature_info[i].control;\n\n\t\t\tfor (j = 0; j < channels; j++) {\n\t\t\t\tunsigned int mask;\n\n\t\t\t\tmask = snd_usb_combine_bytes(bmaControls +\n\t\t\t\t\t\t\t     csize * (j+1), csize);\n\t\t\t\tif (uac_v2v3_control_is_readable(mask, control)) {\n\t\t\t\t\tch_bits |= (1 << j);\n\t\t\t\t\tif (!uac_v2v3_control_is_writeable(mask, control))\n\t\t\t\t\t\tch_read_only |= (1 << j);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * NOTE: build_feature_ctl() will mark the control\n\t\t\t * read-only if all channels are marked read-only in\n\t\t\t * the descriptors. Otherwise, the control will be\n\t\t\t * reported as writeable, but the driver will not\n\t\t\t * actually issue a write command for read-only\n\t\t\t * channels.\n\t\t\t */\n\n\t\t\t/*\n\t\t\t * The first channel must be set\n\t\t\t * (for ease of programming).\n\t\t\t */\n\t\t\tif (ch_bits & 1)\n\t\t\t\tbuild_feature_ctl(state, _ftr, ch_bits, control,\n\t\t\t\t\t\t  &iterm, unitid, ch_read_only);\n\t\t\tif (uac_v2v3_control_is_readable(master_bits, control))\n\t\t\t\tbuild_feature_ctl(state, _ftr, 0, control,\n\t\t\t\t\t\t  &iterm, unitid,\n\t\t\t\t\t\t  !uac_v2v3_control_is_writeable(master_bits,\n\t\t\t\t\t\t\t\t\t\t control));\n\t\t}\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "parse_audio_feature_unit(struct mixer_build *state, int unitid,\n\t\t\t\t    void *_ftr){\n\tint channels, i, j;\n\tstruct usb_audio_term iterm;\n\tunsigned int master_bits;\n\tint err, csize;\n\tstruct uac_feature_unit_descriptor *hdr = _ftr;\n\t__u8 *bmaControls;\n\n\tif (state->mixer->protocol == UAC_VERSION_1) {\n\t\tif (hdr->bLength < 7) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcsize = hdr->bControlSize;\n\t\tif (!csize) {\n\t\t\tusb_audio_dbg(state->chip,\n\t\t\t\t      \"unit %u: invalid bControlSize == 0\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tchannels = (hdr->bLength - 7) / csize - 1;\n\t\tbmaControls = hdr->bmaControls;\n\t\tif (hdr->bLength < 7 + csize) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (state->mixer->protocol == UAC_VERSION_2) {\n\t\tstruct uac2_feature_unit_descriptor *ftr = _ftr;\n\t\tif (hdr->bLength < 6) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcsize = 4;\n\t\tchannels = (hdr->bLength - 6) / 4 - 1;\n\t\tbmaControls = ftr->bmaControls;\n\t\tif (hdr->bLength < 6 + csize) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else { /* UAC_VERSION_3 */\n\t\tstruct uac3_feature_unit_descriptor *ftr = _ftr;\n\n\t\tif (hdr->bLength < 7) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC3_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcsize = 4;\n\t\tchannels = (ftr->bLength - 7) / 4 - 1;\n\t\tbmaControls = ftr->bmaControls;\n\t\tif (hdr->bLength < 7 + csize) {\n\t\t\tusb_audio_err(state->chip,\n\t\t\t\t      \"unit %u: invalid UAC3_FEATURE_UNIT descriptor\\n\",\n\t\t\t\t      unitid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t/* parse the source unit */\n\terr = parse_audio_unit(state, hdr->bSourceID);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* determine the input source type and name */\n\terr = check_input_term(state, hdr->bSourceID, &iterm);\n\tif (err < 0)\n\t\treturn err;\n\n\tmaster_bits = snd_usb_combine_bytes(bmaControls, csize);\n\t/* master configuration quirks */\n\tswitch (state->chip->usb_id) {\n\tcase USB_ID(0x08bb, 0x2702):\n\t\tusb_audio_info(state->chip,\n\t\t\t       \"usbmixer: master volume quirk for PCM2702 chip\\n\");\n\t\t/* disable non-functional volume control */\n\t\tmaster_bits &= ~UAC_CONTROL_BIT(UAC_FU_VOLUME);\n\t\tbreak;\n\tcase USB_ID(0x1130, 0xf211):\n\t\tusb_audio_info(state->chip,\n\t\t\t       \"usbmixer: volume control quirk for Tenx TP6911 Audio Headset\\n\");\n\t\t/* disable non-functional volume control */\n\t\tchannels = 0;\n\t\tbreak;\n\n\t}\n\n\tif (state->mixer->protocol == UAC_VERSION_1) {\n\t\t/* check all control types */\n\t\tfor (i = 0; i < 10; i++) {\n\t\t\tunsigned int ch_bits = 0;\n\t\t\tint control = audio_feature_info[i].control;\n\n\t\t\tfor (j = 0; j < channels; j++) {\n\t\t\t\tunsigned int mask;\n\n\t\t\t\tmask = snd_usb_combine_bytes(bmaControls +\n\t\t\t\t\t\t\t     csize * (j+1), csize);\n\t\t\t\tif (mask & (1 << i))\n\t\t\t\t\tch_bits |= (1 << j);\n\t\t\t}\n\t\t\t/* audio class v1 controls are never read-only */\n\n\t\t\t/*\n\t\t\t * The first channel must be set\n\t\t\t * (for ease of programming).\n\t\t\t */\n\t\t\tif (ch_bits & 1)\n\t\t\t\tbuild_feature_ctl(state, _ftr, ch_bits, control,\n\t\t\t\t\t\t  &iterm, unitid, 0);\n\t\t\tif (master_bits & (1 << i))\n\t\t\t\tbuild_feature_ctl(state, _ftr, 0, control,\n\t\t\t\t\t\t  &iterm, unitid, 0);\n\t\t}\n\t} else { /* UAC_VERSION_2/3 */\n\t\tfor (i = 0; i < ARRAY_SIZE(audio_feature_info); i++) {\n\t\t\tunsigned int ch_bits = 0;\n\t\t\tunsigned int ch_read_only = 0;\n\t\t\tint control = audio_feature_info[i].control;\n\n\t\t\tfor (j = 0; j < channels; j++) {\n\t\t\t\tunsigned int mask;\n\n\t\t\t\tmask = snd_usb_combine_bytes(bmaControls +\n\t\t\t\t\t\t\t     csize * (j+1), csize);\n\t\t\t\tif (uac_v2v3_control_is_readable(mask, control)) {\n\t\t\t\t\tch_bits |= (1 << j);\n\t\t\t\t\tif (!uac_v2v3_control_is_writeable(mask, control))\n\t\t\t\t\t\tch_read_only |= (1 << j);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * NOTE: build_feature_ctl() will mark the control\n\t\t\t * read-only if all channels are marked read-only in\n\t\t\t * the descriptors. Otherwise, the control will be\n\t\t\t * reported as writeable, but the driver will not\n\t\t\t * actually issue a write command for read-only\n\t\t\t * channels.\n\t\t\t */\n\n\t\t\t/*\n\t\t\t * The first channel must be set\n\t\t\t * (for ease of programming).\n\t\t\t */\n\t\t\tif (ch_bits & 1)\n\t\t\t\tbuild_feature_ctl(state, _ftr, ch_bits, control,\n\t\t\t\t\t\t  &iterm, unitid, ch_read_only);\n\t\t\tif (uac_v2v3_control_is_readable(master_bits, control))\n\t\t\t\tbuild_feature_ctl(state, _ftr, 0, control,\n\t\t\t\t\t\t  &iterm, unitid,\n\t\t\t\t\t\t  !uac_v2v3_control_is_writeable(master_bits,\n\t\t\t\t\t\t\t\t\t\t control));\n\t\t}\n\t}\n\n\treturn 0;\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_495": {
        "vulnerable_code": {
            "Code": "snd_usb_copy_string_desc(struct snd_usb_audio *chip,\n\t\t\t\t    int index, char *buf, int maxlen){\n\tint len = usb_string(chip->dev, index, buf, maxlen - 1);\n\n\tif (len < 0)\n\t\treturn 0;\n\n\tbuf[len] = 0;\n\treturn len;\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_usb_copy_string_desc(struct snd_usb_audio *chip,\n\t\t\t\t    int index, char *buf, int maxlen){\n\tint len = usb_string(chip->dev, index, buf, maxlen - 1);\n\n\tif (len < 0)\n\t\treturn 0;\n\n\tbuf[len] = 0;\n\treturn len;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_496": {
        "vulnerable_code": {
            "Code": "snd_usb_create_mixer(struct snd_usb_audio *chip, int ctrlif,\n\t\t\t int ignore_error){\n\tstatic struct snd_device_ops dev_ops = {\n\t\t.dev_free = snd_usb_mixer_dev_free\n\t};\n\tstruct usb_mixer_interface *mixer;\n\tint err;\n\n\tstrcpy(chip->card->mixername, \"USB Mixer\");\n\n\tmixer = kzalloc(sizeof(*mixer), GFP_KERNEL);\n\tif (!mixer)\n\t\treturn -ENOMEM;\n\tmixer->chip = chip;\n\tmixer->ignore_ctl_error = ignore_error;\n\tmixer->id_elems = kcalloc(MAX_ID_ELEMS, sizeof(*mixer->id_elems),\n\t\t\t\t  GFP_KERNEL);\n\tif (!mixer->id_elems) {\n\t\tkfree(mixer);\n\t\treturn -ENOMEM;\n\t}\n\n\tmixer->hostif = &usb_ifnum_to_if(chip->dev, ctrlif)->altsetting[0];\n\tswitch (get_iface_desc(mixer->hostif)->bInterfaceProtocol) {\n\tcase UAC_VERSION_1:\n\tdefault:\n\t\tmixer->protocol = UAC_VERSION_1;\n\t\tbreak;\n\tcase UAC_VERSION_2:\n\t\tmixer->protocol = UAC_VERSION_2;\n\t\tbreak;\n\tcase UAC_VERSION_3:\n\t\tmixer->protocol = UAC_VERSION_3;\n\t\tbreak;\n\t}\n\n\tif (mixer->protocol == UAC_VERSION_3 &&\n\t\t\tchip->badd_profile >= UAC3_FUNCTION_SUBCLASS_GENERIC_IO) {\n\t\terr = snd_usb_mixer_controls_badd(mixer, ctrlif);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t} else {\n\t\terr = snd_usb_mixer_controls(mixer);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t}\n\n\terr = snd_usb_mixer_status_create(mixer);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = create_keep_iface_ctl(mixer);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = snd_usb_mixer_apply_create_quirk(mixer);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = snd_device_new(chip->card, SNDRV_DEV_CODEC, mixer, &dev_ops);\n\tif (err < 0)\n\t\tgoto _error;\n\n\tif (list_empty(&chip->mixer_list))\n\t\tsnd_card_ro_proc_new(chip->card, \"usbmixer\", chip,\n\t\t\t\t     snd_usb_mixer_proc_read);\n\n\tlist_add(&mixer->list, &chip->mixer_list);\n\treturn 0;\n\n_error:\n\tsnd_usb_mixer_free(mixer);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_usb_create_mixer(struct snd_usb_audio *chip, int ctrlif,\n\t\t\t int ignore_error){\n\tstatic struct snd_device_ops dev_ops = {\n\t\t.dev_free = snd_usb_mixer_dev_free\n\t};\n\tstruct usb_mixer_interface *mixer;\n\tint err;\n\n\tstrcpy(chip->card->mixername, \"USB Mixer\");\n\n\tmixer = kzalloc(sizeof(*mixer), GFP_KERNEL);\n\tif (!mixer)\n\t\treturn -ENOMEM;\n\tmixer->chip = chip;\n\tmixer->ignore_ctl_error = ignore_error;\n\tmixer->id_elems = kcalloc(MAX_ID_ELEMS, sizeof(*mixer->id_elems),\n\t\t\t\t  GFP_KERNEL);\n\tif (!mixer->id_elems) {\n\t\tkfree(mixer);\n\t\treturn -ENOMEM;\n\t}\n\n\tmixer->hostif = &usb_ifnum_to_if(chip->dev, ctrlif)->altsetting[0];\n\tswitch (get_iface_desc(mixer->hostif)->bInterfaceProtocol) {\n\tcase UAC_VERSION_1:\n\tdefault:\n\t\tmixer->protocol = UAC_VERSION_1;\n\t\tbreak;\n\tcase UAC_VERSION_2:\n\t\tmixer->protocol = UAC_VERSION_2;\n\t\tbreak;\n\tcase UAC_VERSION_3:\n\t\tmixer->protocol = UAC_VERSION_3;\n\t\tbreak;\n\t}\n\n\tif (mixer->protocol == UAC_VERSION_3 &&\n\t\t\tchip->badd_profile >= UAC3_FUNCTION_SUBCLASS_GENERIC_IO) {\n\t\terr = snd_usb_mixer_controls_badd(mixer, ctrlif);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t} else {\n\t\terr = snd_usb_mixer_controls(mixer);\n\t\tif (err < 0)\n\t\t\tgoto _error;\n\t}\n\n\terr = snd_usb_mixer_status_create(mixer);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = create_keep_iface_ctl(mixer);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = snd_usb_mixer_apply_create_quirk(mixer);\n\tif (err < 0)\n\t\tgoto _error;\n\n\terr = snd_device_new(chip->card, SNDRV_DEV_CODEC, mixer, &dev_ops);\n\tif (err < 0)\n\t\tgoto _error;\n\n\tif (list_empty(&chip->mixer_list))\n\t\tsnd_card_ro_proc_new(chip->card, \"usbmixer\", chip,\n\t\t\t\t     snd_usb_mixer_proc_read);\n\n\tlist_add(&mixer->list, &chip->mixer_list);\n\treturn 0;\n\n_error:\n\tsnd_usb_mixer_free(mixer);\n\treturn err;\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_497": {
        "vulnerable_code": {
            "Code": "snd_usb_mixer_free(struct usb_mixer_interface *mixer){\n\t/* kill pending URBs */\n\tsnd_usb_mixer_disconnect(mixer);\n\n\tkfree(mixer->id_elems);\n\tif (mixer->urb) {\n\t\tkfree(mixer->urb->transfer_buffer);\n\t\tusb_free_urb(mixer->urb);\n\t}\n\tusb_free_urb(mixer->rc_urb);\n\tkfree(mixer->rc_setup_packet);\n\tkfree(mixer);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_usb_mixer_free(struct usb_mixer_interface *mixer){\n\t/* kill pending URBs */\n\tsnd_usb_mixer_disconnect(mixer);\n\n\tkfree(mixer->id_elems);\n\tif (mixer->urb) {\n\t\tkfree(mixer->urb->transfer_buffer);\n\t\tusb_free_urb(mixer->urb);\n\t}\n\tusb_free_urb(mixer->rc_urb);\n\tkfree(mixer->rc_setup_packet);\n\tkfree(mixer);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_498": {
        "vulnerable_code": {
            "Code": "snd_usb_mixer_interrupt_v2(struct usb_mixer_interface *mixer,\n\t\t\t\t       int attribute, int value, int index){\n\tstruct usb_mixer_elem_list *list;\n\t__u8 unitid = (index >> 8) & 0xff;\n\t__u8 control = (value >> 8) & 0xff;\n\t__u8 channel = value & 0xff;\n\tunsigned int count = 0;\n\n\tif (channel >= MAX_CHANNELS) {\n\t\tusb_audio_dbg(mixer->chip,\n\t\t\t\"%s(): bogus channel number %d\\n\",\n\t\t\t__func__, channel);\n\t\treturn;\n\t}\n\n\tfor_each_mixer_elem(list, mixer, unitid)\n\t\tcount++;\n\n\tif (count == 0)\n\t\treturn;\n\n\tfor_each_mixer_elem(list, mixer, unitid) {\n\t\tstruct usb_mixer_elem_info *info;\n\n\t\tif (!list->kctl)\n\t\t\tcontinue;\n\n\t\tinfo = mixer_elem_list_to_info(list);\n\t\tif (count > 1 && info->control != control)\n\t\t\tcontinue;\n\n\t\tswitch (attribute) {\n\t\tcase UAC2_CS_CUR:\n\t\t\t/* invalidate cache, so the value is read from the device */\n\t\t\tif (channel)\n\t\t\t\tinfo->cached &= ~(1 << channel);\n\t\t\telse /* master channel */\n\t\t\t\tinfo->cached = 0;\n\n\t\t\tsnd_ctl_notify(mixer->chip->card, SNDRV_CTL_EVENT_MASK_VALUE,\n\t\t\t\t       &info->head.kctl->id);\n\t\t\tbreak;\n\n\t\tcase UAC2_CS_RANGE:\n\t\t\t/* TODO */\n\t\t\tbreak;\n\n\t\tcase UAC2_CS_MEM:\n\t\t\t/* TODO */\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tusb_audio_dbg(mixer->chip,\n\t\t\t\t\"unknown attribute %d in interrupt\\n\",\n\t\t\t\tattribute);\n\t\t\tbreak;\n\t\t} /* switch */\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "snd_usb_mixer_interrupt_v2(struct usb_mixer_interface *mixer,\n\t\t\t\t       int attribute, int value, int index){\n\tstruct usb_mixer_elem_list *list;\n\t__u8 unitid = (index >> 8) & 0xff;\n\t__u8 control = (value >> 8) & 0xff;\n\t__u8 channel = value & 0xff;\n\tunsigned int count = 0;\n\n\tif (channel >= MAX_CHANNELS) {\n\t\tusb_audio_dbg(mixer->chip,\n\t\t\t\"%s(): bogus channel number %d\\n\",\n\t\t\t__func__, channel);\n\t\treturn;\n\t}\n\n\tfor_each_mixer_elem(list, mixer, unitid)\n\t\tcount++;\n\n\tif (count == 0)\n\t\treturn;\n\n\tfor_each_mixer_elem(list, mixer, unitid) {\n\t\tstruct usb_mixer_elem_info *info;\n\n\t\tif (!list->kctl)\n\t\t\tcontinue;\n\n\t\tinfo = mixer_elem_list_to_info(list);\n\t\tif (count > 1 && info->control != control)\n\t\t\tcontinue;\n\n\t\tswitch (attribute) {\n\t\tcase UAC2_CS_CUR:\n\t\t\t/* invalidate cache, so the value is read from the device */\n\t\t\tif (channel)\n\t\t\t\tinfo->cached &= ~(1 << channel);\n\t\t\telse /* master channel */\n\t\t\t\tinfo->cached = 0;\n\n\t\t\tsnd_ctl_notify(mixer->chip->card, SNDRV_CTL_EVENT_MASK_VALUE,\n\t\t\t\t       &info->head.kctl->id);\n\t\t\tbreak;\n\n\t\tcase UAC2_CS_RANGE:\n\t\t\t/* TODO */\n\t\t\tbreak;\n\n\t\tcase UAC2_CS_MEM:\n\t\t\t/* TODO */\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tusb_audio_dbg(mixer->chip,\n\t\t\t\t\"unknown attribute %d in interrupt\\n\",\n\t\t\t\tattribute);\n\t\t\tbreak;\n\t\t} /* switch */\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_499": {
        "vulnerable_code": {
            "Code": "uac_mixer_unit_get_channels(struct mixer_build *state,\n\t\t\t\t       struct uac_mixer_unit_descriptor *desc){\n\tint mu_channels;\n\tvoid *c;\n\n\tif (desc->bLength < sizeof(*desc))\n\t\treturn -EINVAL;\n\tif (!desc->bNrInPins)\n\t\treturn -EINVAL;\n\n\tswitch (state->mixer->protocol) {\n\tcase UAC_VERSION_1:\n\tcase UAC_VERSION_2:\n\tdefault:\n\t\tif (desc->bLength < sizeof(*desc) + desc->bNrInPins + 1)\n\t\t\treturn 0; /* no bmControls -> skip */\n\t\tmu_channels = uac_mixer_unit_bNrChannels(desc);\n\t\tbreak;\n\tcase UAC_VERSION_3:\n\t\tmu_channels = get_cluster_channels_v3(state,\n\t\t\t\tuac3_mixer_unit_wClusterDescrID(desc));\n\t\tbreak;\n\t}\n\n\tif (!mu_channels)\n\t\treturn 0;\n\n\tc = uac_mixer_unit_bmControls(desc, state->mixer->protocol);\n\tif (c - (void *)desc + (mu_channels - 1) / 8 >= desc->bLength)\n\t\treturn 0; /* no bmControls -> skip */\n\n\treturn mu_channels;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "uac_mixer_unit_get_channels(struct mixer_build *state,\n\t\t\t\t       struct uac_mixer_unit_descriptor *desc){\n\tint mu_channels;\n\tvoid *c;\n\n\tif (desc->bLength < sizeof(*desc))\n\t\treturn -EINVAL;\n\tif (!desc->bNrInPins)\n\t\treturn -EINVAL;\n\tif (desc->bLength < sizeof(*desc) + desc->bNrInPins)\n\t\treturn -EINVAL;\n\n\tswitch (state->mixer->protocol) {\n\tcase UAC_VERSION_1:\n\tcase UAC_VERSION_2:\n\tdefault:\n\t\tif (desc->bLength < sizeof(*desc) + desc->bNrInPins + 1)\n\t\t\treturn 0; /* no bmControls -> skip */\n\t\tmu_channels = uac_mixer_unit_bNrChannels(desc);\n\t\tbreak;\n\tcase UAC_VERSION_3:\n\t\tmu_channels = get_cluster_channels_v3(state,\n\t\t\t\tuac3_mixer_unit_wClusterDescrID(desc));\n\t\tbreak;\n\t}\n\n\tif (!mu_channels)\n\t\treturn 0;\n\n\tc = uac_mixer_unit_bmControls(desc, state->mixer->protocol);\n\tif (c - (void *)desc + (mu_channels - 1) / 8 >= desc->bLength)\n\t\treturn 0; /* no bmControls -> skip */\n\n\treturn mu_channels;\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_500": {
        "vulnerable_code": {
            "Code": "volume_control_quirks(struct usb_mixer_elem_info *cval,\n\t\t\t\t  struct snd_kcontrol *kctl){\n\tstruct snd_usb_audio *chip = cval->head.mixer->chip;\n\tswitch (chip->usb_id) {\n\tcase USB_ID(0x0763, 0x2030): /* M-Audio Fast Track C400 */\n\tcase USB_ID(0x0763, 0x2031): /* M-Audio Fast Track C600 */\n\t\tif (strcmp(kctl->id.name, \"Effect Duration\") == 0) {\n\t\t\tcval->min = 0x0000;\n\t\t\tcval->max = 0xffff;\n\t\t\tcval->res = 0x00e6;\n\t\t\tbreak;\n\t\t}\n\t\tif (strcmp(kctl->id.name, \"Effect Volume\") == 0 ||\n\t\t    strcmp(kctl->id.name, \"Effect Feedback Volume\") == 0) {\n\t\t\tcval->min = 0x00;\n\t\t\tcval->max = 0xff;\n\t\t\tbreak;\n\t\t}\n\t\tif (strstr(kctl->id.name, \"Effect Return\") != NULL) {\n\t\t\tcval->min = 0xb706;\n\t\t\tcval->max = 0xff7b;\n\t\t\tcval->res = 0x0073;\n\t\t\tbreak;\n\t\t}\n\t\tif ((strstr(kctl->id.name, \"Playback Volume\") != NULL) ||\n\t\t\t(strstr(kctl->id.name, \"Effect Send\") != NULL)) {\n\t\t\tcval->min = 0xb5fb; /* -73 dB = 0xb6ff */\n\t\t\tcval->max = 0xfcfe;\n\t\t\tcval->res = 0x0073;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x0763, 0x2081): /* M-Audio Fast Track Ultra 8R */\n\tcase USB_ID(0x0763, 0x2080): /* M-Audio Fast Track Ultra */\n\t\tif (strcmp(kctl->id.name, \"Effect Duration\") == 0) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t       \"set quirk for FTU Effect Duration\\n\");\n\t\t\tcval->min = 0x0000;\n\t\t\tcval->max = 0x7f00;\n\t\t\tcval->res = 0x0100;\n\t\t\tbreak;\n\t\t}\n\t\tif (strcmp(kctl->id.name, \"Effect Volume\") == 0 ||\n\t\t    strcmp(kctl->id.name, \"Effect Feedback Volume\") == 0) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t       \"set quirks for FTU Effect Feedback/Volume\\n\");\n\t\t\tcval->min = 0x00;\n\t\t\tcval->max = 0x7f;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x0d8c, 0x0103):\n\t\tif (!strcmp(kctl->id.name, \"PCM Playback Volume\")) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t \"set volume quirk for CM102-A+/102S+\\n\");\n\t\t\tcval->min = -256;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x0471, 0x0101):\n\tcase USB_ID(0x0471, 0x0104):\n\tcase USB_ID(0x0471, 0x0105):\n\tcase USB_ID(0x0672, 0x1041):\n\t/* quirk for UDA1321/N101.\n\t * note that detection between firmware 2.1.1.7 (N101)\n\t * and later 2.1.1.21 is not very clear from datasheets.\n\t * I hope that the min value is -15360 for newer firmware --jk\n\t */\n\t\tif (!strcmp(kctl->id.name, \"PCM Playback Volume\") &&\n\t\t    cval->min == -15616) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t \"set volume quirk for UDA1321/N101 chip\\n\");\n\t\t\tcval->max = -256;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x046d, 0x09a4):\n\t\tif (!strcmp(kctl->id.name, \"Mic Capture Volume\")) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t\"set volume quirk for QuickCam E3500\\n\");\n\t\t\tcval->min = 6080;\n\t\t\tcval->max = 8768;\n\t\t\tcval->res = 192;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x046d, 0x0807): /* Logitech Webcam C500 */\n\tcase USB_ID(0x046d, 0x0808):\n\tcase USB_ID(0x046d, 0x0809):\n\tcase USB_ID(0x046d, 0x0819): /* Logitech Webcam C210 */\n\tcase USB_ID(0x046d, 0x081b): /* HD Webcam c310 */\n\tcase USB_ID(0x046d, 0x081d): /* HD Webcam c510 */\n\tcase USB_ID(0x046d, 0x0825): /* HD Webcam c270 */\n\tcase USB_ID(0x046d, 0x0826): /* HD Webcam c525 */\n\tcase USB_ID(0x046d, 0x08ca): /* Logitech Quickcam Fusion */\n\tcase USB_ID(0x046d, 0x0991):\n\tcase USB_ID(0x046d, 0x09a2): /* QuickCam Communicate Deluxe/S7500 */\n\t/* Most audio usb devices lie about volume resolution.\n\t * Most Logitech webcams have res = 384.\n\t * Probably there is some logitech magic behind this number --fishor\n\t */\n\t\tif (!strcmp(kctl->id.name, \"Mic Capture Volume\")) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t\"set resolution quirk: cval->res = 384\\n\");\n\t\t\tcval->res = 384;\n\t\t}\n\t\tbreak;\n\t}\n}",
            "Size": 0,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 3,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "volume_control_quirks(struct usb_mixer_elem_info *cval,\n\t\t\t\t  struct snd_kcontrol *kctl){\n\tstruct snd_usb_audio *chip = cval->head.mixer->chip;\n\tswitch (chip->usb_id) {\n\tcase USB_ID(0x0763, 0x2030): /* M-Audio Fast Track C400 */\n\tcase USB_ID(0x0763, 0x2031): /* M-Audio Fast Track C600 */\n\t\tif (strcmp(kctl->id.name, \"Effect Duration\") == 0) {\n\t\t\tcval->min = 0x0000;\n\t\t\tcval->max = 0xffff;\n\t\t\tcval->res = 0x00e6;\n\t\t\tbreak;\n\t\t}\n\t\tif (strcmp(kctl->id.name, \"Effect Volume\") == 0 ||\n\t\t    strcmp(kctl->id.name, \"Effect Feedback Volume\") == 0) {\n\t\t\tcval->min = 0x00;\n\t\t\tcval->max = 0xff;\n\t\t\tbreak;\n\t\t}\n\t\tif (strstr(kctl->id.name, \"Effect Return\") != NULL) {\n\t\t\tcval->min = 0xb706;\n\t\t\tcval->max = 0xff7b;\n\t\t\tcval->res = 0x0073;\n\t\t\tbreak;\n\t\t}\n\t\tif ((strstr(kctl->id.name, \"Playback Volume\") != NULL) ||\n\t\t\t(strstr(kctl->id.name, \"Effect Send\") != NULL)) {\n\t\t\tcval->min = 0xb5fb; /* -73 dB = 0xb6ff */\n\t\t\tcval->max = 0xfcfe;\n\t\t\tcval->res = 0x0073;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x0763, 0x2081): /* M-Audio Fast Track Ultra 8R */\n\tcase USB_ID(0x0763, 0x2080): /* M-Audio Fast Track Ultra */\n\t\tif (strcmp(kctl->id.name, \"Effect Duration\") == 0) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t       \"set quirk for FTU Effect Duration\\n\");\n\t\t\tcval->min = 0x0000;\n\t\t\tcval->max = 0x7f00;\n\t\t\tcval->res = 0x0100;\n\t\t\tbreak;\n\t\t}\n\t\tif (strcmp(kctl->id.name, \"Effect Volume\") == 0 ||\n\t\t    strcmp(kctl->id.name, \"Effect Feedback Volume\") == 0) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t       \"set quirks for FTU Effect Feedback/Volume\\n\");\n\t\t\tcval->min = 0x00;\n\t\t\tcval->max = 0x7f;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x0d8c, 0x0103):\n\t\tif (!strcmp(kctl->id.name, \"PCM Playback Volume\")) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t \"set volume quirk for CM102-A+/102S+\\n\");\n\t\t\tcval->min = -256;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x0471, 0x0101):\n\tcase USB_ID(0x0471, 0x0104):\n\tcase USB_ID(0x0471, 0x0105):\n\tcase USB_ID(0x0672, 0x1041):\n\t/* quirk for UDA1321/N101.\n\t * note that detection between firmware 2.1.1.7 (N101)\n\t * and later 2.1.1.21 is not very clear from datasheets.\n\t * I hope that the min value is -15360 for newer firmware --jk\n\t */\n\t\tif (!strcmp(kctl->id.name, \"PCM Playback Volume\") &&\n\t\t    cval->min == -15616) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t \"set volume quirk for UDA1321/N101 chip\\n\");\n\t\t\tcval->max = -256;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x046d, 0x09a4):\n\t\tif (!strcmp(kctl->id.name, \"Mic Capture Volume\")) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t\"set volume quirk for QuickCam E3500\\n\");\n\t\t\tcval->min = 6080;\n\t\t\tcval->max = 8768;\n\t\t\tcval->res = 192;\n\t\t}\n\t\tbreak;\n\n\tcase USB_ID(0x046d, 0x0807): /* Logitech Webcam C500 */\n\tcase USB_ID(0x046d, 0x0808):\n\tcase USB_ID(0x046d, 0x0809):\n\tcase USB_ID(0x046d, 0x0819): /* Logitech Webcam C210 */\n\tcase USB_ID(0x046d, 0x081b): /* HD Webcam c310 */\n\tcase USB_ID(0x046d, 0x081d): /* HD Webcam c510 */\n\tcase USB_ID(0x046d, 0x0825): /* HD Webcam c270 */\n\tcase USB_ID(0x046d, 0x0826): /* HD Webcam c525 */\n\tcase USB_ID(0x046d, 0x08ca): /* Logitech Quickcam Fusion */\n\tcase USB_ID(0x046d, 0x0991):\n\tcase USB_ID(0x046d, 0x09a2): /* QuickCam Communicate Deluxe/S7500 */\n\t/* Most audio usb devices lie about volume resolution.\n\t * Most Logitech webcams have res = 384.\n\t * Probably there is some logitech magic behind this number --fishor\n\t */\n\t\tif (!strcmp(kctl->id.name, \"Mic Capture Volume\")) {\n\t\t\tusb_audio_info(chip,\n\t\t\t\t\"set resolution quirk: cval->res = 384\\n\");\n\t\t\tcval->res = 384;\n\t\t}\n\t\tbreak;\n\t}\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_501": {
        "vulnerable_code": {
            "Code": "get_fpexc_mode(struct task_struct *tsk, unsigned long adr){\n\tunsigned int val;\n\n\tif (tsk->thread.fpexc_mode & PR_FP_EXC_SW_ENABLE)\n#ifdef CONFIG_SPE\n\t\tif (cpu_has_feature(CPU_FTR_SPE)) {\n\t\t\t/*\n\t\t\t * When the sticky exception bits are set\n\t\t\t * directly by userspace, it must call prctl\n\t\t\t * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE\n\t\t\t * in the existing prctl settings) or\n\t\t\t * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in\n\t\t\t * the bits being set).  <fenv.h> functions\n\t\t\t * saving and restoring the whole\n\t\t\t * floating-point environment need to do so\n\t\t\t * anyway to restore the prctl settings from\n\t\t\t * the saved environment.\n\t\t\t */\n\t\t\ttsk->thread.spefscr_last = mfspr(SPRN_SPEFSCR);\n\t\t\tval = tsk->thread.fpexc_mode;\n\t\t} else\n\t\t\treturn -EINVAL;\n#else\n\t\treturn -EINVAL;\n#endif\n\telse\n\t\tval = __unpack_fe01(tsk->thread.fpexc_mode);\n\treturn put_user(val, (unsigned int __user *) adr);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "get_fpexc_mode(struct task_struct *tsk, unsigned long adr){\n\tunsigned int val;\n\n\tif (tsk->thread.fpexc_mode & PR_FP_EXC_SW_ENABLE)\n#ifdef CONFIG_SPE\n\t\tif (cpu_has_feature(CPU_FTR_SPE)) {\n\t\t\t/*\n\t\t\t * When the sticky exception bits are set\n\t\t\t * directly by userspace, it must call prctl\n\t\t\t * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE\n\t\t\t * in the existing prctl settings) or\n\t\t\t * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in\n\t\t\t * the bits being set).  <fenv.h> functions\n\t\t\t * saving and restoring the whole\n\t\t\t * floating-point environment need to do so\n\t\t\t * anyway to restore the prctl settings from\n\t\t\t * the saved environment.\n\t\t\t */\n\t\t\ttsk->thread.spefscr_last = mfspr(SPRN_SPEFSCR);\n\t\t\tval = tsk->thread.fpexc_mode;\n\t\t} else\n\t\t\treturn -EINVAL;\n#else\n\t\treturn -EINVAL;\n#endif\n\telse\n\t\tval = __unpack_fe01(tsk->thread.fpexc_mode);\n\treturn put_user(val, (unsigned int __user *) adr);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_502": {
        "vulnerable_code": {
            "Code": "giveup_all(struct task_struct *tsk){\n\tunsigned long usermsr;\n\n\tif (!tsk->thread.regs)\n\t\treturn;\n\n\tusermsr = tsk->thread.regs->msr;\n\n\tif ((usermsr & msr_all_available) == 0)\n\t\treturn;\n\n\tmsr_check_and_set(msr_all_available);\n\tcheck_if_tm_restore_required(tsk);\n\n\tWARN_ON((usermsr & MSR_VSX) && !((usermsr & MSR_FP) && (usermsr & MSR_VEC)));\n\n#ifdef CONFIG_PPC_FPU\n\tif (usermsr & MSR_FP)\n\t\t__giveup_fpu(tsk);\n#endif\n#ifdef CONFIG_ALTIVEC\n\tif (usermsr & MSR_VEC)\n\t\t__giveup_altivec(tsk);\n#endif\n#ifdef CONFIG_SPE\n\tif (usermsr & MSR_SPE)\n\t\t__giveup_spe(tsk);\n#endif\n\n\tmsr_check_and_clear(msr_all_available);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "giveup_all(struct task_struct *tsk){\n\tunsigned long usermsr;\n\n\tif (!tsk->thread.regs)\n\t\treturn;\n\n\tcheck_if_tm_restore_required(tsk);\n\n\tusermsr = tsk->thread.regs->msr;\n\n\tif ((usermsr & msr_all_available) == 0)\n\t\treturn;\n\n\tmsr_check_and_set(msr_all_available);\n\n\tWARN_ON((usermsr & MSR_VSX) && !((usermsr & MSR_FP) && (usermsr & MSR_VEC)));\n\n#ifdef CONFIG_PPC_FPU\n\tif (usermsr & MSR_FP)\n\t\t__giveup_fpu(tsk);\n#endif\n#ifdef CONFIG_ALTIVEC\n\tif (usermsr & MSR_VEC)\n\t\t__giveup_altivec(tsk);\n#endif\n#ifdef CONFIG_SPE\n\tif (usermsr & MSR_SPE)\n\t\t__giveup_spe(tsk);\n#endif\n\n\tmsr_check_and_clear(msr_all_available);\n}",
            "Size": 3,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_503": {
        "vulnerable_code": {
            "Code": "restore_sprs(struct thread_struct *old_thread,\n\t\t\t\tstruct thread_struct *new_thread){\n#ifdef CONFIG_ALTIVEC\n\tif (cpu_has_feature(CPU_FTR_ALTIVEC) &&\n\t    old_thread->vrsave != new_thread->vrsave)\n\t\tmtspr(SPRN_VRSAVE, new_thread->vrsave);\n#endif\n#ifdef CONFIG_PPC_BOOK3S_64\n\tif (cpu_has_feature(CPU_FTR_DSCR)) {\n\t\tu64 dscr = get_paca()->dscr_default;\n\t\tif (new_thread->dscr_inherit)\n\t\t\tdscr = new_thread->dscr;\n\n\t\tif (old_thread->dscr != dscr)\n\t\t\tmtspr(SPRN_DSCR, dscr);\n\t}\n\n\tif (cpu_has_feature(CPU_FTR_ARCH_207S)) {\n\t\tif (old_thread->bescr != new_thread->bescr)\n\t\t\tmtspr(SPRN_BESCR, new_thread->bescr);\n\t\tif (old_thread->ebbhr != new_thread->ebbhr)\n\t\t\tmtspr(SPRN_EBBHR, new_thread->ebbhr);\n\t\tif (old_thread->ebbrr != new_thread->ebbrr)\n\t\t\tmtspr(SPRN_EBBRR, new_thread->ebbrr);\n\n\t\tif (old_thread->fscr != new_thread->fscr)\n\t\t\tmtspr(SPRN_FSCR, new_thread->fscr);\n\n\t\tif (old_thread->tar != new_thread->tar)\n\t\t\tmtspr(SPRN_TAR, new_thread->tar);\n\t}\n\n\tif (cpu_has_feature(CPU_FTR_P9_TIDR) &&\n\t    old_thread->tidr != new_thread->tidr)\n\t\tmtspr(SPRN_TIDR, new_thread->tidr);\n#endif\n\n\tthread_pkey_regs_restore(new_thread, old_thread);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "restore_sprs(struct thread_struct *old_thread,\n\t\t\t\tstruct thread_struct *new_thread){\n#ifdef CONFIG_ALTIVEC\n\tif (cpu_has_feature(CPU_FTR_ALTIVEC) &&\n\t    old_thread->vrsave != new_thread->vrsave)\n\t\tmtspr(SPRN_VRSAVE, new_thread->vrsave);\n#endif\n#ifdef CONFIG_PPC_BOOK3S_64\n\tif (cpu_has_feature(CPU_FTR_DSCR)) {\n\t\tu64 dscr = get_paca()->dscr_default;\n\t\tif (new_thread->dscr_inherit)\n\t\t\tdscr = new_thread->dscr;\n\n\t\tif (old_thread->dscr != dscr)\n\t\t\tmtspr(SPRN_DSCR, dscr);\n\t}\n\n\tif (cpu_has_feature(CPU_FTR_ARCH_207S)) {\n\t\tif (old_thread->bescr != new_thread->bescr)\n\t\t\tmtspr(SPRN_BESCR, new_thread->bescr);\n\t\tif (old_thread->ebbhr != new_thread->ebbhr)\n\t\t\tmtspr(SPRN_EBBHR, new_thread->ebbhr);\n\t\tif (old_thread->ebbrr != new_thread->ebbrr)\n\t\t\tmtspr(SPRN_EBBRR, new_thread->ebbrr);\n\n\t\tif (old_thread->fscr != new_thread->fscr)\n\t\t\tmtspr(SPRN_FSCR, new_thread->fscr);\n\n\t\tif (old_thread->tar != new_thread->tar)\n\t\t\tmtspr(SPRN_TAR, new_thread->tar);\n\t}\n\n\tif (cpu_has_feature(CPU_FTR_P9_TIDR) &&\n\t    old_thread->tidr != new_thread->tidr)\n\t\tmtspr(SPRN_TIDR, new_thread->tidr);\n#endif\n\n\tthread_pkey_regs_restore(new_thread, old_thread);\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_504": {
        "vulnerable_code": {
            "Code": "set_dabr(struct arch_hw_breakpoint *brk){\n\tunsigned long dabr, dabrx;\n\n\tdabr = brk->address | (brk->type & HW_BRK_TYPE_DABR);\n\tdabrx = ((brk->type >> 3) & 0x7);\n\n\tif (ppc_md.set_dabr)\n\t\treturn ppc_md.set_dabr(dabr, dabrx);\n\n\treturn __set_dabr(dabr, dabrx);\n}",
            "Size": 3,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "set_dabr(struct arch_hw_breakpoint *brk){\n\tunsigned long dabr, dabrx;\n\n\tdabr = brk->address | (brk->type & HW_BRK_TYPE_DABR);\n\tdabrx = ((brk->type >> 3) & 0x7);\n\n\tif (ppc_md.set_dabr)\n\t\treturn ppc_md.set_dabr(dabr, dabrx);\n\n\treturn __set_dabr(dabr, dabrx);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_505": {
        "vulnerable_code": {
            "Code": "set_debug_reg_defaults(struct thread_struct *thread){\n\tthread->debug.iac1 = thread->debug.iac2 = 0;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\tthread->debug.iac3 = thread->debug.iac4 = 0;\n#endif\n\tthread->debug.dac1 = thread->debug.dac2 = 0;\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\tthread->debug.dvc1 = thread->debug.dvc2 = 0;\n#endif\n\tthread->debug.dbcr0 = 0;\n#ifdef CONFIG_BOOKE\n\t/*\n\t * Force User/Supervisor bits to b11 (user-only MSR[PR]=1)\n\t */\n\tthread->debug.dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |\n\t\t\tDBCR1_IAC3US | DBCR1_IAC4US;\n\t/*\n\t * Force Data Address Compare User/Supervisor bits to be User-only\n\t * (0b11 MSR[PR]=1) and set all other bits in DBCR2 register to be 0.\n\t */\n\tthread->debug.dbcr2 = DBCR2_DAC1US | DBCR2_DAC2US;\n#else\n\tthread->debug.dbcr1 = 0;\n#endif\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "set_debug_reg_defaults(struct thread_struct *thread){\n\tthread->debug.iac1 = thread->debug.iac2 = 0;\n#if CONFIG_PPC_ADV_DEBUG_IACS > 2\n\tthread->debug.iac3 = thread->debug.iac4 = 0;\n#endif\n\tthread->debug.dac1 = thread->debug.dac2 = 0;\n#if CONFIG_PPC_ADV_DEBUG_DVCS > 0\n\tthread->debug.dvc1 = thread->debug.dvc2 = 0;\n#endif\n\tthread->debug.dbcr0 = 0;\n#ifdef CONFIG_BOOKE\n\t/*\n\t * Force User/Supervisor bits to b11 (user-only MSR[PR]=1)\n\t */\n\tthread->debug.dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |\n\t\t\tDBCR1_IAC3US | DBCR1_IAC4US;\n\t/*\n\t * Force Data Address Compare User/Supervisor bits to be User-only\n\t * (0b11 MSR[PR]=1) and set all other bits in DBCR2 register to be 0.\n\t */\n\tthread->debug.dbcr2 = DBCR2_DAC1US | DBCR2_DAC2US;\n#else\n\tthread->debug.dbcr1 = 0;\n#endif\n}",
            "Size": 4,
            "Code Complexity": 4,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_506": {
        "vulnerable_code": {
            "Code": "tm_active_with_altivec(struct task_struct *tsk){\n\treturn MSR_TM_ACTIVE(tsk->thread.regs->msr) &&\n\t\t(tsk->thread.ckpt_regs.msr & MSR_VEC);\n}",
            "Size": 4,
            "Code Complexity": 2,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "tm_active_with_altivec(struct task_struct *tsk){\n\treturn MSR_TM_ACTIVE(tsk->thread.regs->msr) &&\n\t\t(tsk->thread.ckpt_regs.msr & MSR_VEC);\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    },
    "V_507": {
        "vulnerable_code": {
            "Code": "ax25_free_sock(struct sock *sk){\n\tax25_cb_put(sk_to_ax25(sk));\n}",
            "Size": 4,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 4,
            "Error Handling": 0
        },
        "neutral_code": {
            "Code": "ax25_free_sock(struct sock *sk){\n\tax25_cb_put(sk_to_ax25(sk));\n}",
            "Size": 3,
            "Code Complexity": 3,
            "Memory Management": 4,
            "Code Clarity": 0,
            "Error Handling": 0
        }
    }
}